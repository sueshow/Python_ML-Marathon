# Python_ML-Marathon
## 學習大綱
### <a href="#機器學習概論">機器學習概論</a>
  * <a href="#D001-資料介紹與評估資料">D001 資料介紹與評估資料</a>
  * <a href="#D002-機器學習概論">D002 機器學習概論</a>
  * <a href="#D003-機器學習-流程與步驟">D003 機器學習-流程與步驟</a>
  * <a href="#D004-EDA-讀取資料與分析流程">D004 EDA/讀取資料與分析流程</a>
### <a href="#資料清理數據前處理">資料清理數據前處理</a>
  * <a href="#D005-如何新建一個dataframe如何讀取其他資料">D005 如何新建一個 dataframe？如何讀取其他資料？(非csv的資料)</a>
  * <a href="#D006-EDA-欄位的資料類型介紹及處理">D006 EDA-欄位的資料類型介紹及處理</a>
  * <a href="#D007-EDA-特徵類型">D007 EDA-特徵類型</a>
  * <a href="#D008-EDA-資料分佈">D008 EDA-資料分佈</a>
  * <a href="#D009-EDA-Outlier及處理">D009 EDA-Outlier及處理</a>
  * <a href="#D010-EDA-去除離群值-數值型">D010 EDA-去除離群值(數值型)</a>
  * <a href="#D011-EDA-常用的數值取代">D011 EDA-常用的數值取代</a>
  * <a href="#D012-EDA-補缺失值與標準化-數值型">D012 EDA-補缺失值與標準化(數值型)</a>
  * <a href="#D013-常見的DataFrame操作">D013 常見的 DataFrame 操作</a>
  * <a href="#D014-程式實作EDA-相關係數簡介">D014 程式實作 EDA-相關係數簡介</a>
  * <a href="#D015-程式實作EDA-CorrelationCode">D015 程式實作EDA-Correlation code</a>
  * <a href="#D016-EDA-不同數值範圍間的特徵如何檢視">D016 EDA-不同數值範圍間的特徵如何檢視</a>
  * <a href="#D017-EDA-把連續型變數離散化">D017 EDA-把連續型變數離散化</a>
  * <a href="#D018-程式實作EDA-把連續型變數離散化">D018 程式實作EDA-把連續型變數離散化</a>
  * <a href="#D019-程式實作-Subplots">D019 程式實作-Subplots</a>
  * <a href="#D020-程式實作-HeatmapANDGrid-plot">D020 程式實作-Heatmap & Grid-plot</a>
  * <a href="#D021-模型-LogisticRegression">D021 模型-Logistic Regression</a>
### <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
  * <a href="#D022-特徵工程簡介">D022 特徵工程簡介</a>
  * <a href="#D023-特徵工程-數值型-去除偏態">D023 特徵工程(數值型)-去除偏態</a>
  * <a href="#D024-特徵工程-類別型-基礎處理">D024 特徵工程(類別型)-基礎處理</a>
  * <a href="#D025-特徵工程-類別型-均值編碼">D025 特徵工程(類別型)-均值編碼</a>
  * <a href="#D026-特徵工程-類別型-其他進階處理">D026 特徵工程(類別型)-其他進階處理</a>
  * <a href="#D027-特徵工程-時間型">D027 特徵工程(時間型)</a>
  * <a href="#D028-特徵工程-數值與數值組合">D028 特徵工程-數值與數值組合</a>
  * <a href="#D029-特徵工程-類別與數值組合">D029 特徵工程-類別與數值組合</a>
  * <a href="#D030-特徵選擇">D030 特徵選擇</a>
  * <a href="#D031-特徵評估">D031 特徵評估</a>
  * <a href="#D032-特徵優化-分類型-葉編碼">D032 特徵優化(分類型)-葉編碼</a>
### <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
  * <a href="#D033-機器如何學習">D033 機器如何學習?</a>
  * <a href="#D034-訓練AND測試集切分">D034 訓練/測試集切分</a>
  * <a href="#D035-RegressionVSClassification">D035 Regression vs. classification</a>
  * <a href="#D036-評估指標選定EvaluationMetrics">D036 評估指標選定 evaluation metrics</a>
  * <a href="#D037-RegressionModel-線性迴歸AND羅吉斯回歸">D037 Regression model-線性迴歸、羅吉斯回歸</a>
  * <a href="#D038-程式實作-線性迴歸AND羅吉斯回歸">D038 程式實作-線性迴歸、羅吉斯回歸</a>
  * <a href="#D039-RegressionModel-LASSO回歸ANDRidge回歸">D039 Regression model-LASSO回歸、Ridge回歸</a>
  * <a href="#D040-程式實作-LASSO回歸ANDRidge回歸">D040 程式實作-LASSO回歸、Ridge回歸</a>
  * <a href="#D041-TreeBasedModel-決策樹DecisionTree">D041 Tree based model-決策樹(Decision Tree)</a>
  * <a href="#D042-程式實作-決策樹">D042 程式實作-決策樹</a>
  * <a href="#D043-TreeBasedModel-隨機森林RandomForest">D043 Tree based model-隨機森林(Random Forest)</a>
  * <a href="#D044-程式實作-隨機森林">D044 程式實作-隨機森林</a>
  * <a href="#D045-TreeBasedModel-梯度提升機GradientBoostingMachine">D045 Tree based model-梯度提升機(Gradient Boosting Machine)</a>
  * <a href="#D046-程式實作-梯度提升機">D046 程式實作-梯度提升機</a>
### <a href="#機器學習調整參數">機器學習調整參數</a>
  * D047 超參數調整與優化
  * D048 Kaggle 競賽平台介紹
  * D049 集成方法-混和泛化(Blending)
  * D050 集成方法-堆疊泛化(Stacking)
### <a href="#Kaggle期中考">Kaggle期中考</a>
  * D051-D053 Kaggle 期中考
### <a href="#非監督式的機器學習">非監督式的機器學習</a>
  * D054 非監督式機器學習
  * D055 非監督式-分群-K-Means 分群
  * D056 非監督式-分群-K-Means 分群評估：使用輪廓分析
  * D057 非監督式-分群-階層式 Hierarchical Clustering
  * D058 非監督式-分群-Hierarchical Clustering 觀察：使用 2D 樣版資料集
  * D059 降維方法(Dimension Reduction)-主成份分析(PCA)
  * D060 程式實作-PCA：使用手寫辨識資料集
  * D061 降維方法(Dimension Reduction)-T-SNE
  * D062 程式實作-T-SNE：分群與流形還原
### <a href="#深度學習理論與實作">深度學習理論與實作</a>
  * D063 深度學習簡介
  * D064 深度學習-模型調整與學習曲線
  * D065 深度學習-啟動函數與正規化
### <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
  * D066 Keras安裝與介紹
  * D067 Keras Dataset
  * D068 Keras Sequential API
  * D069 Keras Module API
  * D070 深度神經網路的基礎知識
  * D071 損失函數
  * D072 啟動函數
  * D073 梯度下降 Gradient Descent
  * D074 Gradient Descent 數學原理
  * D075 BackPropagation
  * D076 優化器 Optimizers
  * D077 神經網路-訓練細節與技巧：Validation and overfit
  * D078 神經網路-訓練前注意事項
  * D079 神經網路-訓練細節與技巧：Learning rate effect
  * D080 程式實作-優化器與學習率的組合與比較
  * D081 神經網路-訓練細節與技巧：Regularization
  * D082 神經網路-訓練細節與技巧：Dropout
  * D083 神經網路-訓練細節與技巧：Batch normalization
  * D084 程式實作-正規化/機移除/批次標準化的組合與比較
  * D085 神經網路-訓練細節與技巧：使用 callbacks 函數做 earlystop
  * D086 神經網路-訓練細節與技巧：使用 callbacks 函數儲存 model
  * D087 神經網路-訓練細節與技巧：使用 callbacks 函數做 reduce learning rate
  * D088 神經網路-訓練細節與技巧：撰寫 callbacks 函數
  * D089 神經網路-訓練細節與技巧：撰寫 Loss function
  * D090 使用傳統電腦視覺與機器學習進行影像辨識
  * D091 程式實作-使用傳統電腦視覺與機器學習進行影像辨識
### <a href="#深度學習應用卷積神經網路">深度學習應用卷積神經網路</a>
  * D092 卷積神經網路(Convolution Neural Network, CNN)簡介
  * D093 CNN-架構細節
  * D094 CNN-卷積(Convolution)層與參數調整
  * D095 CNN-池化(Pooling)層與參數調整
  * D096 Keras 中的 CNN layers
  * D097 程式實作-CIFAR-10 資料集
  * D098 CNN-訓練細節與技巧：處理大量數據
  * D099 CNN-訓練細節與技巧：處理小量數據
  * D100 CNN-訓練細節與技巧：轉移學習(Transfer learning)
### <a href="#Kaggle期末考">Kaggle期末考</a>
  * D101-D103 影像辨識
### <a href="#Bonus進階補充">Bonus進階補充</a>
  * D104 史丹佛上 ConvNetJS 簡介
  * D105 CNN 卷積網路回顧
  * D106 電腦視覺常用公開資料集
  * D107 電腦視覺應用介紹
<br>
<br>

## 機器學習概論
### D001-資料介紹與評估資料
* 進入資料科學領域的流程
  * 找到問題：挑一個有趣的問題，並解決一個簡單的問題開始
  * 初探：在這個題目上做一個原型解決方案(prototype solution)
  * 改進：試圖改進你的原始解決方案並從中學習(如代碼優化、速度優化、演算法優化)
  * 分享：紀錄是一個好習慣，試著紀錄並分享解決方案歷程
  * 練習：不斷在一系列不同的問題上反覆練習
  * 實戰：認真地參與一場比賽
* 面對資料應思考哪些問題？
  * 好玩，如：預測生存(吃雞)遊戲誰可以活得久、[PUBG](https://www.kaggle.com/c/pubg-finish-placement-prediction)
  * 企業的核心問題，如：用戶廣告投放、[ADPC](https://www.kaggle.com/c/avito-demand-prediction)
  * 公眾利益/影響政策方向，如：[停車方針](https://www.kaggle.com/datasets/new-york-city/nyc-parking-tickets)、[計程車載客優化](https://www.kaggle.com/c/nyc-taxi-trip-duration)
  * 對世界很有貢獻，如：[肺炎偵測](https://www.kaggle.com/c/rsna-pneumonia-detection-challenge)
* 資料來源
  * 來源與品質息息相關
  * 根據不同資料源，合理地推測/懷疑異常資料異常的理由與機率
  * 方式：網站流量、購物車紀錄、網路爬蟲、格式化表單、[Crowdsourcing](https://en.wikipedia.org/wiki/Crowdsourcing)、紙本轉電子檔
* 資料型態
  * 結構化資料需要檢視欄位意義及名稱，如：數值、表格等
  * 非結構化資料需要思考資料轉換與標準化方式，如：圖像、影像、文字、音訊等
* 指標係指可供衡量的數學評估指標(Evaluation Metrics)，常用的衡量指標：
  * 分類問題
    * 正確率
    * AUC(Accuracy)：客群樣貌
    * MAP
  * 迴歸問題
    * MAE(平均絕對誤差)：玩家排名
    * MSE：存活時間
    * RMSE
  * [其他衡量指標](https://blog.csdn.net/aws3217150/article/details/50479457)
    * ROC(Receiver Operating Curve)：客群樣貌、素材好壞
    * MAP@N：如 MAP@5、MAP@12
* 範例與作業
  * [作業D001](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_001_example_of_metrics_Ans.ipynb) 
    * 目標：寫一個 MSE 函數

Back to <a href="#機器學習概論">機器學習概論</a>
<br>
<br>

### D002-機器學習概論
* 機器學習範疇
  * 人工智慧 > 機器學習 > 深度學習
  * 白話文：讓機器從資料中找尋規律與趨勢而不需要給定特殊規則
  * 數學：給定目標函數與訓練資料，學習出能讓目標函數最佳的模型參數
* 機器學習的組成及應用
  * 監督式學習：如圖像分類、詐騙偵測
    * 有成對的 (x,y) 資料，且 x 與 y 之間具有某種關係
    * 如圖像分類，每張圖都有對應到的標記(y)
    * 流程：前處理 Processing → 探索式數據分析 Exploratory Data Analysis (D014-D021：統計值【相關係數、核密度函數、離散化】的視覺化【繪圖排版、常用圖形、模型體驗】) → 特徵工程 Feature Engineering (D022-D032：填補缺值、去離群值、去偏態、特徵縮放、特徵組合、特徵評估) → 模型選擇 Model Selection (D033-D046：驗證基礎、預測模型、評估指標、基本模型、樹狀模型) → 參數調整 Fine Tuning → 集成 Ensemble
  * 非監督式學習：如維度縮減、分群、壓縮
    * 僅有 x 資料而沒有標註的 y
    * 如有圖像資料，但沒有標記
    * 應用：降維 Dimension Reduction、分群 Clustering
  * 強化學習：如下圍棋、打電玩
    * 又稱增強式學習，透過定義環境(Environment)、代理機器人(Agent)及獎勵(Reward)，讓機器人透過與環境的互動學習如何獲取最高的獎勵
    * 應用：Alpha GO
* 範例與作業
  * [作業D002](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_002_Ans.ipynb) 
    * 目標：瞭解機器學習適合應用的領域與範疇
  
Back to <a href="#機器學習概論">機器學習概論</a>
<br>
<br>

### D003-機器學習-流程與步驟
* 機器學習專案開發流程
  * 資料蒐集、前處理
    * 資料來源
      * 結構化資料：Excel檔、CSV檔
      * 非結構化資料：圖片、影音、文字
    * 瞭解資料，使用資料的 Python 套件
      * 開啟圖片：PIL、skimage、open-cv等
      * 開啟文件：pandas
    * 資料前處理，進行特徵工程
      * 缺失值填補
      * 離群值處理
      * 標準化
  * 定義目標與評估準則
    * 回歸問題(數值)？分類問題(類別)？
    * 要使用甚麼資料來進行預測？
    * 資料分為：訓練集training set、驗證集validation set、測試集test set
    * 評估指標
      * 回歸問題
        * RMSE (Root Mean Square Error)
        * Mean Absolute Error
        * R-Square
      * 分類問題
        * Accuracy
        * F1-score
        * AUC (Area Under Curve)
  * 建立模型與調整參數：模型調整、優化、訓練
    * 回歸模型 Regression
    * 樹模型 Tree-based model
    * 神經網絡 Neural network
  * 導入
    * 建立資料蒐集、前處理等流程
    * 送進模型進行預測
    * 輸出預測結果
    * 視專案需求整合前後端，資料格式使用 json、csv
* 範例與作業
  * [作業D003](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_003_Ans.ipynb)
    * 閱讀文章：機器學習巨頭作的專案

Back to <a href="#機器學習概論">機器學習概論</a>
<br>
<br>

### D004-EDA-讀取資料與分析流程
* 範例：Home Credit Default Risk (房貸風險預測 from Kaggle)
  * 目的：預測借款者是否會還款，以還款機率作為最終輸出
  * 此問題為分類問題
  * 步驟：
    * 為何這個問題重要：有人沒有信用資料
    * 資料從何而來：信用局(Credit Bureau)調閱紀錄、Home Credit內部紀錄(過去借貸、信用卡狀況)
    * 資料的型態：結構化資料(數值、類別資料)
    * 可以回答什麼問題：指標
      * [ROC](https://zh.wikipedia.org/wiki/ROC%E6%9B%B2%E7%BA%BF)
      * AUC：0.5代表隨機猜測，~1則代表模型預測力越好
* EDA
  * 初步透過視覺化/統計工具進行分析，達到三個主題目的
    * 了解資料：獲取資料所包含的資訊、結構和特點
    * 發現 outliers 或異常數值：檢查資料是否有誤
    * 分析各變數間的關聯性：找到重要的變數
  * 觀察資料，並檢查是否符合分析前的假設
  * 數據分析流程
    * 收集資料
    * 數據清理 → 特徵萃取 → 資料視覺化 → 建立模型 → 驗證模型
    * 決策應用
* 範例與作業
  * [範例D004](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_004_HW_EDA_%E8%AE%80%E5%8F%96%E8%B3%87%E6%96%99%E8%88%87%E5%88%86%E6%9E%90%E6%B5%81%E7%A8%8B/Day_004_first_EDA.ipynb)
    * 使用 pandas.read_csv 讀取資料
    * 簡單瀏覽 pandas 所讀進的資料
  * [作業D004](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_004_first_EDA_Ans.ipynb)
    * 列出資料的大小：shape
    * 列出所有欄位：columns
    * 擷取部分資料：loc、iloc

Back to <a href="#機器學習概論">機器學習概論</a>
<br>
<br>

## 資料清理數據前處理
### D005-如何新建一個dataframe如何讀取其他資料
* 前處理 Processing
  * 資料讀取 D005 → 格式調整 D006-D008、D013 → 填補缺值 D009、D011-D012 → 去離群值 D010 → 特徵縮放 D011-D012
  * 用途
    * 需要把分析過程中所產生的數據或結果儲存為[結構化的資料](https://daxpowerbi.com/%e7%b5%90%e6%a7%8b%e5%8c%96%e8%b3%87%e6%96%99/) → 使用 pandas
    * 資料量太大，操作很費時，先在具有同樣結構的資料進行小樣本的測試
    * 先建立 dataframe 來瞭解所需的資料結構、分佈
* 讀取其他資料格式：txt / jpg / png / json / mat / npy / pkl
  * 圖像檔 (jpg / png)
    * 範例：可使用 PIL、Skimage、CV2，其中 CV2 速度較快，但須注意讀入的格式為 BGR
      ```
      Import cv2
      image = cv2.imread(...) # 注意 cv2 會以 BGR 讀入
      image = cv2.cvtcolor(image, cv2.COLOR_BGR2RGB)

      from PIL import Image
      image = Image.read(...)
      import skimage.io as skio
      image = skio.imread(...)
      ```
  * Python npy：可儲存處理後的資料
    * 範例
      ```
      import numpy as np
      arr = np.load(example.npy)
      ```
  * Pickle (pkl)：可儲存處理後的資料
    * 範例
      ```
      import pickle
      with open('example.pkl', 'rb') as f:
          arr = pickle.load(f)
      ```
* 程式用法
  <table border="1" width="40%">
    <tr>
        <th width="10%">函式</a>
        <th width="10%">用途</a>
        <th width="10%">函式</a>
        <th width="10%">用途</a>
    </tr>
    <tr>
        <td> pd.DataFrame </td>
        <td> 建立一個 dataframe </td>
        <td> np.random.randint </td>
        <td> 產生隨機數值 </td>
    </tr>
    <tr>
        <td> with open() </td>
        <td> 文字格式 </td>
        <td>  </td>
        <td>  </td>
    </tr>
  </table>
  
* 延伸閱讀
  * [Pandas Foundations](https://www.datacamp.com/courses/data-manipulation-with-pandas)
  * [github repo](https://github.com/guipsamora/pandas_exercises)
* 範例與作業
  * [範例D005-1](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_005_HW_%E5%A6%82%E4%BD%95%E6%96%B0%E5%BB%BA%E4%B8%80%E5%80%8Bdataframe/Day_005-1_build_dataframe_from_scratch.ipynb)
    * Dict → DataFrame
    * List → DataFrame
    * Group by
  * [範例D005-2](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_005_HW_%E5%A6%82%E4%BD%95%E6%96%B0%E5%BB%BA%E4%B8%80%E5%80%8Bdataframe/Day_005-2_read_and_write_files.ipynb)
    * 檔案轉換：txt、json、npy、Pickle
    * 參考資料
      * [寫給自己的技術筆記 - 作為程式開發者我們絕對不能忽略的JSON - Python 如何處理JSON文件](https://matters.news/@CHWang/103773-%E5%AF%AB%E7%B5%A6%E8%87%AA%E5%B7%B1%E7%9A%84%E6%8A%80%E8%A1%93%E7%AD%86%E8%A8%98-%E4%BD%9C%E7%82%BA%E7%A8%8B%E5%BC%8F%E9%96%8B%E7%99%BC%E8%80%85%E6%88%91%E5%80%91%E7%B5%95%E5%B0%8D%E4%B8%8D%E8%83%BD%E5%BF%BD%E7%95%A5%E7%9A%84json-python-%E5%A6%82%E4%BD%95%E8%99%95%E7%90%86json%E6%96%87%E4%BB%B6-bafyreibegh77qc2xaejwbbbv5xdoodgqyaznesq5uhety5von3rpqzdaoa)
  * [範例D005-3](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_005_HW_%E5%A6%82%E4%BD%95%E6%96%B0%E5%BB%BA%E4%B8%80%E5%80%8Bdataframe/Day_005-3_read_and_write_files.ipynb)
    * 用 skimage.io 讀取圖檔
    * 用 PIL.Image 讀取圖檔
    * 用 OpenCV 讀取圖檔：pip install opencv-python
      * cv2.IMREAD_COLOR：讀取 RGB 的三個 CHANNELS 的彩色圖片，忽略透明度的 CHANNELS
        * cv2.IMREAD_GRAYSCALE：灰階
        * cv2.IMREAD_UNCHANGED：讀取圖片的所有 CHANNELS，包含透明度的 CHANNELS
  * [作業D005-1](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_005_%E4%BD%9C%E6%A5%AD%E8%A7%A3%E7%AD%94/Day_005-1_Ans.ipynb)
    * 重點：DataFrame、Group by
  * [作業D005-2](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_005_%E4%BD%9C%E6%A5%AD%E8%A7%A3%E7%AD%94/Day_005-2_Ans.ipynb)  
    * 從網頁上讀取連結清單
    * 從清單網址讀取圖片

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D006-EDA-欄位的資料類型介紹及處理
* EDA (Exploratory Data Analysis)：探索式資料分析運用統計工具或是學畫，對資料有初步的瞭解，以幫助我們後續對資料進行更進一步的分析
* 資料類型
  * 離散變數：只能用整數單位計算的變數，如房間數、性別、國家
  * 連續變數：在一定區間內可以任意取值的變數，如身高、降落花費的時間、車速
* Pandas DataFrame 常見的欄位類型(*)
  <table border="1" width="25%">
    <tr>
        <th width="5%">Pandas 類型</a>
        <th width="5%">Python 類型</a>
        <th width="10%">NumPy 類型</a>
        <th width="5%">說明</a>
    </tr>
    <tr>
        <td> object </td>
        <td> str or mixed  </td>
        <td> string、unicode、mixed types </td>
        <td> 字符串或混和數字，用於表示類別型變數 </td>
    </tr>
    <tr>
        <td> int64(*) </td>
        <td> int </td>
        <td> int、int8、int16、int32、int64、uint8、uint16、uint32、uint64 </td>
        <td> 整數，可表示離散或連續變數 </td>
    </tr>
    <tr>
        <td> float64(*) </td>
        <td> float </td>
        <td> float、float16、float32、float64 </td>
        <td> 浮點數，可表示離散或連續變數 </td>
    </tr>
    <tr>
        <td> bool </td>
        <td> bool </td>
        <td> bool </td>
        <td> True/False </td>
    </tr>
    <tr>
        <td> datetime64(ns) </td>
        <td> nan </td>
        <td> datetime64(ns) </td>
        <td> 日期時間 </td>
    </tr>
    <tr>
        <td> timedelta(ns) </td>
        <td> nan </td>
        <td> nan </td>
        <td> 時間差距 </td>
    </tr>
    <tr>
        <td> category </td>
        <td> nan </td>
        <td> nan </td>
        <td> 分類 </td>
    </tr>
  </table>
  
* 格式調整
  * 訓練模型時，字串/類別類型的資料需要轉為數值型資料，轉換方式：
    <table border="1" width="13%">
      <tr>
        <th width="3%">encode</a>
        <th width="10%">label encode</a>
        <th width="10%">one-hot encode</a>
      </tr>
      <tr>
        <td> 類型 </td>
        <td> 有序類別變量(如學歷)  </td>
        <td> 無序類別變量(如國家) </td>
      </tr>
      <tr>
        <td> 作法 </td>
        <td> 將類別變數中每一個類別賦予數值，不會新增欄位 </td>
        <td> 為每個類別新增一個欄位，0/1表示是否 </td>
      </tr>
      <tr>
        <td> 使用時機 </td>
        <td> 會讓模型學習到「順序關係」，也就是有大小之分 </td>
        <td> 當類別之間不存在優劣、前後、高低之分的時候，也就是「無序」，就適合採用 One-Hot Encoding。但相對地，因為維度提高了，就會較費時且占用較多的空間 </td>
      </tr>
    </table>

* 範例與作業
  * [範例D006](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_006_HW_EDA%E6%AC%84%E4%BD%8D%E7%9A%84%E8%B3%87%E6%96%99%E9%A1%9E%E5%9E%8B%E4%BB%8B%E7%B4%B9%E5%8F%8A%E8%99%95%E7%90%86/Day_006_column_data_type.ipynb)
    * 檢視 DataFrame 的資料型態
    * 瞭解 Label Encoding 如何寫
    * 瞭解 One Hot Encoding 如何寫(pd.get_dummies)
  * [作業D006](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_006_column_data_type_Ans.ipynb)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D007-EDA-特徵類型
* 特徵類別：[參考資料](https://openhome.cc/Gossip/CodeData/PythonTutorial/NumericStringPy3.html)
  * 數值型特徵：有不同轉換方式，函數/條件式都可以，如坪數、年齡
    * 填補缺失值或直接去除離群值的方法：[去偏態](https://ithelp.ithome.com.tw/articles/10219949?sc=iThelpR)，符合常態假設
      * 對數去偏(log1p)
      * 方根去偏(sqrt)
      * 分布去偏(boxcox)
  * 類別型特徵：通常一種類別對應一種分數，如行政區、性別
    * 標籤編碼(Label Encoding)
    * 獨熱編碼(One Hot Encoding)
    * 均值編碼(Mean Encoding)
    * 計數編碼(Counting)
    * 特徵雜湊(Feature Hash)
  * 其他類別
    <table border="1" width="13%">
      <tr>
        <th width="3%">特徵</a>
        <th width="10%">說明</a>
      </tr>
      <tr>
        <td> 二元特徵 </td>
        <td> ● 只有 True/False 兩種數值的特徵 <br>
             ● 可當作類別型，也可當作數值型特徵(True:1/False:0) </td>
      </tr>
      <tr>
        <td> 排序型特徵 </td>
        <td> ● 如名次/百分等級，有大小關係，但非連續數字 <br>
             ● 通常視為數值型特徵，避免失去排序資訊 </td>
      </tr>
      <tr>
        <td> 時間型特徵 </td>
        <td> ● 不適合當作數值型或類別型特徵，可能會失去週期性、排序資訊 <br>
             ● 特殊之處在於有週期性 <br>
             ● 處理方式：時間特徵分解、週期循環特徵 </td>
      </tr>
      <tr>
        <td> 文本型 </td>
        <td> ● TF-IDF、詞袋、word2vec </td>
      </tr>
      <tr>
        <td> 統計型 </td>
        <td> ● 比率、次序、加減乘除平均、分位數 </td>
      </tr>
      <tr>
        <td> 其他類型 </td>
        <td> ● 組合特徵 </td>
      </tr>
    </table> 

* [交叉驗證](https://zhuanlan.zhihu.com/p/24825503)
  * 以 cross_val_score 顯示改善效果
  * 方法
    * 留出法(holdout cross validation)
    * K 拆交叉驗證法(K fold Cross Vaildation)：將所有數據集切成 K 等分，不重複選其中一份當測試集，其他當訓練集，並計算模型在測試集上的 MSE
    * 留一法(Leave one out cross validation; LOOCV)：只用一個數據當測試集，其他全為訓練集
    * Bootstrap Sampling
* 範例與作業
  * [範例D007](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_007_HW_%E7%89%B9%E5%BE%B5%E9%A1%9E%E5%9E%8B/Day_007_Feature_Types.ipynb)
    * 以房價預測為範例，看特徵類型
  * [作業D007](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_007_Ans.ipynb)
    * 以鐵達尼生存預測為範例
    * 目標：完成三種不同特徵類型的三種資料操作，觀察其結果(何類難處理)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D008-EDA-資料分佈
* 統計量化
  * 基本統計分析方法
    * 描述性分析：總量分析、相對數分析、平均數、變異指數等
    * 趨勢概率分析：計算集中趨勢 
      * 算數平均值 Mean
      * 中位數 Median
      * 眾數 Mode
    * 離散程度分析：計算資料分散程度
      * 最小值 Min、最大值 Max、範圍 Range
      * 四分位差 Quartiles
      * 變異數 Variance
      * 標準差 Standard deviation
      * 極差、方差
  * 列表分析
  * 假設檢驗分析 
    * 分布程式：[常見統計分布](https://www.healthknowledge.org.uk/public-health-textbook/research-methods/1b-statistical-methods/statistical-distributions)
    * 參數估計(含點、區間)
    * 統程
    * 多項分析與*2檢驗
  * 多元統計分析
    * 一元線性回歸分析
    * 聚類分析，如KNN
* 視覺化
  * [python 視覺化套件](https://matplotlib.org/3.2.2/gallery/index.html)
  * [The Python Graph Gallery](https://www.python-graph-gallery.com/)
  * [Matploitlib](https://matplotlib.org/3.2.2/gallery/index.html)
  * [The R Graph Gallery](https://r-graph-gallery.com/)
  * [R Graph Gallery (Interactive plot，互動圖)](https://gist.github.com/mbostock)
* 範例與作業
  * [作業D008](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_008_Ans.ipynb) 
    * DataFrame下可用的函數
      * .mean()、median()、.sum()
      * .cumsum()：以上累積
      * .describe()：描述性統計
      * .var()、.std()
      * .skew()、.kurt()
      * .corr()、.cov()
    * [視覺化](https://pandas.pydata.org/pandas-docs/version/0.23.4/visualization.html)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D009-EDA-Outlier及處理
* 離群值、異常值([Outlier](https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba))
  * 定義
    * 數據集中有一個或一些數值與其他數值相比差異較大
    * 一個數值偏離觀測平均值的機率小於等於 1/(2n)，則該數值應當拿掉
    * 數據須符合常態分佈，如值大於3個標準差，則視為異常值
  * 可能出現的原因
    * 未知值
    * 錯誤紀錄/手誤/系統性錯誤
    * 例外情境
  * 檢查流程與方法
    * 確認每一個欄位的意義
    * 透過檢查數值範圍 (平均數、標準差、中位數、分位數(IQR)、zscore) 或畫圖(散點圖(scatter plot)、分佈圖(histogram)、直方圖、盒圖(boxplot)、次數累積分佈、[ECDF](https://medium.com/ai%E5%8F%8D%E6%96%97%E5%9F%8E/exploratory-data-analysis-%E6%8E%A2%E7%B4%A2%E8%B3%87%E6%96%99-ecdf-7fa350c32897)或其他圖)檢查是否有異常
  * 處理方法
    * 視覺化：透過分佈看出離群值
    * 新增欄位用以紀錄異常與否(人工再標註)
    * 填補 (取代)：視情況以中位數、Min、Max、平均數填補(有時會用 NA)
    * 刪除資料
    * [離群值處理參考資料](https://andy6804tw.github.io/2021/04/02/python-outliers-clean/#%E8%B3%87%E6%96%99%E8%A7%80%E5%AF%9F)
* 範例與作業
  * [範例D009](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_009_HW_outlier%E5%8F%8A%E8%99%95%E7%90%86/Day_009_outliers_detection.ipynb)
    * 計算統計值、畫圖(直方圖)來觀察離群值
    * 疑似離群值的資料移除後，看剩餘的資料是否正常
    * 利用變數類型對欄位進行篩選
      * df.dtypes 給出各欄位名稱的 Seires
      * .isin(your_list) 可以用來給出 Seires 內每個元素是否在 your_list 裡面的布林值
      * 可以用布林值的方式進行遮罩的篩選 DataFrame
  * [作業D009](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_009_outlier%E5%8F%8A%E8%99%95%E7%90%86_Ans.ipynb)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D010-EDA-去除離群值-數值型
* [離群值](https://zhuanlan.zhihu.com/p/33468998)
  * 只有少數幾筆資料跟其他數值差異很大，標準化無法處理
    * 常態標準化：Z-score = (Xi-mean(Xi))/std(Xi)
    * 最大最小化：(Xi-min(Xi))/(max(Xi)-min(Xi))，code：MinMaxScaler
    * 參考資料
      * [資料預處理- 特徵工程- 標準化](https://matters.news/@CHWang/77028-machine-learning-%E8%B3%87%E6%96%99%E9%A0%90%E8%99%95%E7%90%86-%E7%89%B9%E5%BE%B5%E5%B7%A5%E7%A8%8B-%E6%A8%99%E6%BA%96%E5%8C%96-standard-scaler-%E5%85%AC%E5%BC%8F%E6%95%99%E5%AD%B8%E8%88%87python%E7%A8%8B%E5%BC%8F%E7%A2%BC%E5%AF%A6%E4%BD%9C%E6%95%99%E5%AD%B8-bafyreihd2uc5clmc7kzzswuhvfd56axliecfzxlk5236o54cvvcphgumzu)
      * [Sklearn 套件教學](https://matters.news/@CHWang/78462-machine-learning-%E6%A8%99%E6%BA%96%E5%8C%96-standard-scaler-%E5%BF%AB%E9%80%9F%E5%AE%8C%E6%88%90%E6%95%B8%E6%93%9A%E6%A8%99%E6%BA%96%E5%8C%96-sklearn-%E5%A5%97%E4%BB%B6%E6%95%99%E5%AD%B8-bafyreibpusofl5b3tt43ovknw2mnjzrmekfldelelyl33luzkfzc4k6loy)
  * 方法：用 cross-validation 來選擇
    * 捨棄離群值：離群值數量夠少時使用
    * 調整離群值：取代
* 範例與作業
  * [範例D010](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_010_HW_%E6%95%B8%E5%80%BC%E5%9E%8B%E7%89%B9%E5%BE%B5/Day_010_Outliers.ipynb)
    * 觀察原始數值的散佈圖及線性迴歸分數(用 cross-validation score 來評估)
    * 觀察將極端值以上下限值取代，對於分布與迴歸分數的影響
    * 觀察將極端值資料直接刪除，對於分布與迴歸分數的影響
  * [作業D010](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_010_Ans.ipynb)
    * 觀察將極端值以上下限值取代，對於分布與迴歸分數的影響
    * 觀察將極端值資料直接刪除，對於分布與迴歸分數的影響

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D011-EDA-常用的數值取代
* 處理例外值：常用以填補的統計值
  <table border="1" width="26%">
      <tr>
        <th width="3%">統計值</a>
        <th width="10%">語法</a>
        <th width="3%">統計值</a>
        <th width="10%">語法</a>        
      </tr>
      <tr>
        <td> 中位數(median) </td>
        <td> np.median(value_array) </td>
        <td> 分位數(quantiles) </td>
        <td> np.quantile(value_array, q=...) </td>
      </tr>
      <tr>
        <td> 眾數(mode) </td>
        <td> scipy.stats.mode(value_array)：較慢的方法 <br>
             dictionary method：較快的方法</td>
        <td> 平均數(mean) </td>
        <td> np.mean(value_array) </td>
      </tr>
  </table>
  
* 連續數據標準化
  * 單位不同對 y 的影響完全不同
  * 模型
    * 有影響的模型：Regression model
    * 影響不大的模型：Tree-based model
  * 常用方式
    * Z 轉換：(Xi-mean(Xi))/std(Xi)  
    * 空間壓縮：將空間轉換到 Y 區間中，有時候不會使用 min/max 方法進行標準化，而會採用 Qlow/Qhigh normalization，min 改為 q1，max 改為 q99，去除極值的影響
      * Y = 0~1，(Xi-min(Xi))/(max(Xi)-min(Xi))
      * Y = -1~1，((Xi-min(Xi))/(max(Xi)-min(Xi))-0.5)*2
      * Y = 0~1，針對特別影像，Xi/255
  * 優缺點
    * 優
      * 某些演算法(如SVM、DL)等，對權眾敏感或對損失函數平滑程度有幫助者
      * 特徵間的量級差異甚大
    * 劣
      * 有些指標，如相關係數不適合在有標準化的空間進行
      * 量的單位在某些特徵上是有意義的
* 範例與作業
  * [範例D011](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_011_HW_%E5%B8%B8%E7%94%A8%E7%9A%84%E6%95%B8%E5%80%BC%E5%8F%96%E4%BB%A3/Day_011_handle_outliers.ipynb)
    * 計算並觀察百分位數：不能有缺失值
    * 計算中位數的方法：不能有缺失值
    * 計算眾數：不能有缺失值
    * 計算標準化與最大最小化
  * [作業D011](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_011_handle_outliers_Ans.ipynb)
    * 填補資料
    * 標準化與最大最小化

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D012-EDA-補缺失值與標準化-數值型
* 填補[缺失值](https://juejin.cn/post/6844903648074416141)
  * 最重要的是欄位的領域知識與欄位中的非缺數
    * 填補指定值
      * 補 0 ：空缺原本就有 0 的含意，如前頁的房間數
      * 補不可能出現的數值：類別型欄位，但不適合用眾數時
    * 填補預測值：速度較慢但精確，從其他資料欄位學得填補知識
      * 若填補範圍廣，且是重要特徵欄位時可用本方式
      * 須提防 overfitting：可能退化成為其他特徵的組合
  * 補值要點：推論分布
    * 類別型態，可視為「另一種類別」或以「眾數」填補
    * 數值型態且偏態不明顯，以「平均數」、「中位數」填補
    * 注意盡量不要破壞資料分布
* 為何要[標準化](https://blog.csdn.net/SanyHo/article/details/107514236)
  * 以合理的方式，平衡特徵間的影響力
  * 方法：將值域拉一致
    * 標準化 (Standard Scaler)：
      * 假定數值為常態分佈，適合本方式平衡特徵
      * 轉換不易受到極端值影響
    * 最小最大化 (MinMax Scaler)：
      * 假定數值為均勻分佈，適合本方式平衡特徵
      * 轉換容易受到極端值影響
  * 適合場合
    * 非樹狀模型：如線性迴歸, 羅吉斯迴歸, 類神經...等，標準化/最小最大化後，對預測會有影響
    * 樹狀模型：如決策樹, 隨機森林, 梯度提升樹...等，標準化/最小最大化後，對預測不會有影響
* 範例與作業
  * [範例D012](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_012_HW_%E8%A3%9C%E7%BC%BA%E5%A4%B1%E5%80%BC%E8%88%87%E6%A8%99%E6%BA%96%E5%8C%96/Day_012_Fill_NaN_and_Scalers.ipynb)
    * 如何查詢個欄位空缺值數量
    * 觀察替換不同補缺方式，對於特徵的影響
    * 觀察替換不同特徵縮放方式，對特徵的影響
  * [作業D012](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_012_Fill_NaN_and_Scalers_Ans.ipynb)
    * 以「鐵達尼生存預測」為例

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D013-常見的DataFrame操作
* 轉換與合併 dataframe
  <table border="1" width="26%">
      <tr>
        <th width="3%">語法</a>
        <th width="10%">用途</a>
        <th width="3%">語法</a>
        <th width="10%">用途</a>        
      </tr>
      <tr>
        <td> pd.melt(df) </td>
        <td> 將「欄(column)」轉成「列(row)」 </td>
        <td> pd.pivot(columns='欄位名稱', values='值') </td>
        <td> 將「列(row)」轉成「欄(column)」 </td>
      </tr>
      <tr>
        <td> pd.concat([df1, df2]) </td>
        <td> 沿「列(row)」合併兩個 dataframe，default：axis=0 <br>
             對應的欄位數、名稱要一致</td>
        <td> pd.concat([df1, df2], axis=1) </td>
        <td> 沿「欄(column)」合併兩個 dataframe <br> 
             可將多個表依照某欄 (key) 結合使用，default：join='outer'進行 <br>
             可調整 join 為 'inner'，僅會以單一欄為結合</td>
      </tr>
      <tr>
        <td> pd.merge(df1, df2, on='id', how='outer') </td>
        <td> 將 df1、df2 以「id」這欄做全合併(遺失以 na 補) </td>
        <td> pd.merge(df1, df2, on='id', how='inner') </td>
        <td> 將 df1、df2 以「id」這欄做部分合併，自動去除重複的欄位 </td>
      </tr>
  </table>
  
* Subset
  * 邏輯操作
    <table border="1" width="30%">
      <tr>
        <th width="5%">用途</a>
        <th width="10%">語法</a>
        <th width="5%">用途</a>
        <th width="10%">語法</a>        
      </tr>
      <tr>
        <td> 大於 / 小於 / 等於 </td>
        <td> >, <, == </td>
        <td> 大於等於 / 小於等於 </td>
        <td> >=, <= </td>
      </tr>
      <tr>
        <td> 不等於 </td>
        <td> != </td>
        <td> 邏輯的 and, or, not, xor </td>
        <td> &, |, ~, ^</td>
      </tr>
      <tr>
        <td> 欄位中包含 value </td>
        <td> df.column.isin(value) </td>
        <td> 為 Nan </td>
        <td> df.isnull(obj) </td>
      </tr>
      <tr>
        <td> 非 Nan </td>
        <td> df.notnull(obj) </td>
        <td> </td>
        <td> </td>
      </tr>
    </table>
  * 列篩選/縮減
    <table border="1" width="30%">
      <tr>
        <th width="5%">用途</a>
        <th width="10%">語法</a>
        <th width="5%">用途</a>
        <th width="10%">語法</a>        
      </tr>
      <tr>
        <td> 邏輯操作 </td>
        <td> df[df.age>20] </td>
        <td> 移除重複 </td>
        <td> df.drop_duplicates() </td>
      </tr>
      <tr>
        <td> 前 n 筆 </td>
        <td> df.head(n=10) </td>
        <td> 後 n 筆 </td>
        <td> df.tail(n=10)</td>
      </tr>
      <tr>
        <td> 隨機抽樣 </td>
        <td> df.sample(frac=0.5)   # 抽50% <br>
             df.sample(n=10)       # 抽10筆 </td>
        <td> 行第 n 到 m 筆的資料 </td>
        <td> df.iloc[n:m] </td>
      </tr>
      <tr>
        <td> 行第 n 到 m 筆且列第 a 到 b 筆的資料 </td>
        <td> df.iloc[n:m, a:b] </td>
        <td> </td>
        <td> </td>
      </tr>
    </table>
  * 欄篩選/縮減
    <table border="1" width="30%">
      <tr>
        <th width="5%">用途</a>
        <th width="10%">語法</a>
        <th width="5%">用途</a>
        <th width="10%">語法</a>        
      </tr>
      <tr>
        <td> 單一欄位 </td>
        <td> df['col1'] 或 df.col1 </td>
        <td> 複數欄位 </td>
        <td> df[['col1', 'col2', 'col3']] # </td>
      </tr>
      <tr>
        <td> Regex 篩選 </td>
        <td> df.filter(regex=...) </td>
        <td> </td>
        <td> </td>
      </tr>
    </table>
* Group operations：常用在計算「組」統計值時會用到的功能
  * 自訂：sub_df_object = df.groupby(['col1'])
  * 應用
    <table border="1" width="30%">
      <tr>
        <th width="5%">用途</a>
        <th width="10%">語法</a>
        <th width="5%">用途</a>
        <th width="10%">語法</a>        
      </tr>
      <tr>
        <td> 計算各組的數量 </td>
        <td> sub_df_object.size() </td>
        <td> 得到各組的基本統計值 </td>
        <td> sub_df_object.describe() </td>
      </tr>
      <tr>
        <td> 根據 col1 分組後，計算 col2 統計值(平均值、最大值、最小值等) </td>
        <td> sub_df_object['col2'].mean() </td>
        <td> 對依 col1 分組後的 col2 引用操作 </td>
        <td> sub_df_object['col2'].apply() </td>
      </tr>
      <tr>
        <td> 對依 col1 分組後的 col2 繪圖 (hist 為例) </td>
        <td> sub_df_object['col2'].hist() </td>
        <td> </td>
        <td> </td>
      </tr>
    </table>
    
* 參考資料
  * [Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)
* 範例與作業
  * [範例D013](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_013_HW_%E5%B8%B8%E7%94%A8%E7%9A%84%20DataFrame%20%E6%93%8D%E4%BD%9C/Day_013_dataFrame_operation.ipynb)
    * DataFrame 的黏合 (concat)
    * 使用條件篩選出 DataFrame 的子集合
    * DataFrame 的群聚 (groupby) 的各種應用方式
  * [作業D013](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_013_dataFrame_operation_Ans.ipynb) 

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D014-程式實作EDA-相關係數簡介
* 相關係數
  * 常用來了解各欄位與我們想要預測的目標之間關係的指標
  * 衡量兩個隨機變量之間線性關係的強度和方向
  * 數值介於 -1~1 之間的值，負值代表負相關，正值代表正相關，數值的大小代表相關性的強度
    * .00-.19：非常弱相關
    * .20-.39：弱相關
    * .40-.59：中度相關
    * .60-.79：強相關
    * .80-1.0：非常強相關
* 範例與作業
  * [範例D014](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_014_HW/Day_014_correlation_example.ipynb)
    * 弱相關的相關矩陣與散佈圖之間的關係
    * 正相關的相關矩陣與散佈圖之間的關係
  * [作業D014](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_014_correlation_example_Ans.ipynb)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D015-程式實作EDA-CorrelationCode
* 相關係數(搭配課程內容)
  * 功能
    * 迅速找到和預測目標最有線性關係的變數
    * 搭配散佈圖來了解預測目標與變數的關係
  * 要點
    * 遇到 y 的本質不是連續數值時，應以 y 軸方向呈現 x 變數的 boxplot (高下立見)
    * 檢視不同數值範圍的變數，且有特殊例外情況(離群值)，將 y 軸進行轉換 (log-scale)
* 範例與作業
  * [範例D015](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_015_HW_EDA%20from%20Correlation/Day_015-supplementary_correlation_and_plot_with_different_range.ipynb)
    * 直接列出的觀察方式
    * 出現異常數值的資料調整方式
    * 散佈圖異常與其調整方式
  * [作業D015](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_015_correlation_Ans.ipynb)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D016-EDA-不同數值範圍間的特徵如何檢視
* 繪圖風格：透過設計過的風格，讓觀看者更清楚明瞭，包含色彩選擇、線條、樣式等
  * 語法：詳細圖示差異搭配課程內容
    ```
    plt.style.use('default')    # 不需設定就會使用預設
    plt.style.use('ggplot')
    plt.style.use('seaborn')    # 或採用 seaborn 套件繪圖
    ```
* Kernel Density Estimation ([KDE](http://rightthewaygeek.blogspot.com/2015/09/kernel-density-estimation.html)) 
  * 步驟
    * 採用無母數方法畫出一個觀察變數的機率密度函數
      * 某個 X 出現的機率為何
    * Density plot 的特性
      * 歸一：線下面積和為 1
      * 對稱：K(-u) = K(u)
    * 常用的 kernel function
      * Gaussian esti. (Normal dist)
      * Cosine esti.
      * Triangular esti.
  * 優點
    * 無母數方法，對分布沒有假設 (使用上不需擔心是否有一些常見的特定假設，如分布為常態)
    * 透過 KDE plot，可較為清楚的看到不同組間的分布差異
  * 缺點
    * 計算量大，電腦不好可能跑不動
* 範例與作業
  * [範例D016](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_016_HW_Kernel%20Density%20Estimation_/Day_016_EDA_KDEplots.ipynb)
    * 各種樣式的長條圖(Bar)、直方圖(Histogram)
    * 不同的 KDE 曲線與繪圖設定以及切換不同 Kernel function 的效果
  * [作業D016](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_016_EDA_KDEplots_Ans.ipynb)
    * 調整對應的資料，以繪製分布圖

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D017-EDA-把連續型變數離散化
* 連續型變數離散化：變數較穩定
  * 要點：如每 10 歲一組，若不分組，outlier 會給模型帶來很大的干擾
    * 組的數量
    * 組的寬度
  * 主要方法
    * 等寬劃分(對應 pandas 的 cut)：按照相同寬度將資料分成幾等份，其缺點是受異常值的影響比較大
    * 等頻劃分(對應 pandas 的 qcut)：將資料分成幾等份，每等份資料裡面的個數是一樣的
    * 聚類劃分：使用聚類演算法將資料聚成幾類，每一個類為一個劃分
* 範例與作業
  * [範例D017](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_017_HW/Day_017_discretizing.ipynb)：數據離散化
    * pandas.cut 的等寬劃分效果
    * pandas.qcut 的等頻劃分效果
  * [作業D017](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_017_discretizing_Ans.ipynb)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D018-程式實作EDA-把連續型變數離散化
* 把連續型的變數離散化後，可以搭配 pandas 的 groupby 畫出與預測目標的圖來判斷兩者之間是否有某種關係/趨勢
* 範例與作業
  * [作業D018](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_018_Ans.ipynb)
    * 對較完整的資料生成離散化特徵
    * 觀察上述離散化特徵，對於目標值的預測有沒有幫助

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D019-程式實作-Subplots
* 使用 subplot 的時機：將圖片分格呈現，有助於資訊傳達
  * 有很多相似的資訊要呈現時 (如不同組別的比較)
  * 同一組資料，但想同時用不同的圖型呈現
* 語法：`plt.figure()` 及 `plt.subplot(列-欄-位置)`
  <table border="1" width="10%">
      <tr>
        <th width="5%">第一行</a>
        <th width="5%">第二行</a>       
      </tr>
      <tr>
        <td> plt.subplot(321)：代表在一個 3 列 2 欄的最左上角(列1欄1) </td>
        <td> plt.subplot(322) </td>
      </tr>
      <tr>
        <td> plt.subplot(323) </td>
        <td> plt.subplot(324) </td>
      </tr>
      <tr>
        <td> plt.subplot(325) </td>
        <td> plt.subplot(326) </td>
      </tr>
    </table>

* 參考資料
  * [matplotlib 官方範例](https://matplotlib.org/2.0.2/examples/pylab_examples/subplots_demo.html)
  * [Multiple Subplots](https://jakevdp.github.io/PythonDataScienceHandbook/04.08-multiple-subplots.html)
  * [Seaborn.jointplot](https://seaborn.pydata.org/generated/seaborn.jointplot.html)
* 範例與作業
  * [範例D019](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_019_HW_%E7%A8%8B%E5%BC%8F%E5%AF%A6%E4%BD%9C_subplots/Day_019_EDA_subplots.ipynb)
    * 傳統的 subplot 三碼：row、column、indx 繪製法
    * subplot index 超過 10 以上的繪圖法：透過 for loop
  * [作業D019](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_019_EDA_subplots_Ans.ipynb)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D020-程式實作-HeatmapANDGrid-plot
* Heatmap
  * 常用於呈現變數間的相關性、混和矩陣(confusion matrix)，以顏色深淺呈現
  * 亦可用於呈現不同條件下，數量的高低關係
  * 參考資料
    * [matplotlib 官方範例](https://matplotlib.org/3.2.2/gallery/images_contours_and_fields/image_annotated_heatmap.html)
    * [Seaborn 数据可视化基础教程](https://huhuhang.com/post/machine-learning/seaborn-basic)
* Grid-plot：結合 scatter plot 與 historgram 的好處來呈現變數間的相關程度
  * subplot 的延伸，但 seaborn 做得更好
    ```
    import seaborn as sns
    sns.set(style='ticks', color_codes=True)
    iris = sns.load_dataset('iris')
    g = sns.pairplot(iris)
    ```
    
    * 對角線呈現該變數的分布(distribution)
    * 非對角線呈現兩兩變數間的散佈圖
  * 參考資料
    * [Seaborn 的 Pairplot](https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166)
* 範例與作業
  * [範例D020](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_020_HW_Heatmap%20%26%20Grid-plot/Day_020_EDA_heatmap.ipynb)
    * Heatmap 的基礎用法：相關矩陣的 Heatmap
    * Heatmap 的進階用法：散佈圖、KDE、密度圖
  * [作業D020](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_020_EDA_heatmap_Ans.ipynb)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D021-模型-LogisticRegression
* A baseline
  * 最終的目的是要預測客戶是否會違約遲繳貸款的機率，在開始使用任何複雜模型之前，有一個最簡單的模型當作 baseline 是一個好習慣
* Logistic Regression
  * 參考資料：[ML-Logistic Regression-Andrew](https://www.youtube.com/watch?v=-la3q9d7AKQ&list=PLNeKWBMsAzboR8vvhnlanxCNr2V7ITuxy)
* 範例與作業
  * [範例D021](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_021_HW_Logistic%20Regression/Day_021_first_model.ipynb)
    * 資料清理
    * 前處理
    * Heatmap 的進階用法：散佈圖、KDE、密度圖
    * 輸出值的紀錄
  * [作業D021](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_021_first_model_Ans.ipynb)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

## 資料科學特徵工程技術
### D022-特徵工程簡介
* [特徵工程](https://www.zhihu.com/question/29316149)
  * 從事實到對應分數的轉換，而非直接轉換成 1 點，點數未必直接對應到總價或機率
  * 資料包含類別型(文字型)特徵以及數值型特徵，最小的特徵工程至少包含一種類別編碼(範例使用標籤編碼)，以及一種特徵縮放方法(範例使用最小最大化)
  * 從原始資料中提取有價值的資訊
* 建模語法
  * 讀取資料：df_train、df_test
  * 分解重組與轉換：將 df_train、df_test 合併為 df   
  * 特徵工程：針對 df 進行轉換
    * Label Encoder
    * MinMax Encoder
  * 訓練模型與預測
    * train_X、train_Y：訓練模型
    * test_X：模型預測，可得到 pred
  * 合成提交檔：將預測結果存成 csv 檔
  * 補充
    * 特徵重要性評估通常發生在模型後
    * 可再用來跌代調整模型
* 資料處理
  * 減少：詳見資料前處理
    * 資料清理
    * 離群值處理
  * 增加
    * 基於事實的特徵轉換
      * 標記 Label
    * 基於數值的特徵轉換
      * 標準化、區間縮放
      * 二值化(連續數值->離散類別)
* 參考資料
  * [特徵工程實例說明【Cupoy A咖共學】](https://www.youtube.com/watch?v=ZhyfKVvoK7I&t=261s)
* 範例與作業
  * [範例D022](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_022_HW_%E7%89%B9%E5%BE%B5%E5%B7%A5%E7%A8%8B%E7%B0%A1%E4%BB%8B/Day_022_Introduction_of_Feature%20Engineering.ipynb)
    * 特徵工程：補缺失值(fillna)、標籤編碼(LabelEncoder)【類別】、最小最大化(MinMaxScaler)【數值】
  * [作業D022](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_022_Introduction_of_Feature%20Engineering_Ans.ipynb)

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D023-特徵工程-數值型-去除偏態
* 去除[偏態](https://blog.csdn.net/u013555719/article/details/78530879)
  * 範例：希望當掉的同學部要太多
    * 標準化(平移)：高低分群體還是分得太明顯
    * 去離群值：高分群的努力都白費，不公平
    * 去除偏態：近似常態分佈(左右對稱、集中點在中央，讓平均值具有代表性)，如開根號乘以 10
  * 使用時機：當離群資料比例太高，或者平均值沒有代表性
  * 方法
    * 對數去偏(log1p)
      * 常見於計數/價格這類非負且可能為 0 的欄位
      * 有 0 時應加 1 (plus one) 再取對數 (log)
      * 還原時先取指數 (exp) 後再減 1 (minus one)
    * 方根去偏(sqrt)
      * 將數值減去最小值後開根號，最大值有限時適用 (如成績轉換)
    * 分布去偏(boxcox)：λ 要介於 0 到 0.5 之間，轉化前的數值不可小於等於 0
      * 函數的 lambda(λ) 參數為 0 時等於 log 函數，lambda(λ) 為 0.5 時等於開根號 (即sqrt)，因此可藉由參數的調整更靈活地轉換數值，但要特別注意 Y 的輸入數值必須要為正 (不可為0)
      * 當 λ≠0，y(λ) = (y^λ-1)/λ；當 λ=0，y(λ) = ln(y)
        <table border="1" width="9%">
          <tr>
            <th width="2%"> λ </a>
            <th width="2%"> Y </a>
            <th width="5%"> Y </a>
          </tr>
          <tr>
            <td> -2 </td>
            <td> 1/Y^2 </td>
            <td>  </td>
          </tr>
          <tr>
            <td> -1 </td>
            <td> 1/Y </td>
            <td> inverse transformation </td>
          </tr>
          <tr>
            <td> -0.5 </td>
            <td> 1/Y^(1/2) </td>
            <td>  </td>
          </tr>
          <tr>
            <td> 0 </td>
            <td> log(Y) </td>
            <td> logarithmic transformation </td>
          </tr>
          <tr>
            <td> 0.5 </td>
            <td> Y^(1/2) </td>
            <td> square root transformation </td>
          </tr>
          <tr>
            <td> 1 </td>
            <td> Y </td>
            <td> no transformation </td>
          </tr>
          <tr>
            <td> 2 </td>
            <td> Y^(2) </td>
            <td> quadratic transformation </td>
          </tr>
        </table>
       
* 範例與作業
  * [範例D023](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_023_HW_%E5%8E%BB%E9%99%A4%E5%81%8F%E6%85%8B/Day_023_Reduce_Skewness.ipynb)
    * DataSet：房價預測
    * 觀察原始數值的散佈圖及線性迴歸分數
    * 使用 log1p 降偏態時，對於分布與迴歸分數的影響
    * 使用 box-cox (λ=0.15)時，對於分布與迴歸分數的影響
    * 使用 sqrt (box-cox, λ=0.5)時，對於分布與迴歸分數的影響
  * [作業D023](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_023_Reduce_Skewness_Ans.ipynb)
    * DataSet：鐵達尼生存預測

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D024-特徵工程-類別型-基礎處理
* 類別型特徵處理
  * 標籤編碼 (Label Encoding)
    * 類似於流水號，依序將新出現的類別依序編上新代碼
    * 缺點：分數的大小順序只與轉換對照有關，與標籤的意義無直接相關
  * 獨熱編碼 (One Hot Encoding)
    * 改良數字大小沒有意義的問題，將不同的類別分別獨立為一欄
    * 缺點：需較大的記憶空間與計算時間，且類別數量越多時越嚴重
  * [比較](https://www.twblogs.net/a/5baab6e32b7177781a0e6859?lang=zh-cn)
    <table border="1" width="9%">
          <tr>
            <th width="2%">  </a>
            <th width="2%"> 儲存空間/計算時間 </a>
            <th width="5%"> 適用學習模型 </a>
          </tr>
          <tr>
            <td> 標籤編碼 (Label Encoding) </td>
            <td> 小 </td>
            <td> 非深度學習模型，如樹狀模型(隨機森林/梯度提升樹) </td>
          </tr>
          <tr>
            <td> 獨熱編碼 (One Hot Encoding) </td>
            <td> 較大 </td>
            <td> ● 深度學習模型，主要依賴倒傳遞，標籤編碼不易收斂 <br>
                 ● 當特徵重要性高，且可能值較少，才考慮</td>
          </tr>
    </table>

* 範例與作業
  * [範例D024](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_024_HW_%E9%A1%9E%E5%88%A5%E5%9E%8B%E7%89%B9%E5%BE%B5%E8%99%95%E7%90%86/Day_024_LabelEncoder_and_OneHotEncoder.ipynb)
    * DataSet：房價預測
    * 使用標籤編碼與獨熱編碼
      * 在特徵數量/線性迴歸分數/線性迴歸時間上，有什麼影響
      * 在特徵數量/梯度提升樹分數/梯度提升樹時間上，有什麼影響
  * [作業D024](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_024_LabelEncoder_and_OneHotEncoder_Ans.ipynb)    
    * DataSet：鐵達尼生存預測
    * 使用標籤編碼與獨熱編碼
      * 在特徵數量/邏輯斯迴歸分數/邏輯斯迴歸時間上，有什麼影響

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D025-特徵工程-類別型-均值編碼
* [均值編碼](https://zhuanlan.zhihu.com/p/26308272)：用 cross validation 確認使用前後分數
  * 類別特徵看起來與目標值有顯著相關，容易 overfitting
  * 使用同一類別目標的平均值取代原本的類別型特徵
    * 平滑化 (Smoothing)
      * 交易樣本非常少，且剛好抽到極端值，平均結果可能會誤差很大
      * 問題：需考慮紀錄筆數，當作可靠度的參考
        * 當平均值的可靠度低時，相信全部的總平均
        * 當平均值的可靠度高時，相信類別的平均
        * 依照紀錄筆數，在上述兩者間取折衷
      * 公式
        * 新類別均值 = (原類別平均*類別樣本數+全部的總平均*調整因子)/(類別樣本數+調整因子)
        * 其中，調整因子：依總樣本數調整
* 範例與作業
  * [範例D025](https://github.com/sueshow/Python_ML-Marathon/tree/main/Homework/Day_025_HW_%E9%A1%9E%E5%88%A5%E5%9E%8B%E7%89%B9%E5%BE%B5%E5%9D%87%E5%80%BC%E7%B7%A8%E7%A2%BC)
    * DataSet：房價預測
    * 使用標籤編碼與均值編碼
      * 在特徵數量/線性迴歸分數/線性迴歸時間上，有什麼影響
      * 在特徵數量/梯度提升樹分數/梯度提升樹時間上，有什麼影響
  * [作業D025](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_025_Mean_Encoder_Ans.ipynb)
    * DataSet：鐵達尼生存預測

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D026-特徵工程-類別型-其他進階處理
* 計數編碼(Counting)
  * 情境：若類別的目標均價與類別筆數呈正相關 (或負相關或高度相關)，也可將筆數本身當成特徵，如自然語言處理時，字詞的計數編碼又稱詞頻
  * 計算方式：計算類別在資料中出現的次數
* 特徵雜湊([Feature Hash](https://blog.csdn.net/laolu1573/article/details/79410187))
  * 情境：相異類別的數量非常龐大，且可能產生新的類別
  * 計算方式
    * 將原始值 hash 後取餘數
    * 記錄到 hash table
    * hash table 的長度就是增加的特徵數目
      ```
      function hashing_vectorizer(features : array of string, N : integer):
          x := new vector[N]
          for f in features:
              h := hash(f)
              x[h mod N] += 1
          return x
      ```
  * 方法
    * 將類別由[雜湊函數](https://en.wikipedia.org/wiki/Hash_function)定應到一組數字
    * 調整雜湊函數對應值的數量
    * 在計算空間/時間與鑑別度間取折衷
    * 提高訊息密度，減少無用的標籤
  * 優缺點
    * 優點
      * 相較於 one-hot encoding，不需要預先維護一個變量表
      * 可以處理新的類別
    * 缺點
      * 可能會把多個原始類別值 hash 到相同的位置上，出現碰撞 (hash collision)，不過實際實驗表明這種衝突對算法的精度影響很小。將高維稀疏的解空間壓縮到低維稠密的空間，壓縮到的維度越低，越容易發生碰撞，此時可通過使用多個 hash function 來減少碰撞造成的信息損失
* bag-of-words
  * 方法：蒐集所有的詞彙當成特徵，每個句子在有出現的詞彙特徵標 1
  * 缺點：當句子的詞彙不固定時,特徵會就爆炸性成長，且也無法處理新出現的詞彙。而且資料分佈可能很稀疏
* 嵌入式編碼(Embedding)：與深度學習相關，在此課程不談
* 範例與作業
  * [範例D026](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_026_HW_%E9%A1%9E%E5%88%A5%E5%9E%8B%E7%89%B9%E5%BE%B5%E9%80%B2%E9%9A%8E%E8%99%95%E7%90%86/Day_026_CountEncoder_and_FeatureHash.ipynb)
    * DataSet：鐵達尼生存預測，欄位：Ticket
    * 使用計數編碼，搭配邏輯斯迴歸對於測結果有什麼影響
    * 使用雜湊編碼及計數編碼+雜湊編碼，搭配邏輯斯迴歸對於測結果有什麼影響
  * [作業D026](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_026_CountEncoder_and_FeatureHash_Ans.ipynb)
    * DataSet：鐵達尼生存預測，欄位：Cabin

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D027-特徵工程-時間型
* 時間特徵分解
  * 年、月、日、時、分、秒、第幾周、星期幾
  * 週期循環特徵：關鍵在於首尾相接，須使用正弦函數(sin)或餘弦函數(cos)加以組合
    <table border="1" width="9%">
          <tr>
            <th width="2%"> 週期 </a>
            <th width="5%"> 相關性 </a>
            <th width="3%"> 正負意義 </a>
            <th width="5%"> 關係 </a>
          </tr>
          <tr>
            <td> 年 </td>
            <td> (季節)與春夏秋冬季節溫度相關 </td>
            <td> 正：冷/負：熱 </td>
            <td> cos((月/6 + 日/180 )π) </td>
          </tr>
          <tr>
            <td> 月 </td>
            <td> 與薪水、繳費相關 </td>
            <td>  </td>
            <td>  </td>
          </tr>
          <tr>
            <td> 周 </td>
            <td> (例假日)與周休、消費習慣相關 </td>
            <td> 正：精神飽滿/負：疲倦 </td>
            <td> sin((星期幾/3.5 + 小時/84 )π) </td>
          </tr>
          <tr>
            <td> 日 </td>
            <td> (日夜與生活作息)與生理時鐘相關 </td>
            <td> 正：精神飽滿/負：疲倦 </td>
            <td> sin((小時/12 + 分/720 + 秒/43200 )π) </td>
          </tr>
    </table>
 
  * 時段特徵
    * 短暫時段內的事件計數，也可能影響事件發生的機率，如：網站銷售預測，點擊網站前 10分鐘/1小時/1天的累計點擊量
  * Python 使用 datetime 
    * [時間日期處理](https://wklken.me/posts/2015/03/03/python-base-datetime.html)
    * [Basic date and time types](https://docs.python.org/3/library/datetime.html)
* 範例與作業
  * [範例D027](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_027_HW_%E6%99%82%E9%96%93%E5%9E%8B%E7%89%B9%E5%BE%B5/Day_027_DayTime_Features.ipynb)
    * DataSet：計程車費率預測，欄位：pickup_datetime
    * 觀察時間特徵分解，在線性迴歸分數/梯度提升樹分數上，分別有什麼影響
    * 觀察加入週期循環特徵，在線性迴歸分數/梯度提升樹分數上，分別有什麼影響
  * [作業D027](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_027_DayTime_Features_Ans.ipynb)
    * DatSet：計程車費率預測，欄位：fire_amount
    * 新增特徵：星期幾(day of week)與第幾周(week of year)

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D028-特徵工程-數值與數值組合
* 合成特徵(synthetic feature)
  * 起點+終點位置：方向、距離
    * 經緯度 -> 平面座標距離 -> 資料緯度集中在 40.75 度附近，可以算得經度與緯度代表的長度比為 cos(40.75度):1 = 0.75756:1，由此校正兩地距離
  * 多個頂點位置：周長、面積
  * 開始時間+結束時間：經過時長
  * 長 x 寬：面積(如iris資料集的花瓣長、寬)
* 特徵組合(feature cross)：對分線性規律進行編碼
  * 種類
    * [A X B]：將兩個特徵的值相乘形成特徵組合
    * [A x B x C x D x E]：將五個特徵的值相乘形成特徵組合
    * [A x A]：將單個特徵的值求平方形成特徵組合
  * 組合獨熱矢量
* 參考資料
  * [特徵組合&特徵交叉 (Feature Crosses)](https://segmentfault.com/a/1190000014799038)
  * [簡單高效的組合特徵自動挖掘框架](https://read01.com/jj8em6E.html#.ZBv4VnZBy5c)
* 範例與作業
  * [範例D028](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_028_HW_%E7%89%B9%E5%BE%B5%E7%B5%84%E5%90%88_%E6%95%B8%E5%80%BC%E8%88%87%E6%95%B8%E5%80%BC%E7%B5%84%E5%90%88/Day_028_Feature_Combination.ipynb)
    * DatSet：計程車費率預測
    * 增加特徵：經度差與緯度差，觀察線性迴歸與梯度提升樹的預測結果有什麼影響
    * 增加座標距離特徵，觀察線性迴歸與梯度提升樹的預測結果有什麼影響
  * [作業D028](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_028_Feature_Combination_Ans.ipynb)
    * DatSet：計程車費率預測
    * 增加特徵：經緯度一圈的長度比，觀察影響性 

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D029-特徵工程-類別與數值組合
* 群聚編碼(Group by Encoding)
  * 數值型特徵之間能合成新特徵，而類別與數值型之間也能類似均值編碼的概念，如取類別型中的平均值(Mean)、中位數(Median)、眾數(Mode)、最大值(Max)、最小值(Min)、次數(Count)等可取代類別作為編碼
  * 重要性(D031)
    * 與數值特徵組合相同，先以「領域知識」或「特徵重要性」挑選強力特徵後，再將特徵組成更強的特徵，兩個特徵都是數值就用特徵組合，其中之一是類別型就用聚類編碼
    * 機器學習的特徵是「寧濫勿缺」的，因為以前以非樹狀模型為主，為了避免共線性，會很注意類似的特徵不要增加太多，但現在強力的模型都是樹狀模型，所以只要有可能就通通做特徵囉！
  * 與均值編碼 (Mean Encoding) 的比較
    <table border="1" width="9%">
          <tr>
            <th width="3%"> 名稱 </a>
            <th width="3%"> 均值編碼 Mean Encoding </a>
            <th width="3%"> 群聚編碼 Group by Encoding </a>
          </tr>
          <tr>
            <td> 平均對象 </td>
            <td> 目標值 </td>
            <td> 其他數值型特徵 </td>
          </tr>
          <tr>
            <td> 過擬合 Overfitting </td>
            <td> 容易 </td>
            <td> 不容易 </td>
          </tr>
          <tr>
            <td> 對均值平滑化 Smoothing </td>
            <td> 需要 </td>
            <td> 不需要 </td>
          </tr>
    </table>
  * 參考資料
    * [利用python数据分析之数据聚合与分组](https://zhuanlan.zhihu.com/p/27590154)
* 範例與作業
  * [範例D029](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_029_HW_%E7%89%B9%E5%BE%B5%E7%B5%84%E5%90%88_%E9%A1%9E%E5%88%A5%E8%88%87%E6%95%B8%E5%80%BC%E7%B5%84%E5%90%88/Day_029_GroupBy_Encoder.ipynb)
    * DataSet：房價預測
    * 了解群聚編碼的語法
    * 觀察群聚編碼，搭配線性迴歸以及隨機森林分別有什麼影響
  * [作業D029](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_029_GroupBy_Encoder_Ans.ipynb)
    * DataSet：鐵達尼生存預測
    * 挑選特徵的群聚編碼
    * 觀察群聚編碼，搭配邏輯迴歸，看看有什麼影響

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D030-特徵選擇
* 特徵選擇
  * 概念：特徵需適當的增加與減少，以提升精確度並減少計算時間
    * 增加特徵：特徵組合(Day028)、群聚編碼(Day029)
    * 減少特徵：特徵選擇(Day030)
  * 方法
    * 過濾法(Filter)：選定統計數值與設定門檻，刪除低於門檻的特徵
      * 相關係數過濾法
        * 找到目標值後，觀察其他特徵與目標值的相關係數
        * 預設顏色越紅表越正相關，越藍越負相關
        * 刪除顏色較淺的特徵：訂出相關係數門檻值，特徵相關係數絕對值低於門檻者刪除
    * 包裝法(Wrapper)：根據目標函數，逐步加入特徵或刪除特徵
    * 嵌入法(Embedded)：使用機器學習模型，根據擬合後的係數，刪除係數低於門檻的特徵
      * L1(Lasso)嵌入法
        * 使用 Lasso Regression 時，調整不同的正規化程度，就會自然使得一部分的特徵係數為０，因此刪除的是係數為０的特徵，不須額外指定門檻，但需調整正規化程度
      * GDBT(梯度提升樹)嵌入法
        * 使用梯度提升樹擬合後，以特徵在節點出現的頻率當作特徵重要性，以此刪除重要性低於門檻的特徵
        * 由於特徵重要性不只可以刪除特徵，也是增加特徵的關鍵參考
  * 比較
    <table border="1" width="12%">
          <tr>
            <th width="3%"> 方法 </a>
            <th width="3%"> 計算時間 </a>
            <th width="3%"> 共線性 </a>
            <th width="3%"> 特徵穩定性 </a>
          </tr>
          <tr>
            <td> 相關係數過濾法 </td>
            <td> 快速 </td>
            <td> 無法排除 </td>
            <td> 穩定 </td>
          </tr>
          <tr>
            <td> Lasso 嵌入法 </td>
            <td> 快速 </td>
            <td> 能排除 </td>
            <td> 不穩定 </td>
          </tr>
          <tr>
            <td> GDBT 嵌入法 </td>
            <td> 較慢，Xgboost等改善計算時間，成為特徵選擇的主流 </td>
            <td> 能排除 </td>
            <td> 穩定 </td>
          </tr>
    </table>
  * 參考資料
    * [特徵選擇](https://zhuanlan.zhihu.com/p/32749489)
* 範例與作業
  * [範例D030](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_030_HW_%E7%89%B9%E5%BE%B5%E9%81%B8%E6%93%87/Day_030_Feature_Selection.ipynb)
    * DataSet：房價預測
    * 觀察相關係數過濾法對線性迴歸與梯度提升機有什麼影響
    * 觀察 L1 嵌入法對線性迴歸與梯度提升機有什麼影響
  * [作業D030](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_030_Feature_Selection_Ans.ipynb)
    * DataSet：鐵達尼生存預測
    * 相關係數過濾法有時候確實能成功提升準確度，但篩選過頭會有反效果
    * 相關係數過濾法的門檻、L1 Embedding 的 alpha 值(去除0)沒有一定的決定準則

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D031-特徵評估
* 樹狀模型的特徵重要性
  * 特徵重要性預設方式是取「特徵決定分支的次數」，範例中坪數x1次、房間數x3次、靠近捷運站x2次、屋齡x1次，故最重要的特徵是「房間數」
  * 更直覺的特徵重要性：特徵覆蓋度、損失函數降低量
    * 範例中，坪數與房間數的覆蓋度相同 (都是8)
    * 損失函數降低量則看損失函數 (loss function) 決定
  * 比較
    <table border="1" width="15%">
          <tr>
            <th width="3%"> 方法 </a>
            <th width="3%"> Xgboost對應參數 (importance_type) </a>
            <th width="3%"> 計算時間 </a>
            <th width="3%"> 估計精確性 </a>
            <th width="3%"> Sklearn 有此功能 </a>
          </tr>
          <tr>
            <td> 分支次數 </td>
            <td> weight </td>
            <td> 最快 </td>
            <td> 最低 </td>
            <td> 有 </td>
          </tr>
          <tr>
            <td> 分支覆蓋度 </td>
            <td> cover </td>
            <td> 快 </td>
            <td> 中 </td>
            <td> 無 </td>
          </tr>
          <tr>
            <td> 損失降低量(資訊增益度) </td>
            <td> gain </td>
            <td> 較慢 </td>
            <td> 最高 </td>
            <td> 無 </td>
          </tr>
    </table>
* [機器學習](https://juejin.cn/post/6844903517799317511)中的優化循環
  * 原始特徵 → 進階版 GDBT 模型擬合 → 用特徵重要性增刪特徵【特徵選擇(刪)、特徵組合(增)】 → 交叉驗證(cross validation)確認特徵效果是否改善 → (Y) 進階版 GDBT 模型擬合、(N) 用特徵重要性增刪特徵
* 比較特徵 vs. 排列重要性
  <table border="1" width="15%">
          <tr>
            <th width="2%"> 比較類型 </a>
            <th width="5%"> 特徵重要性 Feature Importance </a>
            <th width="5%"> 排序重要性 Permutation Importance </a>
          </tr>
          <tr>
            <td> 適用模型 </td>
            <td> 限定樹狀模型 </td>
            <td> 機器學習模型 </td>
          </tr>
          <tr>
            <td> 計算原理 </td>
            <td> 樹狀模型的分歧特徵 </td>
            <td> 打散原始資料中單一特徵的排序 </td>
          </tr>
          <tr>
            <td> 額外計算時間 </td>
            <td> 較短 </td>
            <td> 較長 </td>
          </tr>
  </table>
 
* 範例與作業
  * [範例D031](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_031_HW_%E7%89%B9%E5%BE%B5%E8%A9%95%E4%BC%B0/Day_031_Feature_Importance.ipynb)
    * DataSet：房價預測
    * 使用擬合過的模型，計算特徵重要性
    * 對照原始特徵，觀察特徵重要性較高的一半特徵，搭配隨機森林對於預測結果的影響
    * 重組重要性最高的特徵作為新特徵，觀察效果如何
  * [作業D031](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_031_Feature_Importance_Ans.ipynb)
    * DataSet：鐵達尼生存預測

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D032-特徵優化-分類型-葉編碼
* 分類預測的集成
  * 分類的預測結果，在意義上是對的預估機率
  * 範例：估計鐵達尼號上的生存機率，已知來自法國的旅客生存機率是 0.8，且年齡 40 到 50 區間的生存機率也是 0.8，那麼同時符合兩種條件的旅客，生存機率應該是多少呢？
    * 邏輯斯迴歸 (logistic regression) 可理解成「線性迴歸 + Sigmoid 函數」，而「Sigmoid 函數」理解成「成功可能性與機率的交換」，可能性正表示更可能，負表示較不可能
    * 使用 Sigmoid 反函數將機率重新轉為可能性的值，相加完再用 Sigmoid 函數轉回機率，以此例為例，最後加成的結果是介於 0.9 到 1 之間的機率
* 葉編碼 (leaf encoding) 原理
  * 樹狀模型做出預測時，會將資料重新分成好幾個區塊，決策樹最末端的點稱之為葉點，每個葉點的性質接近，可視為資料的一種分組方式
  * 雖然不適合直接沿用樹狀模型機率，但分組方式有代表性，因此按照葉點將資料「離散化」，比之前提過的離散化方式更精確，這樣的編碼稱為「葉編碼」
  * 葉編碼 (leaf encoding) 是採用決策樹的葉點作為編碼依據重新編碼，每棵樹視為一個新特徵，每個新特徵均為分類型特徵，決策樹的葉點與該特徵標籤一一對應，最後再以邏輯斯迴歸(重新賦予機率)或分解機 (Factorization Machine) 進行合併預測
  * 說明
    * 假設有 300 棵樹，每棵數有3片葉
      * 3 => 第 i 筆資料在第 1 棵樹位於第 3 片葉子
      * 2 => 第 i 筆資料在第 2 棵樹位於第 2 片葉子
      * ...
      * 1 => 第 i 筆資料第 300 棵樹位於第 1 片葉子
        > gbdt.apply(train_X)[:, :, 0][0] 
    * 代表第 1 筆資料在 300 棵樹分別的 index 為 [2, 1,...., 0]
    * 將每一棵樹的葉子進行 one-hot encoding，新特徵的長度等於所有樹的葉子數量相加
      > [0,0,1, 0,1,0, ... ,1,0,0] 共有 300*3=900 elements
    * 將上述結果當成新的 input，進行 logistic regression
* 葉編碼 + 邏輯斯迴歸
  * 葉編碼需要先對樹狀模型擬合後才能生成，如已挑選較佳的參數，後續處理效果也會較好
  * 在分類預測中使用樹狀模型，再對這些擬合完的樹狀模型進行葉編碼+邏輯斯迴歸，通常會將預測效果再進一步提升
* 參考資料
  * [Feature transformations with ensembles of trees](https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html)
  * [Algorithm-GBDT Encoder](https://zhuanlan.zhihu.com/p/31734283)
  * [GBDT + LR for Binary Classification](https://towardsdatascience.com/next-better-player-gbdt-lr-for-binary-classification-f8dc6f32628e)
* 範例與作業
  * [範例D032](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_032_HW_%E7%89%B9%E5%BE%B5%E5%84%AA%E5%8C%96_%E8%91%89%E7%B7%A8%E7%A2%BC/Day_032_Leaf_Encoding.ipynb)
    * DataSet：鐵達尼生存預測
    * 了解葉編碼搭配梯度提升樹的效果
    * 觀察葉編碼搭配邏輯斯迴歸的效果
  * [作業D032](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_032_Leaf_Encoding_Ans.ipynb)
    * DataSet：鐵達尼生存預測
    * 觀察葉編碼搭配隨機森林的效果
    * 觀察葉編碼搭配邏輯斯迴歸的效果
    
Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>


## 機器學習基礎模型建立
### D033-機器如何學習
* 機器學習三步驟
  * 定義好模型 (線性迴歸、決策樹、神經網路等)
    * 機器學習模型中會有許多參數 (parameters)，如線性迴歸中的 w (weights) 跟 b (bias)
    * 希望模型產生的 ŷ 跟真實答案的 y 越接近越好
  * 評估模型的好壞
    * 定義一個目標函數 (Objective function) 也可稱作損失函數 (Loss function)，來衡量模型的好壞
    * 線性迴歸模型我們可以使用均方差 (mean square error) 來衡量
    * Loss 越大，代表這組參數的模型預測出的 ŷ 越不準
  * 找出讓訓練目標最佳的模型參數
    * 梯度下降 (Gradient Descent)、增量訓練 (Additive Training) 等演算法，可找到最佳可能模型的參數
  * 模型訓練的目標是將損失函數的損失降至最低
    <table border="1" width="12%">
          <tr>
            <th width="2%"> 類型 </a>
            <th width="5%"> 中文 </a>
            <th width="5%"> 說明 </a>
          </tr>
          <tr>
            <td> Underfitted </td>
            <td> 欠擬合</td>
            <td> 預測未擬合實際結果 </td>
          </tr>
          <tr>
            <td> Good Fit/Robust </td>
            <td>  </td>
            <td> 預測可擬合實際結果 </td>
          </tr>
          <tr>
            <td> Overfitted </td>
            <td> 過度擬合 </td>
            <td> 模型可能學習到資料中的噪音，導致在實際應用時預測失準 </td>
          </tr>
  </table>
  
* 學習曲線 Learning curve
  * 如何知道模型已經過擬合了?
    * 觀察模型對於訓練資料的誤差與測試資料的誤差，是否有改變的趨勢
  * 解決方法
    * 過擬合
      * 增加資料量
      * 降低模型複雜度
      * 使用正規化 (Regularization)：決策樹模型是非常容易過擬合的模型，必須透過適當的正規化來緩解
    * 欠擬合
      * 增加模型複雜度
      * 減輕或不使用正規化
* 參考資料
  * [利用學習曲線診斷模型的偏差和方差](http://bangqu.com/yjB839.html)
* 範例與作業
  * [作業D033](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_033_Ans.ipynb)
    * 影片：[ML Lecture 1:Regression - Case Study](https://www.youtube.com/watch?v=fegAeph9UaA)
  
Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D034-訓練AND測試集切分
* 為何需要切分訓練/測試集：透過驗證/測試集評估模型是否過擬合
* 方法：使用 Python 中 Scikit-learn 進行資料切分，透過 [train-test split 函數](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)快速對資料進行切分
* K-fold Cross-validation
  * 若僅做一次訓練/測試集切分，有些資料會沒有被拿來訓練過，因此後續就有 cross-validation 的方法，可以讓結果更為穩定，Ｋ為 fold 數量
  * 每筆資料都曾經當過一次驗證集，再取平均得到最終結果
  * 方法：使用 Python 中 Scikit-learn 進行 Cross-validation，透過 [KFold 函數](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)快速運用 Cross-validation
* 參考資料
  * [ML Lecture 2: Where does the error come from?](https://www.youtube.com/watch?v=D_S6y0Jm6dQ&embeds_euri=https%3A%2F%2Fwww.cupoy.com%2F&source_ve_path=MjM4NTE&feature=emb_title)
* 範例與作業
  * [範例D034](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_034_HW/Day_034_train_test_split.ipynb)
    * 使用 train_test_split 函數進行切分
    * 使用 K-fold Cross-validation 來切分資料
  * [作業D034](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_034_Ans.ipynb)
    * 切出固定大小的資料集：適用於 unblance data
  
Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D035-RegressionVSClassification
* Regression：目標值為實數 (-∞ 至 ∞)
  * 回歸問題是可以轉化為分類問題，模型原本是直接預測身高 (cm)，可更改為預測是否高於中位數 (yes or no)
* Classification：目標值為類別 (0 或 1)
  * 二元分類 (Binary-class)
    * 目標的類別僅有兩個，如詐騙分析 (詐騙用戶 vs. 正常用戶)、瑕疵偵測 (瑕疵 vs. 正常) 
  * 多元分類 (Multi-class)
    * 目標類別有兩種以上，如手寫數字辨識有 10 個類別 (0~9)
* Multi-class vs. Multi-label
  * 當每個樣本都只能歸在一個類別，我們稱之為多分類 (Multi-class) 問題
  * 而一個樣本如果可以同時有多個類別，則稱為多標籤 (Multi-label)
* Regularization
  * 不需要考慮 bias，要考慮讓 function 越平滑僅需要考慮 weight
* Variance vs. Bias
  * Variance：資料分散程度
  * Bias：與目標的誤差程度 → 是否瞄準
  * 模型：希望選擇 bias 和 variance 都小的 model
    * 簡單模型：large bias、small variance，function space 小
    * 複雜模型：small bias、large variance，function space 大
    * overfitting：error 來自於 variance 很大
      * 在 training data 上得到小的 error，在 testing data 上得到大的 error
      * 解決方法：
        * 增加資料量
        * Regularization：新加 term(參數越少越好) 讓 function 平滑 → 會傷害 bias
    * underfitting：error 來自於 bias 很大
      * model 無法 fit examples
      * 解決方法：redesign model
        * 增加 feature as input
        * 使用更複雜的 model
* 參考資料
  * [ML Lecture 1:Regression - Case Study](https://www.youtube.com/watch?v=fegAeph9UaA)
  * [ML Lecture 2: Where does the error come from?](https://www.youtube.com/watch?v=D_S6y0Jm6dQ&embeds_euri=https%3A%2F%2Fwww.cupoy.com%2F&source_ve_path=MjM4NTE&feature=emb_title)
  * [Supervised classification和Regression的比較](http://zylix666.blogspot.com/2016/06/supervised-classificationregression.html)
* 範例與作業
  * [作業D035](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_035_Ans.ipynb)
    * 無程式撰寫
  
Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D036-評估指標選定EvaluationMetrics
* 評估指標
  * 最常見的為準確率 (Accuracy) = 正確分類樣本數/總樣本數
  * 迴歸
    * 目的：觀察「預測值」 (Prediction) 與「實際值」 (Ground truth) 的差距
    * 指標
      * MAE, Mean Absolute Error, 範圍：[0, ∞]
      * MSE, Mean Square Error, 範圍：[0, ∞]
      * [常用]R-square, 範圍：[0, 1] 
  * 分類
    * 目的：觀察「預測值」 (prediction) 與「實際值」 (Ground truth) 的正確程度
    * 指標
      * [二元、不平衡]AUC, Area Under ROC Curve, 範圍：[0, 1]
        * 通常分類問題都需要定一個閾值 (threshold) 來決定分類的類別 (通常為機率 > 0.5 判定為 1, 機率 < 0.5 判定為 0)
        * AUC 是衡量曲線下的面積，因此可考量所有閾值下的準確性
        * AUC 越大則表示模型的效能越好
      * [不平衡]F1 - Score (Precision, Recall), 範圍：[0, 1] 
        * 分類問題中，有時會對某一類別的準確率特別有興趣，如瑕疵/正常樣本分類，希望任何瑕疵樣本都不能被漏掉
        * F1-Score 是 Precision、Recall 的調和平均數
          * Precision
            * 模型判定瑕疵，佔樣本確實為瑕疵的比例 
            * 白話：被分類器挑選(selected)出來的正體樣本究竟有多少是真正的樣本
            * = true positives/(true+false positives)
          * Recall
            * 模型判定的瑕疵，佔樣本所有瑕疵的比例
            * 白話：在全部真正的樣本裡面分類器選了多少個
            * = true positives/(true positives + false negative)
      * 混淆矩陣 (Confusion Matrix)
        * 縱軸為模型預測，橫軸為正確答案，可清楚看出每個 Class 間預測的準確率，完美的模型就會在對角線上呈現 100 % 的準確率
      * [平衡]Accuracy
      * [多元]top-k accuracy
        * k 代表模型預測前 k 個類別有包含正確類別即為正確 (ImageNet 競賽通常都是比 [Top-5 Accuracy](https://www.zhihu.com/question/36463511))
    * 比較
      * AUC 計算時 y_pred 的值必須填入每個樣本的預測機率 (probability) 而非分類結果
      * F1-Score 計算時則需填入每個樣本已分類的結果，如機率 >=0.5 則視為 1，而非填入機率值
* 參考資料
  * [深入了解超常用的指標 AUC](https://www.dataschool.io/roc-curves-and-auc-explained/)
  * [機器學習模型評估](https://zhuanlan.zhihu.com/p/30721429)
* 範例與作業
  * [範例D036](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_036_HW/Day_036_evaluation_metrics.ipynb)
    * 觀察各指標的數值範圍，及輸入函數中的資料格式
  * [作業D036](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_036_Ans.ipynb)
    * F1-Score 其實是 F-Score 中的 β 值為 1 的特例
    * 撰寫 F2-Score 的程式碼
  
Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D037-RegressionModel-線性迴歸AND羅吉斯回歸
* 線性迴歸模型(Linear Regression)
  * 特性
    * 用於迴歸問題
    * 訓練速度非常快，但須注意資料共線性、資料標準化等限制
    * 可作為 baseline 模型作為參考點
* 羅吉斯回歸模型(Logistics Regression)
  * 特性
    * 用於二元分類模型，將線性迴歸加上 Sigmoid 函數
* 範例與作業
  * [作業D037](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_037_Ans.ipynb)
    * 無程式撰寫
  
Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D038-程式實作-線性迴歸AND羅吉斯回歸
* 線性迴歸
  * 語法
    > from sklearn.linear_model import LinearRegression <br>
    > reg = LinearRegression().fit(X, y)
* 羅吉斯回歸
  * 語法
    > from sklearn.linear_model import LogisticRegression <br>
    > reg = LogisticRegression().fit(X, y)
  * 參數
    * Penalty：'L1'、'L2'。使用 L1 或 L2 的正則化參數，後續有更詳細介紹
    * C：正則化的強度，數字越小，模型越簡單
    * [Solver](https://blog.csdn.net/lc574260570/article/details/82116197)：對損失函數不同的優化方法
    * Multi-class：選擇 one-vs-rest 或 multi-nominal 分類方式，當目標是 multi-class 時要特別注意，若有 10 個 class， ovr 是訓練 10 個二分類模型，第一個模型負責分類 (class1, non-class1)；第二個負責 (class2, non-class2)，以此類推
* 範例與作業
  * [範例D038](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_038_HW_regression%20model/Day_038_regression_model.ipynb)
    * 理解 linear regression 的參數意義
    * 觀察 linear regression 與 logistic regression 有什麼差異
  * [作業D038](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_038_Ans.ipynb)
    * 確定資料集的目標是分類還是迴歸，選擇正確的模型

Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D039-RegressionModel-LASSO回歸ANDRidge回歸
* 目標函數中重要的知識
  * 損失函數 (Loss function)：衡量預測值與實際值的差異，讓模型能往正確的方向學習
  * 正則化 (Regularization)：
    * 說明
      * 可懲罰模型的複雜度，避免模型變得過於複雜，造成過擬合 (Over-fitting)，當模型越複雜時其值就會越大
      * 希望模型的參數數值不要太大，原因是參數的數值變小，噪音對最終輸出的結果影響越小，提升模型的泛化能力，但也讓模型的擬合能力下降
    * 類型
      * L1 函數：$$ \alpha \sum |weights| $$
      * L2 函數：$$ \alpha \sum (weights)^2 $$
  * 為了避免 Over-fitting，目標函數 = 損失函數 + 正則化
* LASSO 回歸
  * Linear Regression 加上 L1，其中有個超參數 α 可以調整正則化的強度
  * L1 regularization 會讓模型變得較為稀疏，除了能做特徵選取外，也會讓模型變得更輕量，速度較快
* Ridge 回歸
  * Linear Regression 加上 L2，其中有個超參數 α 可以調整正則化的強度
* 範例與作業
  * [作業D039](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_039_Ans.ipynb)
    * 瞭解 L1、L2 的意義與差異
    * 理解 LASSO 與 Ridge 之間的差異與使用情境

Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D040-程式實作-LASSO回歸ANDRidge回歸
* LASSO回歸
  * 語法
    > from sklearn.linear_model import Lasso <br>
    > reg = Lasso(alpha=0.1) <br>
    > reg.fit(X, y) <br>
    > print(reg.coef_) # 印出訓練後的模型參數
* Ridge回歸
  * 語法
    > from sklearn.linear_model import Ridge <br>
    > reg = Ridge (alpha=0.1) <br>
    > reg.fit(X, y) <br>
    > print(reg.coef_) # 印出訓練後的模型參數
* 範例與作業
  * [範例D040](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_040_HW/Day_040_lasso_ridge_regression.ipynb) 
    * 資料集：糖尿病
    * 比較 linear regression 與 LASOO 的差異
  * [作業D040](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_040_Ans.ipynb)
    * 資料集：boston
    * 比較 linear regression 與 ridge 的差異

Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D041-TreeBasedModel-決策樹DecisionTree
* 決策樹 Decision Tree
  * 透過一系列的是非問題，將資料進行切分
  * 從訓練資料中找出規則，讓每一次決策能使訊息增益 (Information Gain) 最大化，訊息增益越大代表切分後的兩群資料，群內相似程度越高
    * 吉尼係數(Gini-Index)
      $$ Gini = 1-\sum_j p_j^2 $$
    * 熵(Entropy)
      $$ Entropy = -\sum_j p_j log_2 P_j$$
  * 從構建樹的過程中，透過 feature 被用來切分的次數，來得知哪些 features 是相對有用的，實務上可以使用 feature importance 來了解模型如何進行分類，總和為 1
  * 可視覺化每個決策的過程，是個具有非常高解釋性的模型
* 範例與作業
  * [作業D041](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_041_Ans.ipynb)
    * 無程式撰寫

Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D042-程式實作-決策樹
* 類型
  * 回歸型
    > from sklearn.tree_model import DecisionTreeRegressor
  * 分類型
    > from sklearn.tree_model import DecisionTreeClassifier <br>
    > clf = DecisionTreeClassifier()
* 超參數
  * Criterion：衡量資料相似程度的 metric
  * Max_depth：樹能生長的最深限制
  * Min_samples_split：至少要多少樣本以上才進行切分
  * Min_samples_leaf：最終的葉子 (節點) 上至少要有多少樣本
    ```
    from sklearn.tree import DecisionTreeClassifier
    clf = DecisionTreeClassifier(
                  criterion = 'gini',
                  max_depth = None,
                  min_samples_split = 2,
                  min_samples_leaf = 1
          )
    # feature importance
    clf.feature_importances_
    ```
* 可安裝額外的套件 [graphviz](https://medium.com/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176)，畫出決策樹的圖形幫助理解模型分類的準則
* 範例與作業
  * [範例D042](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_042_HW_decision_tree/Day_042_decision_tree.ipynb)
    * 資料集：Iris
    * 建立模型四步驟：讀進資料 → 資料切分 → 建立模型 → 評估模型
  * [作業D042](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_042_Ans.ipynb)
    * 資料集：wine、boston
    * 調整 Decision Tree 的超參數是否會影響結果

Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D043-TreeBasedModel-隨機森林RandomForest
* 決策樹的缺點
  * 決策樹生成時考慮所有資料與特徵來做切分的
  * 若不對決策樹進行限制 (樹深度、葉子上至少要有多少樣本等)，決策樹非常容易 Over-fitting
* 隨機森林 (Random Forest)
  * 集成 (Ensemble) 是將多個模型的結果組合在一起，透過投票或是加權的方式得到最終結果
  * 隨機森林的每一棵樹在生成過程中，都是隨機使用一部份的訓練資料與特徵，代表每棵樹都是用隨機的資料訓練而成的
  * [feature 數](http://hhtucode.blogspot.com/2013/06/ml-random-forest.html)
    * 設定最 少要 bagging 出 (k/2)+1 或 square(k)[有夠多] 的 feature，才比較有顯著結果，k 為原本的 feature 數量
* 範例與作業
  * [作業D043](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_043_Ans.ipynb)
    * 無程式撰寫

Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D044-程式實作-隨機森林
* 語法
  ```
  from sklearn.ensemble import RandomForestClassifier 
  from sklearn.ensemble import RandomForestRegressor 
  clf = RandomForestRegressor()
  ```
* 超參數
  * 與決策樹相同(max_depth, min_samples_split)
  * 生成樹的數量，越多越不容易過度擬和，但運算時間會變長
    ```
    from sklearn.ensemble import RandomForestClassifier
    clf = RandomForestClassifier(    
                n_estimators=10, #決策樹的數量
                criterion="gini",
                max_features="auto", #如何選取 features
                max_depth=10,
                min_samples_split=2,
                min_samples_leaf=1
        )
    ```
  * 如何選取 features：auto
* 範例與作業
  * [範例D044](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_044_HW_random_forest/Day_044_random_forest.ipynb)
    * 資料集：Iris
    * 瞭解隨機森林的建模方法與超參數的意義
  * [作業D044](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_044_Ans.ipynb)
    * 資料集：boston、wine
    * 調整超參數對於隨機森林的影響

Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D045-TreeBasedModel-梯度提升機GradientBoostingMachine
* Gradient Boosting Machine
  * 下一棵樹是為了修正前一棵樹的錯誤，故每一棵樹皆有相關聯
  * GBDT(Gradient Boosting Decision Tree)用來做回歸預測，調整後也可以用于分類
* Boosting 與 Bagging 的差異
    <table border="1" width="15%">
          <tr>
            <th width="5%"> </a>
            <th width="5%"> Bagging </a>
            <th width="5%"> Boosting </a>
          </tr>
          <tr>
            <td> 關係 </td>
            <td> 透過抽樣(sampling)的方式來生成每一棵樹，樹與樹之間是獨立生成的 (Independent classifiers) </td>
            <td> 透過序列(additive)的方式來生成每一顆樹，後面的樹要能夠修正前一棵樹，每棵樹都會與前面的樹有關聯 (Sequential classifiers) </td>
          </tr>
          <tr>
            <td> fitting </td>
            <td> Handles overfitting </td>
            <td> Can overfit </td>
          </tr>
          <tr>
            <td> 優點 </td>
            <td> Reduce variance </td>
            <td> Reduce bias & variance </td>
          </tr>
    </table>
* 參考資料
  * [ML Lecture 22: Ensemble](https://www.youtube.com/watch?v=tH9FH1DH5n0)
  * [Kaggle Winning Solution Xgboost Algorithm](https://www.youtube.com/watch?v=ufHo8vbk6g4)
  * [How to explain gradient boosting](https://explained.ai/gradient-boosting/index.html)

Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D046-程式實作-梯度提升機
* 使用 Sklearn 中的梯度提升機
  * 如同隨機森林，從 sklearn.ensemble 這裏 import 進來，代表梯度提升機同樣是個集成模型
  * 透過多棵決策樹依序生成來得到結果，緩解原本決策樹容易過擬和的問題，實務上的結果通常也會比決策樹來得好
  * 語法
    ```
    from sklearn.ensemble import GradientBoostingClassifier
    from sklearn.ensemble import GradientBoostingRegressor
    clf = GradientBoostingClassifier()
    ```
  * 超參數
    * 與決策樹相同(max_depth, min_samples_split)
    * 可決定要生成數的數量，越多越不容易過擬和，但是運算時間會變長
      ```
      from sklearn.ensemble import GradientBoostingClassifier
      clf = GradientBoostingClassifier(
                loss="deviance", #Loss 的選擇，若改為 exponential 則會變成 Adaboosting 演算法，概念相同但實作稍微不同
                learning_rate=0.1, #每棵樹對最終結果的影響，應與 n_estimators 成反比
                n_estimators=100 #決策樹的數量
            )
      ```
    * [如何使用 Python 調整梯度提升機的超參數](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)
  * 常見問題：隨機森林與梯度提升機的特徵重要性結果不相同？
    * 決策樹計算特徵重要性的概念是，觀察某一特徵被用來切分的次數而定
    * 假設有兩個一模一樣的特徵，在隨機森林中每棵樹皆為獨立，因此兩個特徵皆有可能被使用，最終統計出來的次數會被均分
    * 在梯度提升機中，每棵樹皆有關連，因此模型僅會使用其中一個特徵，另一個相同特徵的重要性則會消失
* 範例與作業
  * [範例D046](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_046_HW_gradient_boosting_machine/Day_046_gradient_boosting_machine.ipynb)
  * [作業D046](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_046_Ans.ipynb)

Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

# D047 超參數調整與優化
* 範例與作業(待觀看)
  * [範例D047]()
  * [作業D047]()
# D048 Kaggle 競賽平台介紹
* 範例與作業(待觀看)
  * [範例D048]()
  * [作業D048]()
# D049 集成方法-混和泛化(Blending)
* 範例與作業(待觀看)
  * [範例D049]()
  * [作業D049]()
# D050 集成方法-堆疊泛化(Stacking)
* 範例與作業(待觀看)
  * [範例D050]()
  * [作業D050]()

Back to <a href="#機器學習調整參數">機器學習調整參數</a>
<br>
<br>

# D051-D053 Kaggle 期中考
* 範例與作業(待觀看)
  * [範例D051]()
  * [作業D051]()

Back to <a href="#Kaggle期中考"></a>
<br>
<br>
