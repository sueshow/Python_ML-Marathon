# Python_ML-Marathon
## 學習大綱
### <a href="#機器學習概論">機器學習概論</a>
  * <a href="#D001-資料介紹與評估資料">D001 資料介紹與評估資料</a>
  * <a href="#D002-機器學習概論">D002 機器學習概論</a>
  * <a href="#D003-機器學習-流程與步驟">D003 機器學習-流程與步驟</a>
  * <a href="#D004-EDA-讀取資料與分析流程">D004 EDA/讀取資料與分析流程
  
### <a href="#資料清理數據前處理">資料清理數據前處理</a> 
  * <a href="#D005-如何新建一個dataframe如何讀取其他資料">D005 如何新建一個 dataframe？如何讀取其他資料？(非csv的資料)</a>
  * <a href="#D006-EDA-欄位的資料類型介紹及處理">D006 EDA-欄位的資料類型介紹及處理</a>
  * <a href="#D007-EDA-特徵類型">D007 EDA-特徵類型</a>
  * <a href="#D008-EDA-資料分佈">D008 EDA-資料分佈</a>
  * <a href="#D009-EDA-Outlier及處理">D009 EDA-Outlier及處理</a>
  * <a href="#D010-EDA-去除離群值-數值型">D010 EDA-去除離群值(數值型)</a>
  * <a href="#D011-EDA-常用的數值取代">D011 EDA-常用的數值取代</a>
  * <a href="#D012-EDA-補缺失值與標準化-數值型">D012 EDA-補缺失值與標準化(數值型)</a>
  * <a href="#D013-常見的DataFrame操作">D013 常見的 DataFrame 操作</a>
  * <a href="#D014-程式實作EDA-相關係數簡介">D014 程式實作 EDA-相關係數簡介</a>
  * <a href="#D015-程式實作EDA-CorrelationCode">D015 程式實作EDA-Correlation code</a>
  * <a href="#D016-EDA-不同數值範圍間的特徵如何檢視">D016 EDA-不同數值範圍間的特徵如何檢視</a>
  * <a href="#D017-EDA-把連續型變數離散化">D017 EDA-把連續型變數離散化</a>
  * <a href="#D018-程式實作EDA-把連續型變數離散化">D018 程式實作EDA-把連續型變數離散化</a>
  * <a href="#D019-程式實作-Subplots">D019 程式實作-Subplots</a>
  * <a href="#D020-程式實作-HeatmapANDGrid-plot">D020 程式實作-Heatmap & Grid-plot</a>
  * <a href="#D021-模型-LogisticRegression">D021 模型-Logistic Regression</a>

### <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
  * <a href="#D022-特徵工程簡介">D022 特徵工程簡介</a>
  * <a href="#D023-特徵工程-數值型-去除偏態">D023 特徵工程(數值型)-去除偏態</a>
  * <a href="#D024-特徵工程-類別型-基礎處理">D024 特徵工程(類別型)-基礎處理</a>
  * <a href="#D025-特徵工程-類別型-均值編碼">D025 特徵工程(類別型)-均值編碼</a>
  * <a href="#D026-特徵工程-類別型-其他進階處理">D026 特徵工程(類別型)-其他進階處理</a>
  * <a href="#D027-特徵工程-時間型">D027 特徵工程(時間型)</a>
  * <a href="#D028-特徵工程-數值與數值組合">D028 特徵工程-數值與數值組合</a>
  * <a href="#D029-特徵工程-類別與數值組合">D029 特徵工程-類別與數值組合</a>
  * <a href="#D030-特徵選擇">D030 特徵選擇</a>
  * <a href="#D031-特徵評估">D031 特徵評估</a>
  * <a href="#D032-特徵優化-分類型-葉編碼">D032 特徵優化(分類型)-葉編碼</a>

### <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
  * <a href="#D033-機器如何學習">D033 機器如何學習</a>
  * D034 訓練及測試集切分
  * D035 Regression vs. classification
  * D036 評估指標選定 evaluation metrics
  * D037 Regression model-線性迴歸、羅吉斯回歸
  * D038 程式實作-線性迴歸、羅吉斯回歸
  * D039 Regression model-LASSO回歸、Ridge回歸
  * D040 程式實作-LASSO回歸、Ridge回歸
  * D041 Tree based model-決策樹(Decision Tree)
  * D042 程式實作-決策樹
  * D043 Tree based model-隨機森林(Random Forest)
  * D044 程式實作-隨機森林
  * D045 Tree based model-梯度提升機(Gradient Boosting Machine)
  * D046 程式實作-梯度提升機

### <a href="#機器學習調整參數">機器學習調整參數</a>
  * <a href="#D047-超參數調整及優化">D047 超參數調整及優化</a>
  * D048 Kaggle競賽平台介紹
  * D049 集成方法-混和泛化(Blending)
  * D050 集成方法-堆疊泛化(Stacking)
  
### <a href="#Kaggle期中考">Kaggle期中考</a>
  * <a href="#D051-D053-Kaggle期中考">D051-D053 Kaggle 期中考</a>

### <a href="#非監督式的機器學習">非監督式的機器學習</a>
  * <a href="#D054-非監督式機器學習">D054 非監督式機器學習
  * D055 非監督式-分群-K-Means 分群
  * D056 非監督式-分群-K-Means 分群評估：使用輪廓分析
  * D057 非監督式-分群-階層式 Hierarchical Clustering
  * D058 非監督式-分群-Hierarchical Clustering 觀察：使用 2D 樣版資料集
  * D059 降維方法(Dimension Reduction)-主成份分析(PCA)
  * D060 程式實作-PCA：使用手寫辨識資料集
  * D061 降維方法(Dimension Reduction)-T-SNE
  * D062 程式實作-T-SNE：分群與流形還原

### <a href="#深度學習理論與實作">深度學習理論與實作</a>
  * <a href="#D063-深度學習簡介">D063 深度學習簡介
  * D064 深度學習-模型調整與學習曲線
  * D065 深度學習-啟動函數與正規化

### <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
  * <a href="#D066-Keras安裝與介紹">D066 Keras安裝與介紹
  * D067 Keras Dataset
  * D068 Keras Sequential API
  * D069 Keras Module API
  * D070 深度神經網路的基礎知識
  * D071 損失函數
  * D072 啟動函數
  * D073 梯度下降 Gradient Descent
  * D074 Gradient Descent 數學原理
  * D075 BackPropagation
  * D076 優化器 Optimizers
  * D077 神經網路-訓練細節與技巧：Validation and overfit
  * D078 神經網路-訓練前注意事項
  * D079 神經網路-訓練細節與技巧：Learning rate effect
  * D080 程式實作-優化器與學習率的組合與比較
  * D081 神經網路-訓練細節與技巧：Regularization
  * D082 神經網路-訓練細節與技巧：Dropout
  * D083 神經網路-訓練細節與技巧：Batch normalization
  * D084 程式實作-正規化/機移除/批次標準化的組合與比較
  * D085 神經網路-訓練細節與技巧：使用 callbacks 函數做 earlystop
  * D086 神經網路-訓練細節與技巧：使用 callbacks 函數儲存 model
  * D087 神經網路-訓練細節與技巧：使用 callbacks 函數做 reduce learning rate
  * D088 神經網路-訓練細節與技巧：撰寫 callbacks 函數
  * D089 神經網路-訓練細節與技巧：撰寫 Loss function
  * D090 使用傳統電腦視覺與機器學習進行影像辨識
  * D091 程式實作-使用傳統電腦視覺與機器學習進行影像辨識

### <a href="#深度學習應用卷積神經網路">深度學習應用卷積神經網路</a>
  * D092 卷積神經網路(Convolution Neural Network, CNN)簡介
  * D093 CNN-架構細節
  * D094 CNN-卷積(Convolution)層與參數調整
  * D095 CNN-池化(Pooling)層與參數調整
  * D096 Keras 中的 CNN layers
  * D097 程式實作-CIFAR-10 資料集
  * D098 CNN-訓練細節與技巧：處理大量數據
  * D099 CNN-訓練細節與技巧：處理小量數據
  * D100 CNN-訓練細節與技巧：轉移學習(Transfer learning)

### <a href="#Kaggle期末考">Kaggle期末考</a>
  * D101-D103 影像辨識

### <a href="#Bonus進階補充">Bonus進階補充</a>
  * D104 史丹佛上 ConvNetJS 簡介
  * D105 CNN 卷積網路回顧
  * D106 電腦視覺常用公開資料集
  * D107 電腦視覺應用介紹
<br>
<br>


## 機器學習概論
### D001-資料介紹與評估資料
* 進入資料科學領域的流程
  * 找到問題：挑一個有趣的問題，並解決一個簡單的問題開始
  * 初探：在這個題目上做一個原型解決方案(prototype solution)
  * 改進：試圖改進你的原始解決方案並從中學習(如代碼優化、速度優化、演算法優化)
  * 分享：紀錄是一個好習慣，試著紀錄並分享解決方案歷程
  * 練習：不斷在一系列不同的問題上反覆練習
  * 實戰：認真地參與一場比賽
* 面對資料應思考哪些問題？
  * 好玩，如：預測生存(吃雞)遊戲誰可以活得久、[PUBG](https://www.kaggle.com/c/pubg-finish-placement-prediction)
  * 企業的核心問題，如：用戶廣告投放、[ADPC](https://www.kaggle.com/c/avito-demand-prediction)
  * 公眾利益/影響政策方向，如：[停車方針](https://www.kaggle.com/datasets/new-york-city/nyc-parking-tickets)、[計程車載客優化](https://www.kaggle.com/c/nyc-taxi-trip-duration)
  * 對世界很有貢獻，如：[肺炎偵測](https://www.kaggle.com/c/rsna-pneumonia-detection-challenge)
* 資料來源
  * 來源與品質息息相關
  * 根據不同資料源，合理地推測/懷疑異常資料異常的理由與機率
  * 方式：網站流量、購物車紀錄、網路爬蟲、格式化表單、[Crowdsourcing](https://en.wikipedia.org/wiki/Crowdsourcing)、紙本轉電子檔
* 資料型態
  * 結構化資料需要檢視欄位意義及名稱，如：數值、表格等
  * 非結構化資料需要思考資料轉換與標準化方式，如：圖像、影像、文字、音訊等
* 指標係指可供衡量的數學評估指標(Evaluation Metrics)，常用的衡量指標：
  * 分類問題
    * 正確率
    * AUC(Accuracy)：客群樣貌
    * MAP
  * 迴歸問題
    * MAE(平均絕對誤差)：玩家排名
    * MSE：存活時間
    * RMSE
  * [其他衡量指標](https://blog.csdn.net/aws3217150/article/details/50479457)
    * ROC(Receiver Operating Curve)：客群樣貌、素材好壞
    * MAP@N：如 MAP@5、MAP@12
* 範例與作業
  * [作業D001](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_001_example_of_metrics_Ans.ipynb) 
    * 目標：寫一個 MSE 函數

Back to <a href="#機器學習概論">機器學習概論</a>
<br>
<br>

### D002-機器學習概論
* 機器學習範疇
  * 人工智慧 > 機器學習 > 深度學習
  * 白話文：讓機器從資料中找尋規律與趨勢而不需要給定特殊規則
  * 數學：給定目標函數與訓練資料，學習出能讓目標函數最佳的模型參數
* 機器學習的組成及應用
  * 監督式學習：如圖像分類、詐騙偵測
    * 有成對的 (x,y) 資料，且 x 與 y 之間具有某種關係
    * 如圖像分類，每張圖都有對應到的標記(y)
    * 流程：前處理 Processing → 探索式數據分析 Exploratory Data Analysis (D014-D021：統計值【相關係數、核密度函數、離散化】的視覺化【繪圖排版、常用圖形、模型體驗】) → 特徵工程 Feature Engineering (D022-D032：填補缺值、去離群值、去偏態、特徵縮放、特徵組合、特徵評估) → 模型選擇 Model Selection (D033-D046：驗證基礎、預測模型、評估指標、基本模型、樹狀模型) → 參數調整 Fine Tuning (D047-D050：網格搜尋、隨機搜尋、Kaggle平台) → 集成 Ensemble (D048-D049：混和泛化 Blending、堆疊泛化 Stacking)
  * 非監督式學習(D054-D062)：如維度縮減、分群、壓縮
    * 僅有 x 資料而沒有標註的 y
    * 如有圖像資料，但沒有標記
    * 應用：降維 Dimension Reduction(如：PCA(D059-D060)、t-SNE(D061-D062))、分群 Clustering(如：K-mean、Hierarchical Clustering)
  * 強化學習：如下圍棋、打電玩
    * 又稱增強式學習，透過定義環境(Environment)、代理機器人(Agent)及獎勵(Reward)，讓機器人透過與環境的互動學習如何獲取最高的獎勵
    * 應用：Alpha GO
* 範例與作業
  * [作業D002](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_002_Ans.ipynb) 
    * 目標：瞭解機器學習適合應用的領域與範疇
  
Back to <a href="#機器學習概論">機器學習概論</a>
<br>
<br>

### D003-機器學習-流程與步驟
* 機器學習專案開發流程
  * 資料蒐集、前處理
    * 資料來源
      * 結構化資料：Excel檔、CSV檔
      * 非結構化資料：圖片、影音、文字
    * 瞭解資料，使用資料的 Python 套件
      * 開啟圖片：PIL、skimage、open-cv等
      * 開啟文件：pandas
    * 資料前處理，進行特徵工程
      * 缺失值填補
      * 離群值處理
      * 標準化
  * 定義目標與評估準則
    * 回歸問題(數值)？分類問題(類別)？
    * 要使用甚麼資料來進行預測？
    * 資料分為：訓練集training set、驗證集validation set、測試集test set
    * 評估指標
      * 回歸問題
        * RMSE (Root Mean Square Error)
        * Mean Absolute Error
        * R-Square
      * 分類問題
        * Accuracy
        * F1-score
        * AUC (Area Under Curve)
  * 建立模型與調整參數：模型調整、優化、訓練
    * 回歸模型 Regression
    * 樹模型 Tree-based model
    * 神經網絡 Neural network
  * 導入
    * 建立資料蒐集、前處理等流程
    * 送進模型進行預測
    * 輸出預測結果
    * 視專案需求整合前後端，資料格式使用 json、csv
* 範例與作業
  * [作業D003](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_003_Ans.ipynb)
    * 閱讀文章：機器學習巨頭作的專案

Back to <a href="#機器學習概論">機器學習概論</a>
<br>
<br>

### D004-EDA-讀取資料與分析流程
* 範例：Home Credit Default Risk (房貸風險預測 from Kaggle)
  * 目的：預測借款者是否會還款，以還款機率作為最終輸出
  * 此問題為分類問題
  * 步驟：
    * 為何這個問題重要：有人沒有信用資料
    * 資料從何而來：信用局(Credit Bureau)調閱紀錄、Home Credit內部紀錄(過去借貸、信用卡狀況)
    * 資料的型態：結構化資料(數值、類別資料)
    * 可以回答什麼問題：指標
      * [ROC](https://zh.wikipedia.org/wiki/ROC%E6%9B%B2%E7%BA%BF)
      * AUC：0.5代表隨機猜測，~1則代表模型預測力越好
* EDA
  * 初步透過視覺化/統計工具進行分析，達到三個主題目的
    * 了解資料：獲取資料所包含的資訊、結構和特點
    * 發現 outliers 或異常數值：檢查資料是否有誤
    * 分析各變數間的關聯性：找到重要的變數
  * 觀察資料，並檢查是否符合分析前的假設
  * 數據分析流程
    * 收集資料
    * 數據清理 → 特徵萃取 → 資料視覺化 → 建立模型 → 驗證模型
    * 決策應用
* 範例與作業
  * [範例D004](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_004_HW_EDA_%E8%AE%80%E5%8F%96%E8%B3%87%E6%96%99%E8%88%87%E5%88%86%E6%9E%90%E6%B5%81%E7%A8%8B/Day_004_first_EDA.ipynb)
    * 使用 pandas.read_csv 讀取資料
    * 簡單瀏覽 pandas 所讀進的資料
  * [作業D004](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_004_first_EDA_Ans.ipynb)
    * 列出資料的大小：shape
    * 列出所有欄位：columns
    * 擷取部分資料：loc、iloc

Back to <a href="#機器學習概論">機器學習概論</a>
<br>
<br>


## 資料清理數據前處理
### D005-如何新建一個dataframe如何讀取其他資料
* 前處理 Processing
  * 資料讀取 D005 → 格式調整 D006-D008、D013 → 填補缺值 D009、D011-D012 → 去離群值 D010 → 特徵縮放 D011-D012
  * 用途
    * 需要把分析過程中所產生的數據或結果儲存為[結構化的資料](https://daxpowerbi.com/%e7%b5%90%e6%a7%8b%e5%8c%96%e8%b3%87%e6%96%99/) → 使用 pandas
    * 資料量太大，操作很費時，先在具有同樣結構的資料進行小樣本的測試
    * 先建立 dataframe 來瞭解所需的資料結構、分佈
* 讀取其他資料格式：txt / jpg / png / json / mat / npy / pkl
  * 圖像檔 (jpg / png)
    * 範例：可使用 PIL、Skimage、CV2，其中 CV2 速度較快，但須注意讀入的格式為 BGR
      ```
      Import cv2
      image = cv2.imread(...) # 注意 cv2 會以 BGR 讀入
      image = cv2.cvtcolor(image, cv2.COLOR_BGR2RGB)

      from PIL import Image
      image = Image.read(...)
      import skimage.io as skio
      image = skio.imread(...)
      ```
  * Python npy：可儲存處理後的資料
    * 範例
      ```
      import numpy as np
      arr = np.load(example.npy)
      ```
  * Pickle (pkl)：可儲存處理後的資料
    * 範例
      ```
      import pickle
      with open('example.pkl', 'rb') as f:
          arr = pickle.load(f)
      ```
* 程式用法
  <table border="1" width="40%">
    <tr>
        <th width="10%">函式</a>
        <th width="10%">用途</a>
        <th width="10%">函式</a>
        <th width="10%">用途</a>
    </tr>
    <tr>
        <td> pd.DataFrame </td>
        <td> 建立一個 dataframe </td>
        <td> np.random.randint </td>
        <td> 產生隨機數值 </td>
    </tr>
    <tr>
        <td> with open() </td>
        <td> 文字格式 </td>
        <td>  </td>
        <td>  </td>
    </tr>
  </table>
  
* 延伸閱讀
  * [Pandas Foundations](https://www.datacamp.com/courses/data-manipulation-with-pandas)
  * [github repo](https://github.com/guipsamora/pandas_exercises)
* 範例與作業
  * [範例D005-1](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_005_HW_%E5%A6%82%E4%BD%95%E6%96%B0%E5%BB%BA%E4%B8%80%E5%80%8Bdataframe/Day_005-1_build_dataframe_from_scratch.ipynb)
    * Dict → DataFrame
    * List → DataFrame
    * Group by
  * [範例D005-2](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_005_HW_%E5%A6%82%E4%BD%95%E6%96%B0%E5%BB%BA%E4%B8%80%E5%80%8Bdataframe/Day_005-2_read_and_write_files.ipynb)
    * 檔案轉換：txt、json、npy、Pickle
    * 參考資料
      * [寫給自己的技術筆記 - 作為程式開發者我們絕對不能忽略的JSON - Python 如何處理JSON文件](https://matters.news/@CHWang/103773-%E5%AF%AB%E7%B5%A6%E8%87%AA%E5%B7%B1%E7%9A%84%E6%8A%80%E8%A1%93%E7%AD%86%E8%A8%98-%E4%BD%9C%E7%82%BA%E7%A8%8B%E5%BC%8F%E9%96%8B%E7%99%BC%E8%80%85%E6%88%91%E5%80%91%E7%B5%95%E5%B0%8D%E4%B8%8D%E8%83%BD%E5%BF%BD%E7%95%A5%E7%9A%84json-python-%E5%A6%82%E4%BD%95%E8%99%95%E7%90%86json%E6%96%87%E4%BB%B6-bafyreibegh77qc2xaejwbbbv5xdoodgqyaznesq5uhety5von3rpqzdaoa)
  * [範例D005-3](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_005_HW_%E5%A6%82%E4%BD%95%E6%96%B0%E5%BB%BA%E4%B8%80%E5%80%8Bdataframe/Day_005-3_read_and_write_files.ipynb)
    * 用 skimage.io 讀取圖檔
    * 用 PIL.Image 讀取圖檔
    * 用 OpenCV 讀取圖檔：pip install opencv-python
      * cv2.IMREAD_COLOR：讀取 RGB 的三個 CHANNELS 的彩色圖片，忽略透明度的 CHANNELS
        * cv2.IMREAD_GRAYSCALE：灰階
        * cv2.IMREAD_UNCHANGED：讀取圖片的所有 CHANNELS，包含透明度的 CHANNELS
  * [作業D005-1](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_005_%E4%BD%9C%E6%A5%AD%E8%A7%A3%E7%AD%94/Day_005-1_Ans.ipynb)
    * 重點：DataFrame、Group by
  * [作業D005-2](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_005_%E4%BD%9C%E6%A5%AD%E8%A7%A3%E7%AD%94/Day_005-2_Ans.ipynb)  
    * 從網頁上讀取連結清單
    * 從清單網址讀取圖片

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D006-EDA-欄位的資料類型介紹及處理
* EDA (Exploratory Data Analysis)：探索式資料分析運用統計工具或是學畫，對資料有初步的瞭解，以幫助我們後續對資料進行更進一步的分析
* 資料類型
  * 離散變數：只能用整數單位計算的變數，如房間數、性別、國家
  * 連續變數：在一定區間內可以任意取值的變數，如身高、降落花費的時間、車速
* Pandas DataFrame 常見的欄位類型(*)
  <table border="1" width="25%">
    <tr>
        <th width="5%">Pandas 類型</a>
        <th width="5%">Python 類型</a>
        <th width="10%">NumPy 類型</a>
        <th width="5%">說明</a>
    </tr>
    <tr>
        <td> object </td>
        <td> str or mixed  </td>
        <td> string、unicode、mixed types </td>
        <td> 字符串或混和數字，用於表示類別型變數 </td>
    </tr>
    <tr>
        <td> int64(*) </td>
        <td> int </td>
        <td> int、int8、int16、int32、int64、uint8、uint16、uint32、uint64 </td>
        <td> 整數，可表示離散或連續變數 </td>
    </tr>
    <tr>
        <td> float64(*) </td>
        <td> float </td>
        <td> float、float16、float32、float64 </td>
        <td> 浮點數，可表示離散或連續變數 </td>
    </tr>
    <tr>
        <td> bool </td>
        <td> bool </td>
        <td> bool </td>
        <td> True/False </td>
    </tr>
    <tr>
        <td> datetime64(ns) </td>
        <td> nan </td>
        <td> datetime64(ns) </td>
        <td> 日期時間 </td>
    </tr>
    <tr>
        <td> timedelta(ns) </td>
        <td> nan </td>
        <td> nan </td>
        <td> 時間差距 </td>
    </tr>
    <tr>
        <td> category </td>
        <td> nan </td>
        <td> nan </td>
        <td> 分類 </td>
    </tr>
  </table>
  
* 格式調整
  * 訓練模型時，字串/類別類型的資料需要轉為數值型資料，轉換方式：
    <table border="1" width="13%">
      <tr>
        <th width="3%">encode</a>
        <th width="10%">label encode</a>
        <th width="10%">one-hot encode</a>
      </tr>
      <tr>
        <td> 類型 </td>
        <td> 有序類別變量(如學歷)  </td>
        <td> 無序類別變量(如國家) </td>
      </tr>
      <tr>
        <td> 作法 </td>
        <td> 將類別變數中每一個類別賦予數值，不會新增欄位 </td>
        <td> 為每個類別新增一個欄位，0/1表示是否 </td>
      </tr>
      <tr>
        <td> 使用時機 </td>
        <td> 會讓模型學習到「順序關係」，也就是有大小之分 </td>
        <td> 當類別之間不存在優劣、前後、高低之分的時候，也就是「無序」，就適合採用 One-Hot Encoding。但相對地，因為維度提高了，就會較費時且占用較多的空間 </td>
      </tr>
    </table>

* 範例與作業
  * [範例D006](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_006_HW_EDA%E6%AC%84%E4%BD%8D%E7%9A%84%E8%B3%87%E6%96%99%E9%A1%9E%E5%9E%8B%E4%BB%8B%E7%B4%B9%E5%8F%8A%E8%99%95%E7%90%86/Day_006_column_data_type.ipynb)
    * 檢視 DataFrame 的資料型態
    * 瞭解 Label Encoding 如何寫
    * 瞭解 One Hot Encoding 如何寫(pd.get_dummies)
  * [作業D006](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_006_column_data_type_Ans.ipynb)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D007-EDA-特徵類型
* 特徵類別：[參考資料](https://openhome.cc/Gossip/CodeData/PythonTutorial/NumericStringPy3.html)
  * 數值型特徵：有不同轉換方式，函數/條件式都可以，如坪數、年齡
    * 填補缺失值或直接去除離群值的方法：[去偏態](https://ithelp.ithome.com.tw/articles/10219949?sc=iThelpR)，符合常態假設
      * 對數去偏(log1p)
      * 方根去偏(sqrt)
      * 分布去偏(boxcox)
  * 類別型特徵：通常一種類別對應一種分數，如行政區、性別
    * 標籤編碼(Label Encoding)
    * 獨熱編碼(One Hot Encoding)
    * 均值編碼(Mean Encoding)
    * 計數編碼(Counting)
    * 特徵雜湊(Feature Hash)
  * 其他類別
    <table border="1" width="13%">
      <tr>
        <th width="3%">特徵</a>
        <th width="10%">說明</a>
      </tr>
      <tr>
        <td> 二元特徵 </td>
        <td> ● 只有 True/False 兩種數值的特徵 <br>
             ● 可當作類別型，也可當作數值型特徵(True:1/False:0) </td>
      </tr>
      <tr>
        <td> 排序型特徵 </td>
        <td> ● 如名次/百分等級，有大小關係，但非連續數字 <br>
             ● 通常視為數值型特徵，避免失去排序資訊 </td>
      </tr>
      <tr>
        <td> 時間型特徵 </td>
        <td> ● 不適合當作數值型或類別型特徵，可能會失去週期性、排序資訊 <br>
             ● 特殊之處在於有週期性 <br>
             ● 處理方式：時間特徵分解、週期循環特徵 </td>
      </tr>
      <tr>
        <td> 文本型 </td>
        <td> ● TF-IDF、詞袋、word2vec </td>
      </tr>
      <tr>
        <td> 統計型 </td>
        <td> ● 比率、次序、加減乘除平均、分位數 </td>
      </tr>
      <tr>
        <td> 其他類型 </td>
        <td> ● 組合特徵 </td>
      </tr>
    </table> 

* [交叉驗證](https://zhuanlan.zhihu.com/p/24825503)
  * 以 cross_val_score 顯示改善效果
  * 方法
    * 留出法(holdout cross validation)
    * K 拆交叉驗證法(K fold Cross Vaildation)：將所有數據集切成 K 等分，不重複選其中一份當測試集，其他當訓練集，並計算模型在測試集上的 MSE
    * 留一法(Leave one out cross validation; LOOCV)：只用一個數據當測試集，其他全為訓練集
    * Bootstrap Sampling
* 範例與作業
  * [範例D007](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_007_HW_%E7%89%B9%E5%BE%B5%E9%A1%9E%E5%9E%8B/Day_007_Feature_Types.ipynb)
    * 以房價預測為範例，看特徵類型
  * [作業D007](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_007_Ans.ipynb)
    * 以鐵達尼生存預測為範例
    * 目標：完成三種不同特徵類型的三種資料操作，觀察其結果(何類難處理)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D008-EDA-資料分佈
* 統計量化
  * 基本統計分析方法
    * 描述性分析：總量分析、相對數分析、平均數、變異指數等
    * 趨勢概率分析：計算集中趨勢 
      * 算數平均值 Mean
      * 中位數 Median
      * 眾數 Mode
    * 離散程度分析：計算資料分散程度
      * 最小值 Min、最大值 Max、範圍 Range
      * 四分位差 Quartiles
      * 變異數 Variance
      * 標準差 Standard deviation
      * 極差、方差
  * 列表分析
  * 假設檢驗分析 
    * 分布程式：[常見統計分布](https://www.healthknowledge.org.uk/public-health-textbook/research-methods/1b-statistical-methods/statistical-distributions)
    * 參數估計(含點、區間)
    * 統程
    * 多項分析與*2檢驗
  * 多元統計分析
    * 一元線性回歸分析
    * 聚類分析，如KNN
* 視覺化
  * [python 視覺化套件](https://matplotlib.org/3.2.2/gallery/index.html)
  * [The Python Graph Gallery](https://www.python-graph-gallery.com/)
  * [Matploitlib](https://matplotlib.org/3.2.2/gallery/index.html)
  * [The R Graph Gallery](https://r-graph-gallery.com/)
  * [R Graph Gallery (Interactive plot，互動圖)](https://gist.github.com/mbostock)
* 範例與作業
  * [作業D008](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_008_Ans.ipynb) 
    * DataFrame下可用的函數
      * .mean()、median()、.sum()
      * .cumsum()：以上累積
      * .describe()：描述性統計
      * .var()、.std()
      * .skew()、.kurt()
      * .corr()、.cov()
    * [視覺化](https://pandas.pydata.org/pandas-docs/version/0.23.4/visualization.html)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D009-EDA-Outlier及處理
* 離群值、異常值([Outlier](https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba))
  * 定義
    * 數據集中有一個或一些數值與其他數值相比差異較大
    * 一個數值偏離觀測平均值的機率小於等於 1/(2n)，則該數值應當拿掉
    * 數據須符合常態分佈，如值大於3個標準差，則視為異常值
  * 可能出現的原因
    * 未知值
    * 錯誤紀錄/手誤/系統性錯誤
    * 例外情境
  * 檢查流程與方法
    * 確認每一個欄位的意義
    * 透過檢查數值範圍 (平均數、標準差、中位數、分位數(IQR)、zscore) 或畫圖(散點圖(scatter plot)、分佈圖(histogram)、直方圖、盒圖(boxplot)、次數累積分佈、[ECDF](https://medium.com/ai%E5%8F%8D%E6%96%97%E5%9F%8E/exploratory-data-analysis-%E6%8E%A2%E7%B4%A2%E8%B3%87%E6%96%99-ecdf-7fa350c32897)或其他圖)檢查是否有異常
  * 處理方法
    * 視覺化：透過分佈看出離群值
    * 新增欄位用以紀錄異常與否(人工再標註)
    * 填補 (取代)：視情況以中位數、Min、Max、平均數填補(有時會用 NA)
    * 刪除資料
    * [離群值處理參考資料](https://andy6804tw.github.io/2021/04/02/python-outliers-clean/#%E8%B3%87%E6%96%99%E8%A7%80%E5%AF%9F)
* 範例與作業
  * [範例D009](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_009_HW_outlier%E5%8F%8A%E8%99%95%E7%90%86/Day_009_outliers_detection.ipynb)
    * 計算統計值、畫圖(直方圖)來觀察離群值
    * 疑似離群值的資料移除後，看剩餘的資料是否正常
    * 利用變數類型對欄位進行篩選
      * df.dtypes 給出各欄位名稱的 Seires
      * .isin(your_list) 可以用來給出 Seires 內每個元素是否在 your_list 裡面的布林值
      * 可以用布林值的方式進行遮罩的篩選 DataFrame
  * [作業D009](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_009_outlier%E5%8F%8A%E8%99%95%E7%90%86_Ans.ipynb)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D010-EDA-去除離群值-數值型
* [離群值](https://zhuanlan.zhihu.com/p/33468998)
  * 只有少數幾筆資料跟其他數值差異很大，標準化無法處理
    * 常態標準化：Z-score = (Xi-mean(Xi))/std(Xi)
    * 最大最小化：(Xi-min(Xi))/(max(Xi)-min(Xi))，code：MinMaxScaler
    * 參考資料
      * [資料預處理- 特徵工程- 標準化](https://matters.news/@CHWang/77028-machine-learning-%E8%B3%87%E6%96%99%E9%A0%90%E8%99%95%E7%90%86-%E7%89%B9%E5%BE%B5%E5%B7%A5%E7%A8%8B-%E6%A8%99%E6%BA%96%E5%8C%96-standard-scaler-%E5%85%AC%E5%BC%8F%E6%95%99%E5%AD%B8%E8%88%87python%E7%A8%8B%E5%BC%8F%E7%A2%BC%E5%AF%A6%E4%BD%9C%E6%95%99%E5%AD%B8-bafyreihd2uc5clmc7kzzswuhvfd56axliecfzxlk5236o54cvvcphgumzu)
      * [Sklearn 套件教學](https://matters.news/@CHWang/78462-machine-learning-%E6%A8%99%E6%BA%96%E5%8C%96-standard-scaler-%E5%BF%AB%E9%80%9F%E5%AE%8C%E6%88%90%E6%95%B8%E6%93%9A%E6%A8%99%E6%BA%96%E5%8C%96-sklearn-%E5%A5%97%E4%BB%B6%E6%95%99%E5%AD%B8-bafyreibpusofl5b3tt43ovknw2mnjzrmekfldelelyl33luzkfzc4k6loy)
  * 方法：用 cross-validation 來選擇
    * 捨棄離群值：離群值數量夠少時使用
    * 調整離群值：取代
* 範例與作業
  * [範例D010](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_010_HW_%E6%95%B8%E5%80%BC%E5%9E%8B%E7%89%B9%E5%BE%B5/Day_010_Outliers.ipynb)
    * 觀察原始數值的散佈圖及線性迴歸分數(用 cross-validation score 來評估)
    * 觀察將極端值以上下限值取代，對於分布與迴歸分數的影響
    * 觀察將極端值資料直接刪除，對於分布與迴歸分數的影響
  * [作業D010](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_010_Ans.ipynb)
    * 觀察將極端值以上下限值取代，對於分布與迴歸分數的影響
    * 觀察將極端值資料直接刪除，對於分布與迴歸分數的影響

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D011-EDA-常用的數值取代
* 處理例外值：常用以填補的統計值
  <table border="1" width="26%">
      <tr>
        <th width="3%">統計值</a>
        <th width="10%">語法</a>
        <th width="3%">統計值</a>
        <th width="10%">語法</a>        
      </tr>
      <tr>
        <td> 中位數(median) </td>
        <td> np.median(value_array) </td>
        <td> 分位數(quantiles) </td>
        <td> np.quantile(value_array, q=...) </td>
      </tr>
      <tr>
        <td> 眾數(mode) </td>
        <td> scipy.stats.mode(value_array)：較慢的方法 <br>
             dictionary method：較快的方法</td>
        <td> 平均數(mean) </td>
        <td> np.mean(value_array) </td>
      </tr>
  </table>
  
* 連續數據標準化
  * 單位不同對 y 的影響完全不同
  * 模型
    * 有影響的模型：Regression model
    * 影響不大的模型：Tree-based model
  * 常用方式
    * Z 轉換：(Xi-mean(Xi))/std(Xi)  
    * 空間壓縮：將空間轉換到 Y 區間中，有時候不會使用 min/max 方法進行標準化，而會採用 Qlow/Qhigh normalization，min 改為 q1，max 改為 q99，去除極值的影響
      * Y = 0~1，(Xi-min(Xi))/(max(Xi)-min(Xi))
      * Y = -1~1，((Xi-min(Xi))/(max(Xi)-min(Xi))-0.5)*2
      * Y = 0~1，針對特別影像，Xi/255
  * 優缺點
    * 優
      * 某些演算法(如SVM、DL)等，對權眾敏感或對損失函數平滑程度有幫助者
      * 特徵間的量級差異甚大
    * 劣
      * 有些指標，如相關係數不適合在有標準化的空間進行
      * 量的單位在某些特徵上是有意義的
* 範例與作業
  * [範例D011](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_011_HW_%E5%B8%B8%E7%94%A8%E7%9A%84%E6%95%B8%E5%80%BC%E5%8F%96%E4%BB%A3/Day_011_handle_outliers.ipynb)
    * 計算並觀察百分位數：不能有缺失值
    * 計算中位數的方法：不能有缺失值
    * 計算眾數：不能有缺失值
    * 計算標準化與最大最小化
  * [作業D011](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_011_handle_outliers_Ans.ipynb)
    * 填補資料
    * 標準化與最大最小化

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D012-EDA-補缺失值與標準化-數值型
* 填補[缺失值](https://juejin.cn/post/6844903648074416141)
  * 最重要的是欄位的領域知識與欄位中的非缺數
    * 填補指定值
      * 補 0 ：空缺原本就有 0 的含意，如前頁的房間數
      * 補不可能出現的數值：類別型欄位，但不適合用眾數時
    * 填補預測值：速度較慢但精確，從其他資料欄位學得填補知識
      * 若填補範圍廣，且是重要特徵欄位時可用本方式
      * 須提防 overfitting：可能退化成為其他特徵的組合
  * 補值要點：推論分布
    * 類別型態，可視為「另一種類別」或以「眾數」填補
    * 數值型態且偏態不明顯，以「平均數」、「中位數」填補
    * 注意盡量不要破壞資料分布
* 為何要[標準化](https://blog.csdn.net/SanyHo/article/details/107514236)
  * 以合理的方式，平衡特徵間的影響力
  * 方法：將值域拉一致
    * 標準化 (Standard Scaler)：
      * 假定數值為常態分佈，適合本方式平衡特徵
      * 轉換不易受到極端值影響
    * 最小最大化 (MinMax Scaler)：
      * 假定數值為均勻分佈，適合本方式平衡特徵
      * 轉換容易受到極端值影響
  * 適合場合
    * 非樹狀模型：如線性迴歸, 羅吉斯迴歸, 類神經...等，標準化/最小最大化後，對預測會有影響
    * 樹狀模型：如決策樹, 隨機森林, 梯度提升樹...等，標準化/最小最大化後，對預測不會有影響
* 範例與作業
  * [範例D012](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_012_HW_%E8%A3%9C%E7%BC%BA%E5%A4%B1%E5%80%BC%E8%88%87%E6%A8%99%E6%BA%96%E5%8C%96/Day_012_Fill_NaN_and_Scalers.ipynb)
    * 如何查詢個欄位空缺值數量
    * 觀察替換不同補缺方式，對於特徵的影響
    * 觀察替換不同特徵縮放方式，對特徵的影響
  * [作業D012](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_012_Fill_NaN_and_Scalers_Ans.ipynb)
    * 以「鐵達尼生存預測」為例

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D013-常見的DataFrame操作
* 轉換與合併 dataframe
  <table border="1" width="26%">
      <tr>
        <th width="3%">語法</a>
        <th width="10%">用途</a>
        <th width="3%">語法</a>
        <th width="10%">用途</a>        
      </tr>
      <tr>
        <td> pd.melt(df) </td>
        <td> 將「欄(column)」轉成「列(row)」 </td>
        <td> pd.pivot(columns='欄位名稱', values='值') </td>
        <td> 將「列(row)」轉成「欄(column)」 </td>
      </tr>
      <tr>
        <td> pd.concat([df1, df2]) </td>
        <td> 沿「列(row)」合併兩個 dataframe，default：axis=0 <br>
             對應的欄位數、名稱要一致</td>
        <td> pd.concat([df1, df2], axis=1) </td>
        <td> 沿「欄(column)」合併兩個 dataframe <br> 
             可將多個表依照某欄 (key) 結合使用，default：join='outer'進行 <br>
             可調整 join 為 'inner'，僅會以單一欄為結合</td>
      </tr>
      <tr>
        <td> pd.merge(df1, df2, on='id', how='outer') </td>
        <td> 將 df1、df2 以「id」這欄做全合併(遺失以 na 補) </td>
        <td> pd.merge(df1, df2, on='id', how='inner') </td>
        <td> 將 df1、df2 以「id」這欄做部分合併，自動去除重複的欄位 </td>
      </tr>
  </table>
  
* Subset
  * 邏輯操作
    <table border="1" width="30%">
      <tr>
        <th width="5%">用途</a>
        <th width="10%">語法</a>
        <th width="5%">用途</a>
        <th width="10%">語法</a>        
      </tr>
      <tr>
        <td> 大於 / 小於 / 等於 </td>
        <td> >, <, == </td>
        <td> 大於等於 / 小於等於 </td>
        <td> >=, <= </td>
      </tr>
      <tr>
        <td> 不等於 </td>
        <td> != </td>
        <td> 邏輯的 and, or, not, xor </td>
        <td> &, |, ~, ^</td>
      </tr>
      <tr>
        <td> 欄位中包含 value </td>
        <td> df.column.isin(value) </td>
        <td> 為 Nan </td>
        <td> df.isnull(obj) </td>
      </tr>
      <tr>
        <td> 非 Nan </td>
        <td> df.notnull(obj) </td>
        <td> </td>
        <td> </td>
      </tr>
    </table>
  * 列篩選/縮減
    <table border="1" width="30%">
      <tr>
        <th width="5%">用途</a>
        <th width="10%">語法</a>
        <th width="5%">用途</a>
        <th width="10%">語法</a>        
      </tr>
      <tr>
        <td> 邏輯操作 </td>
        <td> df[df.age>20] </td>
        <td> 移除重複 </td>
        <td> df.drop_duplicates() </td>
      </tr>
      <tr>
        <td> 前 n 筆 </td>
        <td> df.head(n=10) </td>
        <td> 後 n 筆 </td>
        <td> df.tail(n=10)</td>
      </tr>
      <tr>
        <td> 隨機抽樣 </td>
        <td> df.sample(frac=0.5)   # 抽50% <br>
             df.sample(n=10)       # 抽10筆 </td>
        <td> 行第 n 到 m 筆的資料 </td>
        <td> df.iloc[n:m] </td>
      </tr>
      <tr>
        <td> 行第 n 到 m 筆且列第 a 到 b 筆的資料 </td>
        <td> df.iloc[n:m, a:b] </td>
        <td> </td>
        <td> </td>
      </tr>
    </table>
  * 欄篩選/縮減
    <table border="1" width="30%">
      <tr>
        <th width="5%">用途</a>
        <th width="10%">語法</a>
        <th width="5%">用途</a>
        <th width="10%">語法</a>        
      </tr>
      <tr>
        <td> 單一欄位 </td>
        <td> df['col1'] 或 df.col1 </td>
        <td> 複數欄位 </td>
        <td> df[['col1', 'col2', 'col3']] # </td>
      </tr>
      <tr>
        <td> Regex 篩選 </td>
        <td> df.filter(regex=...) </td>
        <td> </td>
        <td> </td>
      </tr>
    </table>
* Group operations：常用在計算「組」統計值時會用到的功能
  * 自訂：sub_df_object = df.groupby(['col1'])
  * 應用
    <table border="1" width="30%">
      <tr>
        <th width="5%">用途</a>
        <th width="10%">語法</a>
        <th width="5%">用途</a>
        <th width="10%">語法</a>        
      </tr>
      <tr>
        <td> 計算各組的數量 </td>
        <td> sub_df_object.size() </td>
        <td> 得到各組的基本統計值 </td>
        <td> sub_df_object.describe() </td>
      </tr>
      <tr>
        <td> 根據 col1 分組後，計算 col2 統計值(平均值、最大值、最小值等) </td>
        <td> sub_df_object['col2'].mean() </td>
        <td> 對依 col1 分組後的 col2 引用操作 </td>
        <td> sub_df_object['col2'].apply() </td>
      </tr>
      <tr>
        <td> 對依 col1 分組後的 col2 繪圖 (hist 為例) </td>
        <td> sub_df_object['col2'].hist() </td>
        <td> </td>
        <td> </td>
      </tr>
    </table>
    
* 參考資料
  * [Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)
* 範例與作業
  * [範例D013](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_013_HW_%E5%B8%B8%E7%94%A8%E7%9A%84%20DataFrame%20%E6%93%8D%E4%BD%9C/Day_013_dataFrame_operation.ipynb)
    * DataFrame 的黏合 (concat)
    * 使用條件篩選出 DataFrame 的子集合
    * DataFrame 的群聚 (groupby) 的各種應用方式
  * [作業D013](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_013_dataFrame_operation_Ans.ipynb) 

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D014-程式實作EDA-相關係數簡介
* 相關係數
  * 常用來了解各欄位與我們想要預測的目標之間關係的指標
  * 衡量兩個隨機變量之間線性關係的強度和方向
  * 數值介於 -1~1 之間的值，負值代表負相關，正值代表正相關，數值的大小代表相關性的強度
    * .00-.19：非常弱相關
    * .20-.39：弱相關
    * .40-.59：中度相關
    * .60-.79：強相關
    * .80-1.0：非常強相關
* 範例與作業
  * [範例D014](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_014_HW/Day_014_correlation_example.ipynb)
    * 弱相關的相關矩陣與散佈圖之間的關係
    * 正相關的相關矩陣與散佈圖之間的關係
  * [作業D014](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_014_correlation_example_Ans.ipynb)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D015-程式實作EDA-CorrelationCode
* 相關係數(搭配課程內容)
  * 功能
    * 迅速找到和預測目標最有線性關係的變數
    * 搭配散佈圖來了解預測目標與變數的關係
  * 要點
    * 遇到 y 的本質不是連續數值時，應以 y 軸方向呈現 x 變數的 boxplot (高下立見)
    * 檢視不同數值範圍的變數，且有特殊例外情況(離群值)，將 y 軸進行轉換 (log-scale)
* 範例與作業
  * [範例D015](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_015_HW_EDA%20from%20Correlation/Day_015-supplementary_correlation_and_plot_with_different_range.ipynb)
    * 直接列出的觀察方式
    * 出現異常數值的資料調整方式
    * 散佈圖異常與其調整方式
  * [作業D015](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_015_correlation_Ans.ipynb)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D016-EDA-不同數值範圍間的特徵如何檢視
* 繪圖風格：透過設計過的風格，讓觀看者更清楚明瞭，包含色彩選擇、線條、樣式等
  * 語法：詳細圖示差異搭配課程內容
    ```
    plt.style.use('default')    # 不需設定就會使用預設
    plt.style.use('ggplot')
    plt.style.use('seaborn')    # 或採用 seaborn 套件繪圖
    ```
* Kernel Density Estimation ([KDE](http://rightthewaygeek.blogspot.com/2015/09/kernel-density-estimation.html)) 
  * 步驟
    * 採用無母數方法畫出一個觀察變數的機率密度函數
      * 某個 X 出現的機率為何
    * Density plot 的特性
      * 歸一：線下面積和為 1
      * 對稱：K(-u) = K(u)
    * 常用的 kernel function
      * Gaussian esti. (Normal dist)
      * Cosine esti.
      * Triangular esti.
  * 優點
    * 無母數方法，對分布沒有假設 (使用上不需擔心是否有一些常見的特定假設，如分布為常態)
    * 透過 KDE plot，可較為清楚的看到不同組間的分布差異
  * 缺點
    * 計算量大，電腦不好可能跑不動
* 範例與作業
  * [範例D016](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_016_HW_Kernel%20Density%20Estimation_/Day_016_EDA_KDEplots.ipynb)
    * 各種樣式的長條圖(Bar)、直方圖(Histogram)
    * 不同的 KDE 曲線與繪圖設定以及切換不同 Kernel function 的效果
  * [作業D016](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_016_EDA_KDEplots_Ans.ipynb)
    * 調整對應的資料，以繪製分布圖

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D017-EDA-把連續型變數離散化
* 連續型變數離散化：變數較穩定
  * 要點：如每 10 歲一組，若不分組，outlier 會給模型帶來很大的干擾
    * 組的數量
    * 組的寬度
  * 主要方法
    * 等寬劃分(對應 pandas 的 cut)：按照相同寬度將資料分成幾等份，其缺點是受異常值的影響比較大
    * 等頻劃分(對應 pandas 的 qcut)：將資料分成幾等份，每等份資料裡面的個數是一樣的
    * 聚類劃分：使用聚類演算法將資料聚成幾類，每一個類為一個劃分
* 範例與作業
  * [範例D017](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_017_HW/Day_017_discretizing.ipynb)：數據離散化
    * pandas.cut 的等寬劃分效果
    * pandas.qcut 的等頻劃分效果
  * [作業D017](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_017_discretizing_Ans.ipynb)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D018-程式實作EDA-把連續型變數離散化
* 把連續型的變數離散化後，可以搭配 pandas 的 groupby 畫出與預測目標的圖來判斷兩者之間是否有某種關係/趨勢
* 範例與作業
  * [作業D018](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_018_Ans.ipynb)
    * 對較完整的資料生成離散化特徵
    * 觀察上述離散化特徵，對於目標值的預測有沒有幫助

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D019-程式實作-Subplots
* 使用 subplot 的時機：將圖片分格呈現，有助於資訊傳達
  * 有很多相似的資訊要呈現時 (如不同組別的比較)
  * 同一組資料，但想同時用不同的圖型呈現
* 語法：`plt.figure()` 及 `plt.subplot(列-欄-位置)`
  <table border="1" width="10%">
      <tr>
        <th width="5%">第一行</a>
        <th width="5%">第二行</a>       
      </tr>
      <tr>
        <td> plt.subplot(321)：代表在一個 3 列 2 欄的最左上角(列1欄1) </td>
        <td> plt.subplot(322) </td>
      </tr>
      <tr>
        <td> plt.subplot(323) </td>
        <td> plt.subplot(324) </td>
      </tr>
      <tr>
        <td> plt.subplot(325) </td>
        <td> plt.subplot(326) </td>
      </tr>
    </table>

* 參考資料
  * [matplotlib 官方範例](https://matplotlib.org/2.0.2/examples/pylab_examples/subplots_demo.html)
  * [Multiple Subplots](https://jakevdp.github.io/PythonDataScienceHandbook/04.08-multiple-subplots.html)
  * [Seaborn.jointplot](https://seaborn.pydata.org/generated/seaborn.jointplot.html)
* 範例與作業
  * [範例D019](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_019_HW_%E7%A8%8B%E5%BC%8F%E5%AF%A6%E4%BD%9C_subplots/Day_019_EDA_subplots.ipynb)
    * 傳統的 subplot 三碼：row、column、indx 繪製法
    * subplot index 超過 10 以上的繪圖法：透過 for loop
  * [作業D019](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_019_EDA_subplots_Ans.ipynb)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D020-程式實作-HeatmapANDGrid-plot
* Heatmap
  * 常用於呈現變數間的相關性、混和矩陣(confusion matrix)，以顏色深淺呈現
  * 亦可用於呈現不同條件下，數量的高低關係
  * 參考資料
    * [matplotlib 官方範例](https://matplotlib.org/3.2.2/gallery/images_contours_and_fields/image_annotated_heatmap.html)
    * [Seaborn 数据可视化基础教程](https://huhuhang.com/post/machine-learning/seaborn-basic)
* Grid-plot：結合 scatter plot 與 historgram 的好處來呈現變數間的相關程度
  * subplot 的延伸，但 seaborn 做得更好
    ```
    import seaborn as sns
    sns.set(style='ticks', color_codes=True)
    iris = sns.load_dataset('iris')
    g = sns.pairplot(iris)
    ```
    
    * 對角線呈現該變數的分布(distribution)
    * 非對角線呈現兩兩變數間的散佈圖
  * 參考資料
    * [Seaborn 的 Pairplot](https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166)
* 範例與作業
  * [範例D020](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_020_HW_Heatmap%20%26%20Grid-plot/Day_020_EDA_heatmap.ipynb)
    * Heatmap 的基礎用法：相關矩陣的 Heatmap
    * Heatmap 的進階用法：散佈圖、KDE、密度圖
  * [作業D020](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_020_EDA_heatmap_Ans.ipynb)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

### D021-模型-LogisticRegression
* A baseline
  * 最終的目的是要預測客戶是否會違約遲繳貸款的機率，在開始使用任何複雜模型之前，有一個最簡單的模型當作 baseline 是一個好習慣
* Logistic Regression
  * 參考資料：[ML-Logistic Regression-Andrew](https://www.youtube.com/watch?v=-la3q9d7AKQ&list=PLNeKWBMsAzboR8vvhnlanxCNr2V7ITuxy)
* 範例與作業
  * [範例D021](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_021_HW_Logistic%20Regression/Day_021_first_model.ipynb)
    * 資料清理
    * 前處理
    * Heatmap 的進階用法：散佈圖、KDE、密度圖
    * 輸出值的紀錄
  * [作業D021](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_021_first_model_Ans.ipynb)

Back to <a href="#資料清理數據前處理">資料清理數據前處理</a>
<br>
<br>

## 資料科學特徵工程技術
### D022-特徵工程簡介
* [特徵工程](https://www.zhihu.com/question/29316149)
  * 從事實到對應分數的轉換，而非直接轉換成 1 點，點數未必直接對應到總價或機率
  * 資料包含類別型(文字型)特徵以及數值型特徵，最小的特徵工程至少包含一種類別編碼(範例使用標籤編碼)，以及一種特徵縮放方法(範例使用最小最大化)
  * 從原始資料中提取有價值的資訊
* 建模語法
  * 讀取資料：df_train、df_test
  * 分解重組與轉換：將 df_train、df_test 合併為 df   
  * 特徵工程：針對 df 進行轉換
    * Label Encoder
    * MinMax Encoder
  * 訓練模型與預測
    * train_X、train_Y：訓練模型
    * test_X：模型預測，可得到 pred
  * 合成提交檔：將預測結果存成 csv 檔
  * 補充
    * 特徵重要性評估通常發生在模型後
    * 可再用來跌代調整模型
* 資料處理
  * 減少：詳見資料前處理
    * 資料清理
    * 離群值處理
  * 增加
    * 基於事實的特徵轉換
      * 標記 Label
    * 基於數值的特徵轉換
      * 標準化、區間縮放
      * 二值化(連續數值->離散類別)
* 參考資料
  * [特徵工程實例說明【Cupoy A咖共學】](https://www.youtube.com/watch?v=ZhyfKVvoK7I&t=261s)
* 範例與作業
  * [範例D022](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_022_HW_%E7%89%B9%E5%BE%B5%E5%B7%A5%E7%A8%8B%E7%B0%A1%E4%BB%8B/Day_022_Introduction_of_Feature%20Engineering.ipynb)
    * 特徵工程：補缺失值(fillna)、標籤編碼(LabelEncoder)【類別】、最小最大化(MinMaxScaler)【數值】
  * [作業D022](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_022_Introduction_of_Feature%20Engineering_Ans.ipynb)

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D023-特徵工程-數值型-去除偏態
* 去除[偏態](https://blog.csdn.net/u013555719/article/details/78530879)
  * 範例：希望當掉的同學部要太多
    * 標準化(平移)：高低分群體還是分得太明顯
    * 去離群值：高分群的努力都白費，不公平
    * 去除偏態：近似常態分佈(左右對稱、集中點在中央，讓平均值具有代表性)，如開根號乘以 10
  * 使用時機：當離群資料比例太高，或者平均值沒有代表性
  * 方法
    * 對數去偏(log1p)
      * 常見於計數/價格這類非負且可能為 0 的欄位
      * 有 0 時應加 1 (plus one) 再取對數 (log)
      * 還原時先取指數 (exp) 後再減 1 (minus one)
    * 方根去偏(sqrt)
      * 將數值減去最小值後開根號，最大值有限時適用 (如成績轉換)
    * 分布去偏(boxcox)：λ 要介於 0 到 0.5 之間，轉化前的數值不可小於等於 0
      * 函數的 lambda(λ) 參數為 0 時等於 log 函數，lambda(λ) 為 0.5 時等於開根號 (即sqrt)，因此可藉由參數的調整更靈活地轉換數值，但要特別注意 Y 的輸入數值必須要為正 (不可為0)
      * 當 λ≠0，y(λ) = (y^λ-1)/λ；當 λ=0，y(λ) = ln(y)
        <table border="1" width="9%">
          <tr>
            <th width="2%"> λ </a>
            <th width="2%"> Y </a>
            <th width="5%"> Y </a>
          </tr>
          <tr>
            <td> -2 </td>
            <td> 1/Y^2 </td>
            <td>  </td>
          </tr>
          <tr>
            <td> -1 </td>
            <td> 1/Y </td>
            <td> inverse transformation </td>
          </tr>
          <tr>
            <td> -0.5 </td>
            <td> 1/Y^(1/2) </td>
            <td>  </td>
          </tr>
          <tr>
            <td> 0 </td>
            <td> log(Y) </td>
            <td> logarithmic transformation </td>
          </tr>
          <tr>
            <td> 0.5 </td>
            <td> Y^(1/2) </td>
            <td> square root transformation </td>
          </tr>
          <tr>
            <td> 1 </td>
            <td> Y </td>
            <td> no transformation </td>
          </tr>
          <tr>
            <td> 2 </td>
            <td> Y^(2) </td>
            <td> quadratic transformation </td>
          </tr>
        </table>
       
* 範例與作業
  * [範例D023](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_023_HW_%E5%8E%BB%E9%99%A4%E5%81%8F%E6%85%8B/Day_023_Reduce_Skewness.ipynb)
    * DataSet：房價預測
    * 觀察原始數值的散佈圖及線性迴歸分數
    * 使用 log1p 降偏態時，對於分布與迴歸分數的影響
    * 使用 box-cox (λ=0.15)時，對於分布與迴歸分數的影響
    * 使用 sqrt (box-cox, λ=0.5)時，對於分布與迴歸分數的影響
  * [作業D023](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_023_Reduce_Skewness_Ans.ipynb)
    * DataSet：鐵達尼生存預測

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D024-特徵工程-類別型-基礎處理
* 類別型特徵處理
  * 標籤編碼 (Label Encoding)
    * 類似於流水號，依序將新出現的類別依序編上新代碼
    * 缺點：分數的大小順序只與轉換對照有關，與標籤的意義無直接相關
  * 獨熱編碼 (One Hot Encoding)
    * 改良數字大小沒有意義的問題，將不同的類別分別獨立為一欄
    * 缺點：需較大的記憶空間與計算時間，且類別數量越多時越嚴重
  * [比較](https://www.twblogs.net/a/5baab6e32b7177781a0e6859?lang=zh-cn)
    <table border="1" width="9%">
          <tr>
            <th width="2%">  </a>
            <th width="2%"> 儲存空間/計算時間 </a>
            <th width="5%"> 適用學習模型 </a>
          </tr>
          <tr>
            <td> 標籤編碼 (Label Encoding) </td>
            <td> 小 </td>
            <td> 非深度學習模型，如樹狀模型(隨機森林/梯度提升樹) </td>
          </tr>
          <tr>
            <td> 獨熱編碼 (One Hot Encoding) </td>
            <td> 較大 </td>
            <td> ● 深度學習模型，主要依賴倒傳遞，標籤編碼不易收斂 <br>
                 ● 當特徵重要性高，且可能值較少，才考慮</td>
          </tr>
    </table>

* 範例與作業
  * [範例D024](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_024_HW_%E9%A1%9E%E5%88%A5%E5%9E%8B%E7%89%B9%E5%BE%B5%E8%99%95%E7%90%86/Day_024_LabelEncoder_and_OneHotEncoder.ipynb)
    * DataSet：房價預測
    * 使用標籤編碼與獨熱編碼
      * 在特徵數量/線性迴歸分數/線性迴歸時間上，有什麼影響
      * 在特徵數量/梯度提升樹分數/梯度提升樹時間上，有什麼影響
  * [作業D024](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_024_LabelEncoder_and_OneHotEncoder_Ans.ipynb)    
    * DataSet：鐵達尼生存預測
    * 使用標籤編碼與獨熱編碼
      * 在特徵數量/邏輯斯迴歸分數/邏輯斯迴歸時間上，有什麼影響

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D025-特徵工程-類別型-均值編碼
* [均值編碼](https://zhuanlan.zhihu.com/p/26308272)：用 cross validation 確認使用前後分數
  * 類別特徵看起來與目標值有顯著相關，容易 overfitting
  * 使用同一類別目標的平均值取代原本的類別型特徵
    * 平滑化 (Smoothing)
      * 交易樣本非常少，且剛好抽到極端值，平均結果可能會誤差很大
      * 問題：需考慮紀錄筆數，當作可靠度的參考
        * 當平均值的可靠度低時，相信全部的總平均
        * 當平均值的可靠度高時，相信類別的平均
        * 依照紀錄筆數，在上述兩者間取折衷
      * 公式
        * 新類別均值 = (原類別平均*類別樣本數+全部的總平均*調整因子)/(類別樣本數+調整因子)
        * 其中，調整因子：依總樣本數調整
* 範例與作業
  * [範例D025](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_025_HW_%E9%A1%9E%E5%88%A5%E5%9E%8B%E7%89%B9%E5%BE%B5%E5%9D%87%E5%80%BC%E7%B7%A8%E7%A2%BC/Day_025_Mean_Encoder.ipynb)
    * DataSet：房價預測
    * 使用標籤編碼與均值編碼
      * 在特徵數量/線性迴歸分數/線性迴歸時間上，有什麼影響
      * 在特徵數量/梯度提升樹分數/梯度提升樹時間上，有什麼影響
  * [作業D025](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_025_Mean_Encoder_Ans.ipynb)
    * DataSet：鐵達尼生存預測

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D026-特徵工程-類別型-其他進階處理
* 計數編碼(Counting)
  * 情境：若類別的目標均價與類別筆數呈正相關 (或負相關或高度相關)，也可將筆數本身當成特徵，如自然語言處理時，字詞的計數編碼又稱詞頻
  * 計算方式：計算類別在資料中出現的次數
* 特徵雜湊([Feature Hash](https://blog.csdn.net/laolu1573/article/details/79410187))
  * 情境：相異類別的數量非常龐大，且可能產生新的類別
  * 計算方式
    * 將原始值 hash 後取餘數
    * 記錄到 hash table
    * hash table 的長度就是增加的特徵數目
      ```
      function hashing_vectorizer(features : array of string, N : integer):
          x := new vector[N]
          for f in features:
              h := hash(f)
              x[h mod N] += 1
          return x
      ```
  * 方法
    * 將類別由[雜湊函數](https://en.wikipedia.org/wiki/Hash_function)定應到一組數字
    * 調整雜湊函數對應值的數量
    * 在計算空間/時間與鑑別度間取折衷
    * 提高訊息密度，減少無用的標籤
  * 優缺點
    * 優點
      * 相較於 one-hot encoding，不需要預先維護一個變量表
      * 可以處理新的類別
    * 缺點
      * 可能會把多個原始類別值 hash 到相同的位置上，出現碰撞 (hash collision)，不過實際實驗表明這種衝突對算法的精度影響很小。將高維稀疏的解空間壓縮到低維稠密的空間，壓縮到的維度越低，越容易發生碰撞，此時可通過使用多個 hash function 來減少碰撞造成的信息損失
* bag-of-words
  * 方法：蒐集所有的詞彙當成特徵，每個句子在有出現的詞彙特徵標 1
  * 缺點：當句子的詞彙不固定時,特徵會就爆炸性成長，且也無法處理新出現的詞彙。而且資料分佈可能很稀疏
* 嵌入式編碼(Embedding)：與深度學習相關，在此課程不談
* 範例與作業
  * [範例D026](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_026_HW_%E9%A1%9E%E5%88%A5%E5%9E%8B%E7%89%B9%E5%BE%B5%E9%80%B2%E9%9A%8E%E8%99%95%E7%90%86/Day_026_CountEncoder_and_FeatureHash.ipynb)
    * DataSet：鐵達尼生存預測，欄位：Ticket
    * 使用計數編碼，搭配邏輯斯迴歸對於測結果有什麼影響
    * 使用雜湊編碼及計數編碼+雜湊編碼，搭配邏輯斯迴歸對於測結果有什麼影響
  * [作業D026](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_026_CountEncoder_and_FeatureHash_Ans.ipynb)
    * DataSet：鐵達尼生存預測，欄位：Cabin

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D027-特徵工程-時間型
* 時間特徵分解
  * 年、月、日、時、分、秒、第幾周、星期幾
  * 週期循環特徵：關鍵在於首尾相接，須使用正弦函數(sin)或餘弦函數(cos)加以組合
    <table border="1" width="9%">
          <tr>
            <th width="2%"> 週期 </a>
            <th width="5%"> 相關性 </a>
            <th width="3%"> 正負意義 </a>
            <th width="5%"> 關係 </a>
          </tr>
          <tr>
            <td> 年 </td>
            <td> (季節)與春夏秋冬季節溫度相關 </td>
            <td> 正：冷/負：熱 </td>
            <td> cos((月/6 + 日/180 )π) </td>
          </tr>
          <tr>
            <td> 月 </td>
            <td> 與薪水、繳費相關 </td>
            <td>  </td>
            <td>  </td>
          </tr>
          <tr>
            <td> 周 </td>
            <td> (例假日)與周休、消費習慣相關 </td>
            <td> 正：精神飽滿/負：疲倦 </td>
            <td> sin((星期幾/3.5 + 小時/84 )π) </td>
          </tr>
          <tr>
            <td> 日 </td>
            <td> (日夜與生活作息)與生理時鐘相關 </td>
            <td> 正：精神飽滿/負：疲倦 </td>
            <td> sin((小時/12 + 分/720 + 秒/43200 )π) </td>
          </tr>
    </table>
 
  * 時段特徵
    * 短暫時段內的事件計數，也可能影響事件發生的機率，如：網站銷售預測，點擊網站前 10分鐘/1小時/1天的累計點擊量
  * Python 使用 datetime 
    * [時間日期處理](https://wklken.me/posts/2015/03/03/python-base-datetime.html)
    * [Basic date and time types](https://docs.python.org/3/library/datetime.html)
* 範例與作業
  * [範例D027](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_027_HW_%E6%99%82%E9%96%93%E5%9E%8B%E7%89%B9%E5%BE%B5/Day_027_DayTime_Features.ipynb)
    * DataSet：計程車費率預測，欄位：pickup_datetime
    * 觀察時間特徵分解，在線性迴歸分數/梯度提升樹分數上，分別有什麼影響
    * 觀察加入週期循環特徵，在線性迴歸分數/梯度提升樹分數上，分別有什麼影響
  * [作業D027](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_027_DayTime_Features_Ans.ipynb)
    * DatSet：計程車費率預測，欄位：fire_amount
    * 新增特徵：星期幾(day of week)與第幾周(week of year)

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D028-特徵工程-數值與數值組合
* 合成特徵(synthetic feature)
  * 起點+終點位置：方向、距離
    * 經緯度 -> 平面座標距離 -> 資料緯度集中在 40.75 度附近，可以算得經度與緯度代表的長度比為 cos(40.75度):1 = 0.75756:1，由此校正兩地距離
  * 多個頂點位置：周長、面積
  * 開始時間+結束時間：經過時長
  * 長 x 寬：面積(如iris資料集的花瓣長、寬)
* 特徵組合(feature cross)：對分線性規律進行編碼
  * 種類
    * [A X B]：將兩個特徵的值相乘形成特徵組合
    * [A x B x C x D x E]：將五個特徵的值相乘形成特徵組合
    * [A x A]：將單個特徵的值求平方形成特徵組合
  * 組合獨熱矢量
* 參考資料
  * [特徵組合&特徵交叉 (Feature Crosses)](https://segmentfault.com/a/1190000014799038)
  * [簡單高效的組合特徵自動挖掘框架](https://read01.com/jj8em6E.html#.ZBv4VnZBy5c)
* 範例與作業
  * [範例D028](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_028_HW_%E7%89%B9%E5%BE%B5%E7%B5%84%E5%90%88_%E6%95%B8%E5%80%BC%E8%88%87%E6%95%B8%E5%80%BC%E7%B5%84%E5%90%88/Day_028_Feature_Combination.ipynb)
    * DatSet：計程車費率預測
    * 增加特徵：經度差與緯度差，觀察線性迴歸與梯度提升樹的預測結果有什麼影響
    * 增加座標距離特徵，觀察線性迴歸與梯度提升樹的預測結果有什麼影響
  * [作業D028](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_028_Feature_Combination_Ans.ipynb)
    * DatSet：計程車費率預測
    * 增加特徵：經緯度一圈的長度比，觀察影響性 

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D029-特徵工程-類別與數值組合
* 群聚編碼(Group by Encoding)
  * 數值型特徵之間能合成新特徵，而類別與數值型之間也能類似均值編碼的概念，如取類別型中的平均值(Mean)、中位數(Median)、眾數(Mode)、最大值(Max)、最小值(Min)、次數(Count)等可取代類別作為編碼
  * 重要性(D031)
    * 與數值特徵組合相同，先以「領域知識」或「特徵重要性」挑選強力特徵後，再將特徵組成更強的特徵，兩個特徵都是數值就用特徵組合，其中之一是類別型就用聚類編碼
    * 機器學習的特徵是「寧濫勿缺」的，因為以前以非樹狀模型為主，為了避免共線性，會很注意類似的特徵不要增加太多，但現在強力的模型都是樹狀模型，所以只要有可能就通通做特徵囉！
  * 與均值編碼 (Mean Encoding) 的比較
    <table border="1" width="9%">
          <tr>
            <th width="3%"> 名稱 </a>
            <th width="3%"> 均值編碼 Mean Encoding </a>
            <th width="3%"> 群聚編碼 Group by Encoding </a>
          </tr>
          <tr>
            <td> 平均對象 </td>
            <td> 目標值 </td>
            <td> 其他數值型特徵 </td>
          </tr>
          <tr>
            <td> 過擬合 Overfitting </td>
            <td> 容易 </td>
            <td> 不容易 </td>
          </tr>
          <tr>
            <td> 對均值平滑化 Smoothing </td>
            <td> 需要 </td>
            <td> 不需要 </td>
          </tr>
    </table>
  * 參考資料
    * [利用python数据分析之数据聚合与分组](https://zhuanlan.zhihu.com/p/27590154)
* 範例與作業
  * [範例D029](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_029_HW_%E7%89%B9%E5%BE%B5%E7%B5%84%E5%90%88_%E9%A1%9E%E5%88%A5%E8%88%87%E6%95%B8%E5%80%BC%E7%B5%84%E5%90%88/Day_029_GroupBy_Encoder.ipynb)
    * DataSet：房價預測
    * 了解群聚編碼的語法
    * 觀察群聚編碼，搭配線性迴歸以及隨機森林分別有什麼影響
  * [作業D029](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_029_GroupBy_Encoder_Ans.ipynb)
    * DataSet：鐵達尼生存預測
    * 挑選特徵的群聚編碼
    * 觀察群聚編碼，搭配邏輯迴歸，看看有什麼影響

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D030-特徵選擇
* 特徵選擇
  * 概念：特徵需適當的增加與減少，以提升精確度並減少計算時間
    * 增加特徵：特徵組合(Day028)、群聚編碼(Day029)
    * 減少特徵：特徵選擇(Day030)
  * 方法
    * 過濾法(Filter)：選定統計數值與設定門檻，刪除低於門檻的特徵
      * 相關係數過濾法
        * 找到目標值後，觀察其他特徵與目標值的相關係數
        * 預設顏色越紅表越正相關，越藍越負相關
        * 刪除顏色較淺的特徵：訂出相關係數門檻值，特徵相關係數絕對值低於門檻者刪除
    * 包裝法(Wrapper)：根據目標函數，逐步加入特徵或刪除特徵
    * 嵌入法(Embedded)：使用機器學習模型，根據擬合後的係數，刪除係數低於門檻的特徵
      * L1(Lasso)嵌入法
        * 使用 Lasso Regression 時，調整不同的正規化程度，就會自然使得一部分的特徵係數為０，因此刪除的是係數為０的特徵，不須額外指定門檻，但需調整正規化程度
      * GDBT(梯度提升樹)嵌入法
        * 使用梯度提升樹擬合後，以特徵在節點出現的頻率當作特徵重要性，以此刪除重要性低於門檻的特徵
        * 由於特徵重要性不只可以刪除特徵，也是增加特徵的關鍵參考
  * 比較
    <table border="1" width="12%">
          <tr>
            <th width="3%"> 方法 </a>
            <th width="3%"> 計算時間 </a>
            <th width="3%"> 共線性 </a>
            <th width="3%"> 特徵穩定性 </a>
          </tr>
          <tr>
            <td> 相關係數過濾法 </td>
            <td> 快速 </td>
            <td> 無法排除 </td>
            <td> 穩定 </td>
          </tr>
          <tr>
            <td> Lasso 嵌入法 </td>
            <td> 快速 </td>
            <td> 能排除 </td>
            <td> 不穩定 </td>
          </tr>
          <tr>
            <td> GDBT 嵌入法 </td>
            <td> 較慢，Xgboost等改善計算時間，成為特徵選擇的主流 </td>
            <td> 能排除 </td>
            <td> 穩定 </td>
          </tr>
    </table>
  * 參考資料
    * [特徵選擇](https://zhuanlan.zhihu.com/p/32749489)
* 範例與作業
  * [範例D030](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_030_HW_%E7%89%B9%E5%BE%B5%E9%81%B8%E6%93%87/Day_030_Feature_Selection.ipynb)
    * DataSet：房價預測
    * 觀察相關係數過濾法對線性迴歸與梯度提升機有什麼影響
    * 觀察 L1 嵌入法對線性迴歸與梯度提升機有什麼影響
  * [作業D030](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_030_Feature_Selection_Ans.ipynb)
    * DataSet：鐵達尼生存預測
    * 相關係數過濾法有時候確實能成功提升準確度，但篩選過頭會有反效果
    * 相關係數過濾法的門檻、L1 Embedding 的 alpha 值(去除0)沒有一定的決定準則

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D031-特徵評估
* 樹狀模型的特徵重要性
  * 特徵重要性預設方式是取「特徵決定分支的次數」，範例中坪數x1次、房間數x3次、靠近捷運站x2次、屋齡x1次，故最重要的特徵是「房間數」
  * 更直覺的特徵重要性：特徵覆蓋度、損失函數降低量
    * 範例中，坪數與房間數的覆蓋度相同 (都是8)
    * 損失函數降低量則看損失函數 (loss function) 決定
  * 比較
    <table border="1" width="15%">
          <tr>
            <th width="3%"> 方法 </a>
            <th width="3%"> Xgboost對應參數 (importance_type) </a>
            <th width="3%"> 計算時間 </a>
            <th width="3%"> 估計精確性 </a>
            <th width="3%"> Sklearn 有此功能 </a>
          </tr>
          <tr>
            <td> 分支次數 </td>
            <td> weight </td>
            <td> 最快 </td>
            <td> 最低 </td>
            <td> 有 </td>
          </tr>
          <tr>
            <td> 分支覆蓋度 </td>
            <td> cover </td>
            <td> 快 </td>
            <td> 中 </td>
            <td> 無 </td>
          </tr>
          <tr>
            <td> 損失降低量(資訊增益度) </td>
            <td> gain </td>
            <td> 較慢 </td>
            <td> 最高 </td>
            <td> 無 </td>
          </tr>
    </table>
* [機器學習](https://juejin.cn/post/6844903517799317511)中的優化循環
  * 原始特徵 → 進階版 GDBT 模型擬合 → 用特徵重要性增刪特徵【特徵選擇(刪)、特徵組合(增)】 → 交叉驗證(cross validation)確認特徵效果是否改善 → (Y) 進階版 GDBT 模型擬合、(N) 用特徵重要性增刪特徵
* 比較特徵 vs. 排列重要性
  <table border="1" width="15%">
          <tr>
            <th width="2%"> 比較類型 </a>
            <th width="5%"> 特徵重要性 Feature Importance </a>
            <th width="5%"> 排序重要性 Permutation Importance </a>
          </tr>
          <tr>
            <td> 適用模型 </td>
            <td> 限定樹狀模型 </td>
            <td> 機器學習模型 </td>
          </tr>
          <tr>
            <td> 計算原理 </td>
            <td> 樹狀模型的分歧特徵 </td>
            <td> 打散原始資料中單一特徵的排序 </td>
          </tr>
          <tr>
            <td> 額外計算時間 </td>
            <td> 較短 </td>
            <td> 較長 </td>
          </tr>
  </table>
 
* 範例與作業
  * [範例D031](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_031_HW_%E7%89%B9%E5%BE%B5%E8%A9%95%E4%BC%B0/Day_031_Feature_Importance.ipynb)
    * DataSet：房價預測
    * 使用擬合過的模型，計算特徵重要性
    * 對照原始特徵，觀察特徵重要性較高的一半特徵，搭配隨機森林對於預測結果的影響
    * 重組重要性最高的特徵作為新特徵，觀察效果如何
  * [作業D031](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_031_Feature_Importance_Ans.ipynb)
    * DataSet：鐵達尼生存預測

Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>

### D032-特徵優化-分類型-葉編碼
* 分類預測的集成
  * 分類的預測結果，在意義上是對的預估機率
  * 範例：估計鐵達尼號上的生存機率，已知來自法國的旅客生存機率是 0.8，且年齡 40 到 50 區間的生存機率也是 0.8，那麼同時符合兩種條件的旅客，生存機率應該是多少呢？
    * 邏輯斯迴歸 (logistic regression) 可理解成「線性迴歸 + Sigmoid 函數」，而「Sigmoid 函數」理解成「成功可能性與機率的交換」，可能性正表示更可能，負表示較不可能
    * 使用 Sigmoid 反函數將機率重新轉為可能性的值，相加完再用 Sigmoid 函數轉回機率，以此例為例，最後加成的結果是介於 0.9 到 1 之間的機率
* 葉編碼 (leaf encoding) 原理
  * 樹狀模型做出預測時，會將資料重新分成好幾個區塊，決策樹最末端的點稱之為葉點，每個葉點的性質接近，可視為資料的一種分組方式
  * 雖然不適合直接沿用樹狀模型機率，但分組方式有代表性，因此按照葉點將資料「離散化」，比之前提過的離散化方式更精確，這樣的編碼稱為「葉編碼」
  * 葉編碼 (leaf encoding) 是採用決策樹的葉點作為編碼依據重新編碼，每棵樹視為一個新特徵，每個新特徵均為分類型特徵，決策樹的葉點與該特徵標籤一一對應，最後再以邏輯斯迴歸(重新賦予機率)或分解機 (Factorization Machine) 進行合併預測
  * 說明
    * 假設有 300 棵樹，每棵數有3片葉
      * 3 => 第 i 筆資料在第 1 棵樹位於第 3 片葉子
      * 2 => 第 i 筆資料在第 2 棵樹位於第 2 片葉子
      * ...
      * 1 => 第 i 筆資料第 300 棵樹位於第 1 片葉子
        > gbdt.apply(train_X)[:, :, 0][0] 
    * 代表第 1 筆資料在 300 棵樹分別的 index 為 [2, 1,...., 0]
    * 將每一棵樹的葉子進行 one-hot encoding，新特徵的長度等於所有樹的葉子數量相加
      > [0,0,1, 0,1,0, ... ,1,0,0] 共有 300*3=900 elements
    * 將上述結果當成新的 input，進行 logistic regression
* 葉編碼 + 邏輯斯迴歸
  * 葉編碼需要先對樹狀模型擬合後才能生成，如已挑選較佳的參數，後續處理效果也會較好
  * 在分類預測中使用樹狀模型，再對這些擬合完的樹狀模型進行葉編碼+邏輯斯迴歸，通常會將預測效果再進一步提升
* 參考資料
  * [Feature transformations with ensembles of trees](https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html)
  * [Algorithm-GBDT Encoder](https://zhuanlan.zhihu.com/p/31734283)
  * [GBDT + LR for Binary Classification](https://towardsdatascience.com/next-better-player-gbdt-lr-for-binary-classification-f8dc6f32628e)
* 範例與作業
  * [範例D032](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_032_HW_%E7%89%B9%E5%BE%B5%E5%84%AA%E5%8C%96_%E8%91%89%E7%B7%A8%E7%A2%BC/Day_032_Leaf_Encoding.ipynb)
    * DataSet：鐵達尼生存預測
    * 了解葉編碼搭配梯度提升樹的效果
    * 觀察葉編碼搭配邏輯斯迴歸的效果
  * [作業D032](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_032_Leaf_Encoding_Ans.ipynb)
    * DataSet：鐵達尼生存預測
    * 觀察葉編碼搭配隨機森林的效果
    * 觀察葉編碼搭配邏輯斯迴歸的效果
    
Back to <a href="#資料科學特徵工程技術">資料科學特徵工程技術</a>
<br>
<br>


## 機器學習基礎模型建立
### D033-機器如何學習
* 機器學習三步驟
  * 定義好模型 (線性迴歸、決策樹、神經網路等)
    * 機器學習模型中會有許多參數 (parameters)，如線性迴歸中的 w (weights) 跟 b (bias)
    * 希望模型產生的 ŷ 跟真實答案的 y 越接近越好
  * 評估模型的好壞
    * 定義一個目標函數 (Objective function) 也可稱作損失函數 (Loss function)，來衡量模型的好壞
    * 線性迴歸模型我們可以使用均方差 (mean square error) 來衡量
    * Loss 越大，代表這組參數的模型預測出的 ŷ 越不準
  * 找出讓訓練目標最佳的模型參數
    * 梯度下降 (Gradient Descent)、增量訓練 (Additive Training) 等演算法，可找到最佳可能模型的參數
  * 模型訓練的目標是將損失函數的損失降至最低
    <table border="1" width="12%">
          <tr>
            <th width="2%"> 類型 </a>
            <th width="5%"> 中文 </a>
            <th width="5%"> 說明 </a>
          </tr>
          <tr>
            <td> Underfitted </td>
            <td> 欠擬合</td>
            <td> 預測未擬合實際結果 </td>
          </tr>
          <tr>
            <td> Good Fit/Robust </td>
            <td>  </td>
            <td> 預測可擬合實際結果 </td>
          </tr>
          <tr>
            <td> Overfitted </td>
            <td> 過度擬合 </td>
            <td> 模型可能學習到資料中的噪音，導致在實際應用時預測失準 </td>
          </tr>
  </table>
  
* 學習曲線 Learning curve
  * 如何知道模型已經過擬合了?
    * 觀察模型對於訓練資料的誤差與測試資料的誤差，是否有改變的趨勢
  * 解決方法
    * 過擬合
      * 增加資料量
      * 降低模型複雜度
      * 使用正規化 (Regularization)：決策樹模型是非常容易過擬合的模型，必須透過適當的正規化來緩解
    * 欠擬合
      * 增加模型複雜度
      * 減輕或不使用正規化
* 參考資料
  * [利用學習曲線診斷模型的偏差和方差](http://bangqu.com/yjB839.html)
* 範例與作業
  * [作業D033](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_033_Ans.ipynb)
    * 影片：[ML Lecture 1:Regression - Case Study](https://www.youtube.com/watch?v=fegAeph9UaA)
  
Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D034-訓練及測試集切分
* 為何需要切分訓練/測試集：透過驗證/測試集評估模型是否過擬合
* 方法：使用 Python 中 Scikit-learn 進行資料切分，透過 [train-test split 函數](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)快速對資料進行切分
* K-fold Cross-validation
  * 若僅做一次訓練/測試集切分，有些資料會沒有被拿來訓練過，因此後續就有 cross-validation 的方法，可以讓結果更為穩定，Ｋ為 fold 數量
  * 每筆資料都曾經當過一次驗證集，再取平均得到最終結果
  * 方法：使用 Python 中 Scikit-learn 進行 Cross-validation，透過 [KFold 函數](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)快速運用 Cross-validation
* 參考資料
  * [ML Lecture 2: Where does the error come from?](https://www.youtube.com/watch?v=D_S6y0Jm6dQ&embeds_euri=https%3A%2F%2Fwww.cupoy.com%2F&source_ve_path=MjM4NTE&feature=emb_title)
* 範例與作業
  * [範例D034](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_034_HW_train_test_split/Day_034_train_test_split.ipynb)
    * 使用 train_test_split 函數進行切分
    * 使用 K-fold Cross-validation 來切分資料
  * [作業D034](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_034_Ans.ipynb)
    * 切出固定大小的資料集：適用於 unblance data
  
Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D035-RegressionVSClassification
* Regression：目標值為實數 (-∞ 至 ∞)
  * 回歸問題是可以轉化為分類問題，模型原本是直接預測身高 (cm)，可更改為預測是否高於中位數 (yes or no)
* Classification：目標值為類別 (0 或 1)
  * 二元分類 (Binary-class)
    * 目標的類別僅有兩個，如詐騙分析 (詐騙用戶 vs. 正常用戶)、瑕疵偵測 (瑕疵 vs. 正常) 
  * 多元分類 (Multi-class)
    * 目標類別有兩種以上，如手寫數字辨識有 10 個類別 (0~9)
* Multi-class vs. Multi-label
  * 當每個樣本都只能歸在一個類別，我們稱之為多分類 (Multi-class) 問題
  * 而一個樣本如果可以同時有多個類別，則稱為多標籤 (Multi-label)
* Regularization
  * 不需要考慮 bias，要考慮讓 function 越平滑僅需要考慮 weight
* Variance vs. Bias
  * Variance：資料分散程度
  * Bias：與目標的誤差程度 → 是否瞄準
  * 模型：希望選擇 bias 和 variance 都小的 model
    * 簡單模型：large bias、small variance，function space 小
    * 複雜模型：small bias、large variance，function space 大
    * overfitting：error 來自於 variance 很大
      * 在 training data 上得到小的 error，在 testing data 上得到大的 error
      * 解決方法：
        * 增加資料量
        * Regularization：新加 term(參數越少越好) 讓 function 平滑 → 會傷害 bias
    * underfitting：error 來自於 bias 很大
      * model 無法 fit examples
      * 解決方法：redesign model
        * 增加 feature as input
        * 使用更複雜的 model
* 參考資料
  * [ML Lecture 1:Regression - Case Study](https://www.youtube.com/watch?v=fegAeph9UaA)
  * [ML Lecture 2: Where does the error come from?](https://www.youtube.com/watch?v=D_S6y0Jm6dQ&embeds_euri=https%3A%2F%2Fwww.cupoy.com%2F&source_ve_path=MjM4NTE&feature=emb_title)
  * [Supervised classification和Regression的比較](http://zylix666.blogspot.com/2016/06/supervised-classificationregression.html)
* 範例與作業
  * [作業D035](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_035_Ans.ipynb)
    * 無程式撰寫
  
Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D036-評估指標選定EvaluationMetrics
* 評估指標
  * 最常見的為準確率 (Accuracy) = 正確分類樣本數/總樣本數
  * 迴歸
    * 目的：觀察「預測值」 (Prediction) 與「實際值」 (Ground truth) 的差距
    * 指標
      * MAE, Mean Absolute Error, 範圍：[0, ∞]
      * MSE, Mean Square Error, 範圍：[0, ∞]
      * [常用]R-square, 範圍：[0, 1] 
  * 分類
    * 目的：觀察「預測值」 (prediction) 與「實際值」 (Ground truth) 的正確程度
    * 指標
      * [二元、不平衡]AUC, Area Under ROC Curve, 範圍：[0, 1]
        * 通常分類問題都需要定一個閾值 (threshold) 來決定分類的類別 (通常為機率 > 0.5 判定為 1, 機率 < 0.5 判定為 0)
        * AUC 是衡量曲線下的面積，因此可考量所有閾值下的準確性
        * AUC 越大則表示模型的效能越好
      * [不平衡]F1 - Score (Precision, Recall), 範圍：[0, 1] 
        * 分類問題中，有時會對某一類別的準確率特別有興趣，如瑕疵/正常樣本分類，希望任何瑕疵樣本都不能被漏掉
        * F1-Score 是 Precision、Recall 的調和平均數
          * Precision
            * 模型判定瑕疵，佔樣本確實為瑕疵的比例 
            * 白話：被分類器挑選(selected)出來的正體樣本究竟有多少是真正的樣本
            * = true positives/(true+false positives)
          * Recall
            * 模型判定的瑕疵，佔樣本所有瑕疵的比例
            * 白話：在全部真正的樣本裡面分類器選了多少個
            * = true positives/(true positives + false negative)
      * 混淆矩陣 (Confusion Matrix)
        * 縱軸為模型預測，橫軸為正確答案，可清楚看出每個 Class 間預測的準確率，完美的模型就會在對角線上呈現 100 % 的準確率
      * [平衡]Accuracy
      * [多元]top-k accuracy
        * k 代表模型預測前 k 個類別有包含正確類別即為正確 (ImageNet 競賽通常都是比 [Top-5 Accuracy](https://www.zhihu.com/question/36463511))
    * 比較
      * AUC 計算時 y_pred 的值必須填入每個樣本的預測機率 (probability) 而非分類結果
      * F1-Score 計算時則需填入每個樣本已分類的結果，如機率 >=0.5 則視為 1，而非填入機率值
* 參考資料
  * [深入了解超常用的指標 AUC](https://www.dataschool.io/roc-curves-and-auc-explained/)
  * [機器學習模型評估](https://zhuanlan.zhihu.com/p/30721429)
* 範例與作業
  * [範例D036](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_036_HW_evaluation_metrics/Day_036_evaluation_metrics.ipynb)
    * 觀察各指標的數值範圍，及輸入函數中的資料格式
  * [作業D036](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_036_Ans.ipynb)
    * F1-Score 其實是 F-Score 中的 β 值為 1 的特例
    * 撰寫 F2-Score 的程式碼
  
Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D037-RegressionModel-線性迴歸AND羅吉斯回歸
* 線性迴歸模型(Linear Regression)
  * 特性
    * 用於迴歸問題
    * 訓練速度非常快，但須注意資料共線性、資料標準化等限制
    * 可作為 baseline 模型作為參考點
* 羅吉斯回歸模型(Logistics Regression)
  * 特性
    * 用於二元分類模型，將線性迴歸加上 Sigmoid 函數
* 範例與作業
  * [作業D037](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_037_Ans.ipynb)
    * 無程式撰寫
  
Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D038-程式實作-線性迴歸AND羅吉斯回歸
* 線性迴歸
  * 語法
    > from sklearn.linear_model import LinearRegression <br>
    > reg = LinearRegression().fit(X, y)
* 羅吉斯回歸
  * 語法
    > from sklearn.linear_model import LogisticRegression <br>
    > reg = LogisticRegression().fit(X, y)
  * 參數
    * Penalty：'L1'、'L2'。使用 L1 或 L2 的正則化參數，後續有更詳細介紹
    * C：正則化的強度，數字越小，模型越簡單
    * [Solver](https://blog.csdn.net/lc574260570/article/details/82116197)：對損失函數不同的優化方法
    * Multi-class：選擇 one-vs-rest 或 multi-nominal 分類方式，當目標是 multi-class 時要特別注意，若有 10 個 class， ovr 是訓練 10 個二分類模型，第一個模型負責分類 (class1, non-class1)；第二個負責 (class2, non-class2)，以此類推
* 範例與作業
  * [範例D038](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_038_HW_regression%20model/Day_038_regression_model.ipynb)
    * 理解 linear regression 的參數意義
    * 觀察 linear regression 與 logistic regression 有什麼差異
  * [作業D038](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_038_Ans.ipynb)
    * 確定資料集的目標是分類還是迴歸，選擇正確的模型

Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D039-RegressionModel-LASSO回歸ANDRidge回歸
* 目標函數中重要的知識
  * 損失函數 (Loss function)：衡量預測值與實際值的差異，讓模型能往正確的方向學習
  * 正則化 (Regularization)：
    * 說明
      * 可懲罰模型的複雜度，避免模型變得過於複雜，造成過擬合 (Over-fitting)，當模型越複雜時其值就會越大
      * 希望模型的參數數值不要太大，原因是參數的數值變小，噪音對最終輸出的結果影響越小，提升模型的泛化能力，但也讓模型的擬合能力下降
    * 類型
      * L1 函數：$$ \alpha \sum |weights| $$
      * L2 函數：$$ \alpha \sum (weights)^2 $$
  * 為了避免 Over-fitting，目標函數 = 損失函數 + 正則化
* LASSO 回歸
  * Linear Regression 加上 L1，其中有個超參數 α 可以調整正則化的強度
  * L1 regularization 會讓模型變得較為稀疏，除了能做特徵選取外，也會讓模型變得更輕量，速度較快
* Ridge 回歸
  * Linear Regression 加上 L2，其中有個超參數 α 可以調整正則化的強度
* 範例與作業
  * [作業D039](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_039_Ans.ipynb)
    * 瞭解 L1、L2 的意義與差異
    * 理解 LASSO 與 Ridge 之間的差異與使用情境

Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D040-程式實作-LASSO回歸ANDRidge回歸
* LASSO回歸
  * 語法
    > from sklearn.linear_model import Lasso <br>
    > reg = Lasso(alpha=0.1) <br>
    > reg.fit(X, y) <br>
    > print(reg.coef_) # 印出訓練後的模型參數
* Ridge回歸
  * 語法
    > from sklearn.linear_model import Ridge <br>
    > reg = Ridge (alpha=0.1) <br>
    > reg.fit(X, y) <br>
    > print(reg.coef_) # 印出訓練後的模型參數
* 範例與作業
  * [範例D040](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_040_HW_lasso_ridge_regression/Day_040_lasso_ridge_regression.ipynb)
    * 資料集：糖尿病
    * 比較 linear regression 與 LASOO 的差異
  * [作業D040](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_040_Ans.ipynb)
    * 資料集：boston
    * 比較 linear regression 與 ridge 的差異

Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D041-TreeBasedModel-決策樹DecisionTree
* 決策樹 Decision Tree
  * 透過一系列的是非問題，將資料進行切分
  * 從訓練資料中找出規則，讓每一次決策能使訊息增益 (Information Gain) 最大化，訊息增益越大代表切分後的兩群資料，群內相似程度越高
    * 吉尼係數(Gini-Index)
      $$ Gini = 1-\sum_j p_j^2 $$
    * 熵(Entropy)
      $$ Entropy = -\sum_j p_j log_2 P_j$$
  * 從構建樹的過程中，透過 feature 被用來切分的次數，來得知哪些 features 是相對有用的，實務上可以使用 feature importance 來了解模型如何進行分類，總和為 1
  * 可視覺化每個決策的過程，是個具有非常高解釋性的模型
* 範例與作業
  * [作業D041](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_041_Ans.ipynb)
    * 無程式撰寫

Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D042-程式實作-決策樹
* 類型
  * 回歸型
    > from sklearn.tree_model import DecisionTreeRegressor
  * 分類型
    > from sklearn.tree_model import DecisionTreeClassifier <br>
    > clf = DecisionTreeClassifier()
* 超參數
  * Criterion：衡量資料相似程度的 metric
  * Max_depth：樹能生長的最深限制
  * Min_samples_split：至少要多少樣本以上才進行切分
  * Min_samples_leaf：最終的葉子 (節點) 上至少要有多少樣本
    ```
    from sklearn.tree import DecisionTreeClassifier
    clf = DecisionTreeClassifier(
                  criterion = 'gini',
                  max_depth = None,
                  min_samples_split = 2,
                  min_samples_leaf = 1
          )
    # feature importance
    clf.feature_importances_
    ```
  * 參考資料
    * [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)
    * [資料分類--Decision Tree](https://ithelp.ithome.com.tw/articles/10204450)
* 可安裝額外的套件 [graphviz](https://medium.com/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176)，畫出決策樹的圖形幫助理解模型分類的準則
* 範例與作業
  * [範例D042](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_042_HW_decision_tree/Day_042_decision_tree.ipynb)
    * 資料集：Iris
    * 建立模型四步驟：讀進資料 → 資料切分 → 建立模型 → 評估模型
  * [作業D042](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_042_Ans.ipynb)
    * 資料集：wine、boston
    * 調整 Decision Tree 的超參數是否會影響結果

Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D043-TreeBasedModel-隨機森林RandomForest
* 決策樹的缺點
  * 決策樹生成時考慮所有資料與特徵來做切分的
  * 若不對決策樹進行限制 (樹深度、葉子上至少要有多少樣本等)，決策樹非常容易 Over-fitting
* 隨機森林 (Random Forest)
  * 可以處理分類(classification)問題也可以處理迴歸(regression)問題
  * 集成 (Ensemble) 是將多個模型的結果組合在一起，透過投票或是加權的方式得到最終結果(多數決或平均數)
  * 隨機森林的每一棵樹在生成過程中，都是「隨機」使用「一部份」的訓練資料與特徵，代表每棵樹都是用隨機的資料訓練而成的，且不剪枝(prune) => bootstrap
  * [feature 數](http://hhtucode.blogspot.com/2013/06/ml-random-forest.html)
    * 設定最 少要 bagging 出 (k/2)+1 或 square(k)[有夠多] 的 feature，才比較有顯著結果，k 為原本的 feature 數量
* 範例與作業
  * [作業D043](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_043_Ans.ipynb)
    * 無程式撰寫

Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D044-程式實作-隨機森林
* 語法
  ```
  from sklearn.ensemble import RandomForestClassifier 
  from sklearn.ensemble import RandomForestRegressor 
  clf = RandomForestRegressor()
  ```
* 超參數
  * 與決策樹相同(max_depth, min_samples_split)
  * 生成樹的數量，越多越不容易過度擬和，但運算時間會變長
    ```
    from sklearn.ensemble import RandomForestClassifier
    clf = RandomForestClassifier(    
                n_estimators=10, #決策樹的數量
                criterion="gini",
                max_features="auto", #如何選取 features
                max_depth=10,
                min_samples_split=2,
                min_samples_leaf=1
        )
    ```
  * 如何選取 features：auto
  * 參考資料
    * [分類問題 RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
    * [迴歸問題 RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)
* 範例與作業
  * [範例D044](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_044_HW_random_forest/Day_044_random_forest.ipynb)
    * 資料集：Iris
    * 瞭解隨機森林的建模方法與超參數的意義
  * [作業D044](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_044_Ans.ipynb)
    * 資料集：boston、wine
    * 調整超參數對於隨機森林的影響

Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D045-TreeBasedModel-梯度提升機GradientBoostingMachine
* Gradient Boosting Machine
  * 下一棵樹是為了修正前一棵樹的錯誤，故每一棵樹皆有相關聯
  * GBDT(Gradient Boosting Decision Tree)用來做回歸預測，調整後也可以用于分類
* Boosting 與 Bagging 的差異
    <table border="1" width="15%">
          <tr>
            <th width="5%"> </a>
            <th width="5%"> Bagging </a>
            <th width="5%"> Boosting </a>
          </tr>
          <tr>
            <td> 關係 </td>
            <td> 透過抽樣(sampling)的方式來生成每一棵樹，樹與樹之間是獨立生成的 (Independent classifiers) </td>
            <td> 透過序列(additive)的方式來生成每一顆樹，後面的樹要能夠修正前一棵樹，每棵樹都會與前面的樹有關聯 (Sequential classifiers) </td>
          </tr>
          <tr>
            <td> fitting </td>
            <td> Handles overfitting </td>
            <td> Can overfit </td>
          </tr>
          <tr>
            <td> 優點 </td>
            <td> Reduce variance </td>
            <td> Reduce bias & variance </td>
          </tr>
    </table>
    
  * 參考語法
    ```
    from sklearn import datasets, metrics
    from sklearn.model_selection import train_test_split, KFold, GridSearchCV
    ## 梯度提升樹算法 https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html
    ## 梯度提升用法 補充資料: https://sklearn.apachecn.org/docs/master/12.html 
    from sklearn.ensemble import GradientBoostingRegressor

    # 設定要訓練的超參數組合
    n_estimators = [100, 200, 300, 400, 500]
    max_depth = [1, 3, 5, 7, 9]
    param_grid = dict(n_estimators=n_estimators, max_depth=max_depth)

    ## 建立搜尋物件，放入模型及參數組合字典 (n_jobs=-1 會使用全部 cpu 平行運算)
    ## GridSearchCV:https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
    ## scoring選擇 https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter
    grid_search = GridSearchCV(clf, param_grid, scoring="neg_mean_squared_error", n_jobs=-1, verbose=1)
    # 開始搜尋最佳參數
    grid_result = grid_search.fit(x_train, y_train)
    # 預設會跑 5-fold cross-validadtion，總共 9 種參數組合，總共要 train 27 次模型

    # 印出最佳結果與最佳參數
    print("Best Accuracy: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

    grid_result.best_params_

    # 使用最佳參數重新建立模型
    clf_bestparam = GradientBoostingRegressor(max_depth=grid_result.best_params_['max_depth'], n_estimators=grid_result.best_params_['n_estimators'])
    # 訓練模型
    clf_bestparam.fit(x_train, y_train)

    # 預測測試集
    y_pred = clf_bestparam.predict(x_test)
    y_pred

    # 調整參數後約可降至 8.30 的 MSE
    print(metrics.mean_squared_error(y_test, y_pred))
    ```  
* 參考資料
  * [ML Lecture 22: Ensemble](https://www.youtube.com/watch?v=tH9FH1DH5n0)
  * [Kaggle Winning Solution Xgboost Algorithm](https://www.youtube.com/watch?v=ufHo8vbk6g4)
  * [How to explain gradient boosting](https://explained.ai/gradient-boosting/index.html)
  * [決策樹的案例(TSMC)](https://sci-hub.se/10.1109/issm.2006.4493094)

Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>

### D046-程式實作-梯度提升機
* 使用 Sklearn 中的梯度提升機
  * 如同隨機森林，從 sklearn.ensemble 這裏 import 進來，代表梯度提升機同樣是個集成模型
  * 透過多棵決策樹依序生成來得到結果，緩解原本決策樹容易過擬和的問題，實務上的結果通常也會比決策樹來得好
  * 語法
    ```
    from sklearn.ensemble import GradientBoostingClassifier
    from sklearn.ensemble import GradientBoostingRegressor
    clf = GradientBoostingClassifier()
    ```
  * 超參數
    * 與決策樹相同(max_depth, min_samples_split)
    * 可決定要生成數的數量，越多越不容易過擬和，但是運算時間會變長
      ```
      from sklearn.ensemble import GradientBoostingClassifier
      clf = GradientBoostingClassifier(
                loss="deviance", #Loss 的選擇，若改為 exponential 則會變成 Adaboosting 演算法，概念相同但實作稍微不同
                learning_rate=0.1, #每棵樹對最終結果的影響，應與 n_estimators 成反比
                n_estimators=100 #決策樹的數量
            )
      ```
    * [如何使用 Python 調整梯度提升機的超參數](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)
  * 常見問題：隨機森林與梯度提升機的特徵重要性結果不相同？
    * 決策樹計算特徵重要性的概念是，觀察某一特徵被用來切分的次數而定
    * 假設有兩個一模一樣的特徵，在隨機森林中每棵樹皆為獨立，因此兩個特徵皆有可能被使用，最終統計出來的次數會被均分
    * 在梯度提升機中，每棵樹皆有關連，因此模型僅會使用其中一個特徵，另一個相同特徵的重要性則會消失
* 範例與作業
  * [範例D046](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_046_HW_gradient_boosting_machine/Day_046_gradient_boosting_machine.ipynb)
    * 資料集：Iris
  * [作業D046](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_046_Ans.ipynb)
    * 資料集：手寫辨識資料集

Back to <a href="#機器學習基礎模型建立">機器學習基礎模型建立</a>
<br>
<br>


## 機器學習調整參數
### D047-超參數調整及優化
* 參數：訓練模型中，模型根據數據學習出來的變量
* 超參數：會影響模型訓練的結果，建議先使用預設值，再慢慢人工進行調整
  * 類型
    * LASSO，Ridge：α 的大小 
    * 決策樹：樹的深度、節點最小樣本數
    * 隨機森林：樹的數量
  * 重要
    * 超參數會影響模型訓練的結果，建議先使用預設值，再慢慢進行調整
    * 超參數會影響結果，但提升的效果有限，資料清理與特徵工程才能最有效的提升準確率，調整參數只是一個加分的工具
  * 調整方法
    * 網格搜尋(窮舉法) [Grid Search](https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881)
      * 直接指定超參數的組合範圍，每一組參數都訓練完成，再根據驗證集 (validation) 的結果選擇最佳參數，即為暴力破解法
    * 隨機搜尋 Random Search
      * 指定超參數的範圍，用均勻分布進行參數抽樣，用抽到的參數進行訓練，再根據驗證集的結果選擇最佳參數
    * 貝葉斯優化演算法
  * 調整步驟：若持續使用同一份驗證集 (validation) 來調參，可能讓模型的參數過於擬合該驗證集，正確的步驟是使用 Cross-validation 確保模型泛化性
    * 先將資料切分為訓練/測試集，測試集保留不使用
    * 將剛切分好的訓練集，再使用 Cross-validation 切分 K 份訓練/驗證集
    * 用 grid/random search 的超參數進行訓練與評估
    * 選出最佳的參數，用該參數與全部訓練集建模
    * 最後使用測試集評估結果
  * 影響性
    * 超參數調整通常都是機器學習專案的最後步驟，這對於最終的結果影響不會太多，多半是近一步提升 3-5 % 的準確率
    * 好的特徵工程與資料清理是能夠一口氣提升 10-20 ％ 的準確率
* 機器學習模型訓練步驟
  * Loading and pre-processing dataset of interest
  * Hyperparameter optimization using cross-validation
  * Fitting tuned algorithm to the training data
  * Applying learned model to test data
* 參考資料
  * [劍橋實驗室教你如何調參數](https://cambridgecoding.wordpress.com/2016/04/03/scanning-hyperspace-how-to-tune-machine-learning-models/)
  * [教你使用 Python 調整隨機森林參數](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)
* 範例與作業
  * [範例D047](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_047_HW_hyper_parameter_tunning/Day_047_hyper_parameter_tunning.ipynb)
    * 資料集：boston
    * 模型：regressor
    * 方法：GridSearch
  * [作業D047](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_047_Ans.ipynb)
    * 資料集：breast_cancer、Iris
    * 模型：GradientBoostingClassifier
    * 方法：GridSearchCV

Back to <a href="#機器學習調整參數">機器學習調整參數</a>
<br>
<br>

### D048-Kaggle競賽平台介紹
* Kaggle 競賽平台(有實際操作說明)
  * 為全球資料科學競賽的網站，許多資料科學的競賽均會在此舉辦
  * 名列前茅的參賽者都會分享自己的作法與程式碼供參考，非常具有參考價值
  * 排行榜
    * 主辦單位通常會把測試資料分為 public set 與 private set，參賽者上傳預測結果可以看到 public set 的成績，但比賽最終會使用 private set 的成績作為排名
  * Kernels 與 Discussion
    * Kernels 可以看見許多高手們分享的程式碼與結果，多半會以 jupyter notebook 呈現
    * Discussion 可以看到高手們互相討論可能的做法，或是資料中是否存在某些問題
* 範例與作業
  * [作業D048](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_048_HW.ipynb)
    * 範例練習

Back to <a href="#機器學習調整參數">機器學習調整參數</a>
<br>
<br>

### D049D050-集成方法-混和泛化AND堆疊泛化-BlendingANDStacking
* 集成 Aggregation
  * 使用不同方式結合多個/多種不同分類器，作為綜合預測的做法統稱
    * select the most trust-worthy friend from their usual performance <br>
      → 從中選出最小的 Validation error <br>
      → $G(x) = g_{t_i}(x), t_i = argmin_{t \in (1,2,...T)} E_{val}(g_t^-)$
    * mix the predictions from all your friends uniformly <br>
      → 進行投票表決，每人票數一樣 <br>
      → $G(x) = sign(\sum\limits_{t=1}^T 1 * g_t(x))$
    * mix the predictions from all your friends non-uniformly <br>
      → 進行投票表決，每人票數不一樣，前 2 種情形為此的特例 <br>
      → $G(x) = sign(\sum\limits_{t=1}^T \alpha_t * g_t(x)), \alpha_t \geq 0$      
    * combine the predictions conditionally <br>
      → 依據特性選擇不同投票方式，前 3 種情形為此的特例 <br>
      → $G(x) = sign(\sum\limits_{t=1}^T q_t * g_t(x)), q_t \geq 0$      
  * 將模型截長補短，也就是機器學習的和議制/多數決
  * 類型
    * 資料集成
      * 裝袋法(Bagging)
        * 說明：將資料放入袋中抽取，每回合結束後全部放回袋中重抽，再搭配弱分類器取平均/多數決結果，如<a href="#D043-TreeBasedModel-隨機森林RandomForest">D043 隨機森林(Random Forest)</a>，這種組織的方法在統計上稱為 bootstrap
        * 優點：當訓練資料中有噪聲資料(不好的資料)，透過 bagging 抽樣就有機會讓這些噪聲資料不被訓練到，因此可以降低模型的不確定性
      * 提升法(Boosting)
        * 說明
          * 將許多弱分類器(weak classifier)合成變成一個強分類器，與 bagging 不同的點在於分類器之間是有關聯性的，透過將就分類器的錯誤資料權重提高、正確資料權重降低，接著重新訓練新的分類器，這樣新的分類器就可以學到錯誤分類的資料，以提升分類結果的準確性
          * 改進 Boosting 的演算法，如 AdaBoost、Adaptive Boosting
            * AdaBoost
              * 將前幾個分類器線性組合的錯誤樣本的權重提高，這樣可以讓每次訓練新分類器的時候都聚焦在容易分類錯誤的訓練數據上
              * 每個弱分類不再採用平均投票機制，而是採用加權投票機制
              * 準確率較大的弱分類器會擁有較大的權重，準確率較小的弱分類器權重就會比較低
            * [LightGBM](https://matters.town/@CHWang/233779-machine-learning-%E7%B5%A6%E8%87%AA%E5%B7%B1%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E7%AD%86%E8%A8%98-kaggle%E7%AB%B6%E8%B3%BD%E5%BF%85%E5%82%99-light-gbm-light-gradient-boosting-machine-%E5%AF%A6%E4%BD%9C%E6%BC%94%E7%B7%B4-%E7%AD%86%E8%A8%98-%E4%BA%8C-bafyreigqihgvixmnnikwmxaxgxzihuzbnj7bwsnus77izo2j7x34zx24nq)
        * 若依照估計誤差的殘差項調整新目標值，就是<a href="#D045-TreeBasedModel-梯度提升機GradientBoostingMachine">D045 梯度提升機(Gradient Boosting Machine)</a>，只是梯度提升機還加上用梯度來選擇決策樹分支
    * 模型與特徵集成
      * 混合泛化(Blending)
        * 將不同模型的預測值加權合成，權重和為 1，如果取預測的平均 or 一人一票多數決(每個模型權重相同)，則又稱為投票泛化(Voting)
        * 容易使用
          * 只要有[預測值(Submit 檔案)](https://www.kaggle.com/code/tunguz/superblend/script)就可以使用，許多跨國隊伍就是靠這個方式合作
          * 也因為只要用預測值就能計算，在競賽中可以快速合成多種比例的答案，妥善消耗掉每一天剩餘的 Submit 次數
        * 效果顯著
          * Kaggle 競賽截止日前的 Kernel，有許多只是對其他人的輸出結果做 Blending，但是因為分數較高，因此也有許多人樂於推薦與發表
          * 在2015年前的大賽中，Blending 仍是主流，如[林軒田老師曾在機器學習技法 Lecture 7: Blending and Bagging 的課程中提及](https://www.youtube.com/watch?v=mjUKsp0MvMI)：有競賽的送出結果，是上百個模型的 Blending
        * 個別單模效果都很好(有調參)並且模型差異大，單模要好尤其重要，如果單模效果差異太大，Blending 的效果提升就相當有限
        * 混合泛化提升預測力的原因是基於模型差異度大，在預測細節上能互補，因此預測模型只要各自調參優化過且原理不同，通常都能使用混合泛化集成
      * 堆疊泛化(Stacking)
        * 2012 年由 [David H. Wolpert 發布](http://www.machine-learning.martinsewell.com/ensembles/stacking/Wolpert1992.pdf)，2014 年底被廣泛應用於 Kaggle 競賽開始，後來出現加速混合與計算速度的 StackNet
        * 相對於 Blending 的改良
          * 不只將預測結果混合，而是使用預測結果當新特徵
          * 更進一步地運用資料輔助集成，使得 Stacking 複雜許多
        * Stacking 的設計
          * Stacking 把模型當作下一階的特徵編碼器來使用，但待編碼資料與用來訓練編碼器的資料不可重複，但待編碼資料太少，下一層的資料筆數就會太少；訓練編碼器的資料太少，則編碼器的強度就會不夠
          * 利用 K-Fold 概念將資料拆成 K 份，每 1/K 的資料要編碼時，使用其他的 K-1 組資料訓練模型/編碼器 → 問題：計算時間隨 K 變大而變長，但 K 可以調整，相對深度學習所需的時間來說，這樣的時間長度還可接受
          * 自我遞迴的 Sracking：在原本特徵上用模型造出新特徵
            * 新舊特徵能一起用，再用模型預測，須謹慎切分 Fold 及新增次數
            * 新的特徵可以再搭配模型創特徵，第三層第四層...一直下去，但精準度可能會下降
            * 既然同層新特徵會 Overfitting，層數加深會增加泛化，同時使用能把缺點互相抵銷，但程式複雜且運算時間長，故難以實踐且較少人使用
      * 好處：利用一系列性能良好的模型在分類或迴歸任務上的能力，並做出比集成中的任何一個模型更好的預測
      * 參考資料
        * [StackingCVClassifier](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/)
    * 資料集成 VS. 模型與特徵集成差異
      * 資料集成
        * 如 Bagging/Boosting，詳如：<a href="#D045-TreeBasedModel-梯度提升機GradientBoostingMachine">D045 梯度提升機(Gradient Boosting Machine)</a>
        * 使用「不同訓練資料」且同一模型，多次估計的結果合成最終預測
      * 模型與特徵集成
        * 如 Voting/Blending/stacking，使用同一資料且「不同模型」，合成出不同預測結果
* 範例與作業
  * [範例D049](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_049_HW_blending/Day_049_Blending.ipynb)
    * 混和泛化
    * 資料集：房價預測
    * 模型：線性迴歸/梯度提升機/隨機森林，參數使用 Random Search 尋找
    * 重點
      * 觀察混合泛化的準確度
      * 自行調整權重
  * [作業D049](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_049_Blending_Ans.ipynb)
    * 分類預測要用權重混合時，需以機率的形式混合
    * 資料集：鐵達尼號
    * 模型：羅吉斯迴歸/梯度提升機/隨機森林，參數使用 Random Search 尋找
    * 重點：
      * 觀察混和泛化的準確度
      * 自行調整權重
  * [範例D050](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_050_HW_stacking/Day_050_Stacking.ipynb)
    * 堆疊泛化
    * 資料集：房價預測
    * 模型：線性迴歸/梯度提升機/隨機森林
    * 重點：觀察堆疊泛化的準確度是否比單一模型準確度為高
  * [作業D050](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_050_Stacking_Ans.ipynb)
    * 資料集：鐵達尼號
    * 模型：羅吉斯迴歸/梯度提升機/隨機森林

Back to <a href="#機器學習調整參數">機器學習調整參數</a>
<br>
<br>


## Kaggle期中考 
### D051-D053-Kaggle期中考
* 範例與作業(待練習sample)
  * [作業D051](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_051_053_Midterm_Enron_Fraud.ipynb)

Back to <a href="#Kaggle期中考">Kaggle期中考</a>
<br>
<br>


## 非監督式的機器學習
### D054-非監督式機器學習
* 非監督式學習
  * 簡介
    * 非監督式學習演算法只基於輸入資料找出模式，當無法確定尋找內容或無標記 (y) 資料時，通常會使用這個演算法，以幫助了解資料模式、資料特性，或作為提升監督式學習效能的預處理步驟
    * 非監督式學習由於沒有監督式學習算法所參照的 ground truth，不易判斷模型是否真的學到隱藏資料中的模式，需要透過像是「輪廓分析」等方法評估分群的品質
  * 應用情境
    * 分群(Clustering)
      * 又稱聚類分析、集群分析，將擁有各自相似屬性的樣本聚到各自一群，使其成為不同的群體。分群結果會給予每個樣本點一個標籤(label)
      * 常見應用案例，如：用戶/文章/影片/語音貼標，目的為行銷自動化、數位媒體投放等
      * 重要概念
        * 通常分群後計算各群特徵的統計指標(如群內樣本、平均指標)，方便我們比較不同群間的差異，也可當作分群解釋方法
        * 建議群數不超過資料集的特徵維度數，如此一來各群的特徵差異(前一步所算出的統計指標)會更容易顯現
      * 方法
        * 聚合式階層分群法(Agglomerative Hierarchical Clustering)
          * 將每個樣本視為一個群聚，從樹狀結構底部不斷融合相近的樣本；假如生成的群數多於預期的群數，則反覆重複聚合最近距離的兩群的動作，直到群數降到條件範圍內
          * 完成的分群會以樹狀結構呈現，稱作 dendrogram
          * 在階層式分群中常見的距離定義方式
            * Single Linkage：群與群的距離為不同群中最接近兩點間的距離，容易產生大者恆大的分群結果
            * Average Linkage：群與群的距離為不同群中各點與各點間距離總和平均，相比 Single Linkage 則較容易產生齊頭並進、各群規模相近的分群結果
            * Complete Linkage：群與群的距離為不同群中最遠兩點間的距離
            * Ward's Method：將兩群合併後，各點到合併後的群中心的距離平方和<br>
            ![Cluster Dendrogram](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F_Cluster%20Dendrogram.png)    
        * 分割式分群(Partitional Clustering)
          * K-Means
            * 把 n 個點切分到 k 個群中，不斷重複調整群心、重新分群，最後達到每個點都隸屬於與其群心最近的聚類(群)中
    * 關聯規則(Association Rule)
      * 從大量數據中發現變數間隱藏關係的方法
      * 常見應用案例，如購物籃分析，可作為銷售團隊線上或線下商品組合的決策參考<br>
        ![關聯規則](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F-%E9%97%9C%E8%81%AF%E5%88%86%E6%9E%90.png) 
      * 方法
        <table border="1" width="25%">
          <tr>
            <th width="5%"> 演算法 </a>
            <th width="5%"> 主要特色 </a>
            <th width="5%"> 缺點 </a>
            <th width="5%"> 搜尋方式 </a>
            <th width="5%"> 資料配置方式 </a>
          </tr>
          <tr>
            <td> Apriori </td>
            <td> ● 反覆產生候選項目集，找出所有高頻項目集，進而推倒規則 <br>
                 ● 逐層搜索的迭代方法，先找出出現頻次為 1 的項目集合，高頻次存為 L1；再用 L1 找出現頻次為 2 的項目集合 L2，L2 再用來找 L3，依此類推，直到不能找到更多頻次共同出現的項目集合 </td>
            <td> 需反覆搜尋資料庫，花費I/O時間 </td>
            <td> 廣度優先 </td>
            <td> 水平資料配置 </td>
          </tr>
          <tr>
            <td> Partition Apriori </td>
            <td> 將資料庫分區段，找出各區段之高頻項目集加以集合，再次搜尋資料庫找出真正高頻項目集 </td>
            <td> 在各區段中會產生較多的非相關項目集 </td>
            <td> 廣度優先 </td>
            <td> 垂直資料配置 </td>
          </tr>
          <tr>
            <td> DHP </td>
            <td> 利用雜湊表(hash table)刪減部必要的候選項目集 </td>
            <td> 一開始需花時間建立雜湊表 </td>
            <td> 廣度優先 </td>
            <td> 水平資料配置 </td>
          </tr>
          <tr>
            <td> MSApriori </td>
            <td> 在資料項目出現頻率不一致的情況下，挖掘低頻率但重要事件之關聯規則 </td>
            <td> 需多加探討多重最小支持度與演算法中參數的主觀訂定 </td>
            <td> 廣度優先 </td>
            <td> 水平資料配置 </td>
          </tr>
          <tr>
            <td> FP-Growth </td>
            <td> 頻率樣式成長為演算法的演繹基礎，可改善Apriori無法有效處理大量資料缺點 </td>
            <td> 需較多的額外處理時間及儲存空間來存放FP樹 </td>
            <td> 深度優先 </td>
            <td> 水平資料配置 </td>
          </tr>
        </table>         
    * 異常檢測(Anomaly Detection)
      * 透過樣本特徵的群聚，將相對異常的模式、樣本或事件辨識出來
      * 常見應用案例，如交易詐欺、結構缺陷檢測、醫療問題、文字錯誤辨識、入侵檢測<br>
        ![異常偵測](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F-%E7%95%B0%E5%B8%B8%E7%9B%A3%E6%B8%AC.png)  
      * 方法
        * LOF (Local Outlier Factor)
          * 如果樣本點 p 的 LOF 得分接近 1，表示 p 的局部密度與相鄰點差不多
          * 如果 p 的 LOF 得分小於 1，表示 p 落在一個相對密集的區域，最不像是異常點
          * 如果 p 的 LOF 得分遠大於 1，表示 p 跟其他樣本點相對疏遠，很有可能是異常點
    * 降維(Dimension Reduction)
      * 當特徵數太多難於理解及呈現的情況下，藉由抽象化的技術轉換原資料的表示方式、降低資料維度、組合成新的特徵，同時不失去原有的資訊
      * 方法
        * 主成分分析(Principle Component Analysis, PCA)
          * 對一系列可能相關的特徵進行線性轉換，將他們投影成另一系列「線性不相關」的特徵值，這些不相關特徵稱為「主成分」<br>
      ![降維](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F-%E9%99%8D%E7%B6%AD.png)
    * 結構化資料分析
      ![結構化資料](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F-%E7%B5%90%E6%A7%8B%E5%8C%96%E8%B3%87%E6%96%99.png)
    * 非結構化資料分析
      * 非結構化資料如文字、影像等，可藉由非監督式學習技術，幫助呈現、描述並在一系列資料中萃取關鍵字
      * 常見案例，如主題模型(topic model)
      * 方法
        * 隱含狄利克雷分布(Latent Dirichlet Allocation, LDA)
          * 事先定義好有限的主題，並透過觀察文件與朋友用詞來計算出主題之間的關聯，以及各個文件的主題分佈，如此一來，只要文件夠多，就可以有效的快速理解不同文件的主題分佈
* 參考資料
  * [Unsupervised learning](https://scikit-learn.org/stable/unsupervised_learning.html)
  * [階層式分群法 Clustering: Hierarchical Clustering](https://www.mropengate.com/2015/06/ai-ch17-6-clustering-hierarchical.html)
  * [Apriori 演算法詳解及 Python實作](https://matters.town/@CHWang/75663-machine-learning-%E9%97%9C%E8%81%AF%E5%88%86%E6%9E%90-apriori%E6%BC%94%E7%AE%97%E6%B3%95-%E8%A9%B3%E7%B4%B0%E8%A7%A3%E8%AA%AA%E5%95%A4%E9%85%92%E8%88%87%E5%B0%BF%E5%B8%83%E7%9A%84%E8%83%8C%E5%BE%8C%E5%8E%9F%E7%90%86-python%E5%AF%A6%E4%BD%9C-scikit-learn%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%95%99%E5%AD%B8-bafyreiblhbu7qc3go3aatvabovx2yykd5t4azssw56c5ewcezshjkr74m4)
  * [基於密度的經典異常偵測算法：LOF 算法詳解](https://zhuanlan.zhihu.com/p/28178476)
  * [Unsupervised learning：PCA](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/PCA.mp4)
  * [主題模型與 LDA 概念理解](https://tengyuanchang.medium.com/%E7%9B%B4%E8%A7%80%E7%90%86%E8%A7%A3-lda-latent-dirichlet-allocation-%E8%88%87%E6%96%87%E4%BB%B6%E4%B8%BB%E9%A1%8C%E6%A8%A1%E5%9E%8B-ab4f26c27184)
  * [Unsupervised Learning, Recommenders, Reinforcement Learning](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning)
* 範例與作業
  * [作業D054](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_054_Clustering_Ans.ipynb)
    * 無程式撰寫

Back to <a href="#非監督式的機器學習">非監督式的機器學習</a>
<br>
<br>

### D055-非監督式-分群-K-Means
* 監督式與非監督式學習
  * 監督式學習目標在於找出決策邊界(decision boundary)
  * 非監督式學習-分群目標在於找出資料結構
    * 分群算法(或稱聚類算法)用於把族群或資料點分隔成一系列的組合，使得相同 cluster 中的資料點比其他的組更相似
    * 分群算法依照他們區分樣本點的方式不同，還可分為幾個類別：
      * 階層式分群(Hierarchical)
      * 分割式分群(Partitional)：如 K-means、K-Medoids
      * 基於密度的分群：如 DBSCAN
      * 基於機率的分群：如 高斯混合模型(Gaussian Mixture Model, GMM)<br>
  ![類型_監督式與非監督式](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%A1%9E%E5%9E%8B_%E7%9B%A3%E7%9D%A3%E5%BC%8F%E8%88%87%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F.png)
* K-Means
  * 簡介
    * 把所有資料點分成 k 個 clusters，使得相同 cluster 中的所有資料點彼此儘量相似，而不同 cluster 的資料點儘量不同
    * 距離測量(e.g. 歐氏距離)用於計算資料點的相似度和相異度，每個 cluster 有一個中心點，中心點可理解為最能代表 cluster 的點
  * 分群算法流程
    * 目標是將 training set 分成 2 群<br>
      ![part01](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/Kmean_part01.png)
    * 隨機選取 2 個點，稱爲 cluster centroid(群心、中心點)<br>
      ![part02](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/Kmean_part02.png)
    * 對每一個樣本點根據它距離哪一群的中心點較近，標記為落在該群之一的點(*cluster assignment)<br>
      ![part03](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/Kmean_part03.png)
    * 然後把群心移到同一群樣本點的中心點(*update centroid)<br>
      ![part04](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/Kmean_part04.png)
    * 反覆進行上述的 *cluster assignment 及 *update centroid，直到樣本點不再被 assign 到與上一次不同的群，便是算法成功收斂完畢
  * 最佳化目標：分群算法執行的目的是希望產出品質最好的分群結果，使總體群內平方誤差最小
    $$\sum\limits_{i=0}^n min_{u_j \in C}(||x_i-u_j||^2)$$
  * 要點
    * 初始值設定 Random initialization：一開始隨機選擇的群中心不同將會得到不同的分群結果！可能導致 local optima(區域最佳解)，而非 global optima(全域最佳解)
    * 因爲沒有預先標記，群數多少才是最佳解，沒有標準答案，需視資料集情況而定。其中一個方式是使用輪廓係數衡量群內距與群間距，係數越高表示群分得越開、群內越聚集
    * 分群的目的是為了更好的解讀資料特性，因此也可將群的特徵量化來對各群觀察，群數的設定建議小於訓練特徵維度數，各群的差異會更加顯著
    * K-means 分群算法需要事先定義群數，而群數多少才是最佳解，可透過衡量不同群數時的輪廓係數，或以各群樣本特徵指標來觀察是否產生具代表意義的群
* 參考資料
  * K應該為多少？[StatQuest: K-means clustering](https://www.youtube.com/watch?v=4b5d3muPQmA)
  * [Kaggle kernel 示範用 K-means Clustering 做消費者區隔](https://www.kaggle.com/code/kushal1996/customer-segmentation-k-means-analysis/notebook)
  * [ravel()、flatten()、squeeze()的用法與區別](https://blog.csdn.net/tymatlab/article/details/79009618)
* 範例與作業
  * [範例D055](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_055_HW_kmean/Day_055_kmean_sample.ipynb)
    * 資料集：toy
  * [作業D055](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_055_kmean_Ans.ipynb)
    * 資料集：Iris
  * 其他參考資料：[非監督式學習範例](https://github.com/sueshow/Python_Machine-Learning-Base/blob/main/%E9%9D%9E%E7%9B%A3%E7%9D%A3_%E5%AE%8C%E6%95%B4%E7%89%88_Iris.ipynb)

Back to <a href="#非監督式的機器學習">非監督式的機器學習</a>
<br>
<br>

### D056-非監督式-分群-K-Means分群評估-使用輪廓分析
* 分群模型的評估
  * 有目標值的分群：資料具有目標值，只是先忽略目標值做非監督學習，則只要微調後，就可以使用原本監督的測量函數評估準確性
  * 無目標值的分群：但通常沒有目標值/目標值非常少才會用非監督模型，這種情況下，只能使用資料本身的分布資訊，來做模型的評估
* 輪廓分析(Silhouette analysis)
  * 歷史：最早由 Peter J. Rousseeuw 於 1986 提出，同時考慮群內及相鄰群的距離，除可評估資料點分群是否得當，也可用來評估不同分群方式對於資料的分群效果
  * 設計精神
    * 同一群的資料點應該很近，不同群的資料點應該很遠，設計一種當同群資料點越近/不同群資料點越遠時越大的分數
    * 當資料點在兩群交界附近，希望分數接近 0
  * [單點輪廓值](https://ithelp.ithome.com.tw/articles/10304848)：「找出相同群凝聚度越小、不同群分離度越高」的值，也就是滿足 Cluster 一開始的目標
    * 對任意單一資料點 i，「與 i 同一群」的資料點，距離 i 的平均稱為 $a_i$
    * 「與 i 不同群」的資料點中，不同群距離 i 平均中，最近的稱為 $b_i$ (其實就是要取第二靠近 i 的那一群平均，滿足交界上分數為 0 的設計)
    * i 點的輪廓分數 $s_i$ 值總和越大，表示效果越好，適合作為 K
      $$ \frac{b_i-a_i}{max(b_i, a_i)} $$
      * 凝聚度($a_i$)是指與相同群內其他點的平均距離
      * 分離度($b_i$)是指與不同群其他點的平均距離
      * S 是指以一個點作為計算的值
      * 輪廓係數法是將所有的點都計算 S 後再取平均。平均值越大，表示效果越好，適合作為 K
    * 只要不是刻意分錯，$b_i$ 通常會大於等於 $a_i$，上述公式在此條件下可以化簡為 $1-a_i/b_i$
    * 解讀
      ![輪廓係數](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E8%81%9A%E9%A1%9E%E8%A9%95%E4%BC%B0%E7%AE%97%E6%B3%95-%E8%BC%AA%E5%BB%93%E4%BF%82%E6%95%B8.png)
      * 依照不同類別將同類別的輪廓分數排序後顯示，分數越大代表分群結果越好
      * 黃綠兩組的輪廓值大多在平均以下，且比例上接近零的點也比較多，顯示這兩組沒有分得那麼開
      * 平均值(紅色虛線)：計算分群的輪廓分數總平均，分的群數分數越多應該分數越小
    * 輪廓分析套件[sklearn.metrics.silhouette_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)
  * [整體的輪廓分析](https://scikit-learn.org.cn/view/74.html)
    * 分組觀察：將同類別的輪廓分數排序後，可發現黃綠兩組的輪廓值大多在平均以下，且比例上接近 0 的點也比較多，這些情況都表示這兩組似乎沒分得那麼開
    * 平均值觀察：計算分群的輪廓分數總平均，分的群數越多應該分數越小，說明了那些分數較不洽當
    * 輪廓分數是一種「同群資料點越近/不同群資料點越遠」時會越大的分數，除可評估資料點分群是否得當，也可用來評估分群效果
    * 要以輪廓分析觀察 K-means，除可將每個資料點分組觀察以評估資料點分群是否得當，也可用平均值觀察評估不同 K 值的分群效果
* 其他評估方法[Clustering performance evaluatio](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)
  * 手肘法(elbow method)：基於 SSE(sum of the squared errors，誤差平方和)作為指標，去計算每一個群中的每一個點，到群中心的距離，
* 範例與作業
  * [範例D056](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_056_HW_kmean/Day_056_kmean.ipynb)
    * 資料集：Iris
  * [作業D056](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_056_kmean_Ans.ipynb)
    * 資料集：隨機生成 5 群高斯分布資料

Back to <a href="#非監督式的機器學習">非監督式的機器學習</a>
<br>
<br>

### D057-非監督式-分群-階層式HierarchicalClustering
* 階層式分群
  * 概念
    * 測量樣本點間的距離後，以樹狀資料結構進行聚類。又分為 Top-down(由上而下分裂)及 Bottom-up(由下而上聚合)兩種算法
    * 小族群逐漸融合成大族群的算法
  * 使用時機
    * 資料量不大的時候
    * data 所蘊含的內容比起明確的 feature 更多由彼此直接的關係所影響
  * 優點
    * 以樹狀結構表示計算過程更易懂
    * 只以樣本間的距離就可以進行，無需指定群數、也無需樣本的實際座標位置
  * 缺點
    * 適合少量樣本，面對大量資料時效能不佳(計算量：$O(n^2)~O(n^3)$，內存佔用：$O(n^2)$)
  * 聚合式階層分群法
    * 運作方式：將每個樣本視為一個群聚(Cluster)，計算族群兩兩之間的距離，從樹狀結構底部不斷融合相近的樣本；假如生成的群數多於我們預期的群數，則反覆重複聚合最近距離的兩群的動作，直到群數降到條件範圍內，最後再根據條件進行分群(族群數量、族群距離)
    * 判讀方式
      * 樹狀結構圖 dendrogram
        * 透過一張樹狀結構圖(dendrogram)來呈現分群的過程和結果，看水平線切過多少「樹枝」，「樹枝」水平線以下代表是同一群<br>
          ![樹狀結構圖](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%9A%8E%E5%B1%A4%E5%BC%8F%E5%88%86%E7%BE%A4_%E6%A8%B9%E7%8B%80%E7%B5%90%E6%A7%8B%E5%9C%96%20dendrogram.png)
* K-means vs. 階層分群
  * K-means：預先定義群數(n of clusters)
  * 階層分群：依據定義距離來分群(bottom-up)，也可以決定群數做分群(top-down)<br>
  ![比較](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/Kmean%20vs%20%E9%9A%8E%E5%B1%A4%E5%88%86%E7%BE%A4.png)
* 階層分群演算法
  * 不指定分群的數量
  * 每筆資料為一個 cluster，計算每兩群之間的距離
    * Single-link：群聚間的距離定義為不同群聚中最接近兩點間的距離
        $$d(C_i,C_j)=min_{a\in C_i,b\in C_j} d(a,b)$$
      ![單一連結](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%9A%8E%E5%B1%A4%E5%BC%8F%E5%88%86%E7%BE%A4_single-link.png)
    * Complete-link：群聚間的距離定義為不同群聚中最遠兩點間的距離，這樣可以保證這兩個集合合併後，任何一對的距離不會大於 d
        $$d(C_i,C_j)=max_{a\in C_i,b\in C_j} d(a,b)$$ 
      ![完整連結](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%9A%8E%E5%B1%A4%E5%BC%8F%E5%88%86%E7%BE%A4_complete-link.png)
    * Average-link：群聚間的距離定義為不同群聚間各點與各點間距離總和的平均
        $$d(C_i,C_j)=\sum\limits_{a\in C_i,b\in C_j}\frac{d(a,b)}{|C_i||C_j|}$$
        where $|C_i|$ and $|C_j|$ are the sizes for $C_i$ and $C_j$, respectively
      ![平均連結](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%9A%8E%E5%B1%A4%E5%BC%8F%E5%88%86%E7%BE%A4_average-link.png)
    * 沃德法[Ward's Method](https://tomohiroliu22.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E5%AD%B8%E7%BF%92%E7%AD%86%E8%A8%98%E7%B3%BB%E5%88%97-83-%E6%B2%83%E5%BE%B7%E9%9A%8E%E5%B1%A4%E5%88%86%E7%BE%A4%E6%B3%95-ward-hierarchical-clustering-273c0e21050)：群聚間的距離定義為各點到兩群合併後的群中心的距離平方和
        $$d(C_i,C_j)=\sum\limits_{a \in C_i \cup C_j} ||a-\mu||$$
        where $\mu$ is the mean vector of $C_i \cup C_j$
      ![沃德法](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%9A%8E%E5%B1%A4%E5%BC%8F%E5%88%86%E7%BE%A4_wards%20method.png)
    * 特性
      * single linkage 會在群聚的過程中產生「大者恆大」的效果
      * complete linkage 和 average linkage 比較容易產生「齊頭並進」的效果
  * 將最近的兩群合併成一群，重覆步驟 2、3，直到所有資料合併成同一 cluster
* 參考資料
  * [階層式分群法](http://mirlab.org/jang/books/dcpr/dcHierClustering.asp?title=3-2%20Hierarchical%20Clustering%20(%B6%A5%BCh%A6%A1%A4%C0%B8s%AAk)&language=chinese)
  * [使用階層式分群用來尋找圖片中的區塊(英文)](https://scikit-learn.org/stable/auto_examples/cluster/plot_coin_ward_segmentation.html#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py)
* 範例與作業
  * [範例D057](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_057_HW_hierarchical_clustering/Day_057_hierarchical_clustering_sample.ipynb)
    * 資料集：toy
    * 重點：設定模型估計參數集資料建模
  * [作業D057](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_057_hierarchical_clustering_Ans.ipynb)
    * 資料集：Iris
    * 重點：設定模型估計參數集資料建模
  * 其他參考資料：[非監督式學習範例](https://github.com/sueshow/Python_Machine-Learning-Base/blob/main/%E9%9D%9E%E7%9B%A3%E7%9D%A3_%E5%AE%8C%E6%95%B4%E7%89%88_Iris.ipynb)

Back to <a href="#非監督式的機器學習">非監督式的機器學習</a>
<br>
<br>

### D058-非監督式-分群-HierarchicalClustering觀察-使用2D樣版資料集
* 2D 樣版資料集(2d toy dataset)
  * 2D 樣版資料集著重於圖形的人機差異：挑選人眼容易分群，但非監督模型常常有困難的圖案樣板來展示
    ![2d toy dataset](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%9A%8E%E5%B1%A4%E5%BC%8F%E5%88%86%E7%BE%A4_2d%20toy%20dataset.png)
  * 用途：用來讓人眼評估非監督模型的好壞，因為非監督模型的任務包含分群(對應於監督的分類)與流形還原(對應監督的迴歸)，所以 2D 樣板資料集在設計上也包含這兩種類型的資料集
* sklearn 的資料集
  * 主要分為
    * 載入式(Loaders)：固定資料 
    * 生成式(Samples generator)：先有既定模式，在模式下有限度的隨機生成每次使用的資料集
  * 2D 樣版資料集
    * 屬於生成式資料集(Samples generator)，使用不同分布，用以顯示各種非監督模型的優缺點
      ![生成式](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%9A%8E%E5%B1%A4%E5%BC%8F%E5%88%86%E7%BE%A4_2d%20toy%20dataset_%E7%94%9F%E6%88%90%E5%BC%8F.png)
    * 參考資料
      * [使用 2D 樣版資料集](https://lemon-dolomite-062.notion.site/Day-58-2D-bc40d32b0c65479b8c93011d9609ec21)
* 範例與作業
  * [範例D058](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_058_HW_hierarchical_clustering/Day_058_hierarchical_clustering.ipynb)
  * [作業D058](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_058_hierarchical_clustering_Ans.ipynb)
  * 其他參考資料：[非監督式學習範例](https://github.com/sueshow/Python_Machine-Learning-Base/blob/main/%E9%9D%9E%E7%9B%A3%E7%9D%A3_%E5%AE%8C%E6%95%B4%E7%89%88_Iris.ipynb)
 
Back to <a href="#非監督式的機器學習">非監督式的機器學習</a>
<br>
<br>

### D059-降維方法DimensionReduction-主成份分析PCA
* 為何需要降維(Dimensionlit Reduction)
  * 數據為度過大 → 提高模型的複雜度，當資料樣本不足時，模型訓練的結果較差
  * 壓縮資料
    * 有助於使用較少的 RAM 或 disk space，也助於加速 learning algorithms
    * 影像壓縮：壓縮後圖片可能變得模糊，但依然保有明顯的輪廓和特徵<br>
    ![影像壓縮](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%99%8D%E7%B6%AD_%E5%BD%B1%E5%83%8F%E5%A3%93%E7%B8%AE.png)
  * 特徵組合及抽象化：壓縮後可組合出新的、抽象化的特徵，減少冗餘、無用的資訊<br>
    ![特徵組合及抽象化](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%99%8D%E7%B6%AD_%E7%89%B9%E5%BE%B5%E7%B5%84%E5%90%88%E5%8F%8A%E6%8A%BD%E8%B1%A1%E5%8C%96.png)
  * 資料視覺化
    * 特徵太多時，很難 visualize data，不容易觀察資料
    * 將資料維度(特徵)降至 2 到 3 個，能夠用一般的 2D 或 3D 圖表呈現資料
* 降維度的目的
  * 減少特徵的個數、去除特徵間的共線性問題
  * 降低模型的計算量，減少模型執行時間
  * 減少雜訊對模型的影響
  * 確保特徵間互相獨立
* 常見降維方法
  * 主成分分析 PCA (Principal components analysis)<br>
    ![PCA](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%99%8D%E7%B6%AD_PCA_01.png)
    * 利用線性轉換，將資料從高雄(k維)映射到低維(m維)空間，並提取(m維)資料的主要特徵，保有原始資料的重要資訊
    * PCA 不是從原始資料中捨棄不重要的特徵來降維，而是由這些特徵與其向量(eigenvector)的線性組合，降維至二維平面上，所產生的新特徵來代表原始資料
    * 特性
      * 原始資料特徵之間表現出較強的相關性，若相關性較弱，降維效果較差
      * 新 features 為舊 features 的線性組合
    * 流程：PCA 保證樣本投影後方差最大
      * 標準化 d 維原資料集，使各特徵具有相同的重要性，若無標準化，則會容易導致 PCA 偏向數值較大的特徵
      * 建立共變異數矩陣(covariance matrix)
        * 協方差：衡量 2 個變數的相關程度
          * Cov(X, Y)>0，表 X 與 Y 線性正相關，X 增加 → Y 增加
          * Cov(X, Y)<0，表 X 與 Y 線性負相關，X 增加 → Y 減少
          * Cov(X, Y)=0，表 X 與 Y 線性不相關
        * 皮爾森相關係數(Pearson correlation coefficient)：探討各變數之間的線性關係，值介於 -1~1 之間
          * 公式：
            $$Corr(x,Y)=\frac{Cov(X,Y)}{\sqrt[2]{Var(X) Var(Y)}}=\frac{Cov(X,Y)}{\sigma_x \sigma_y}$$
          * 性質：
            * $|corr(X,Y)| \leq 1$
            * $corr(X,Y)=1 \leftrightarrow X、Y$ 線性相關
            * corr(X,Y)=0 表示X、Y不存在線性相關
            * $corr(X,Y)=corr(Y,X)$
          * 相關性(correlation)與獨立性(independence)
            * 若 X 和 Y 的線性不相關，則 |corr(X,Y)| = 0
            * 若 X 和 Y 的彼此獨立，則 |corr(X,Y)| = 0 且 X 和 Y 不存在任何線性或非線性關係
            * 獨立係指兩個變量彼此之間不相互影響，故獨立一定不相關，但不相關不一定相互獨立
      * 將共變異數矩陣(covariance matrix)分解為特徵向量(eigenvector)與特徵值(eigenvalues)
        * [特徵值(EVD)分解](https://blog.csdn.net/Feeryman_Lee/article/details/104339696)
        * [奇異值(SVD)分解](https://blog.csdn.net/Feeryman_Lee/article/details/104339696)
        * 其他參考資料
          * [特徵值分解](https://blog.csdn.net/Sunshine_in_Moon/article/details/51513880)
          * [分解原理與計算](https://blog.csdn.net/weixin_42462804/article/details/83905320?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase)<br>
        ![分解](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%99%8D%E7%B6%AD_PCA_%E7%89%B9%E5%BE%B5%E5%80%BC%20(EVD)%20%E8%88%87%E5%A5%87%E7%95%B0%E5%80%BC%20(SVD).png)
      * 選取 k 個最大特徵值(eigenvalues)相對應 k 個的特徵向量(eigenvector)，其中 k 即為新特徵子空間的維度
      * 使用排序最上面的 k 個的特徵向量(eigenvector)，建立投影矩陣(project matrix) W
      * 使用投影矩陣(project matrix) W 轉換原本 d 維的原數據至新的 k 維特徵子空間<br>
        ![建立投影](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%99%8D%E7%B6%AD_PCA_%E5%BB%BA%E7%AB%8B%E6%8A%95%E5%BD%B1.png)
    * 優點
      * 組合出來後新的 feature 可用來做 supervised learning 預測模型
      * 以判斷人臉為例，最重要的特徵都可以捨棄，將不必要的資訊捨棄除可加速 learning，也可避免 overfitting<br>
      ![PCA](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%99%8D%E7%B6%AD_PCA_03.png)
    * 要點
      * 不建議在早期就做，否則可能會丟失重要的 features 而 underfitting
      * 可在 optimization 階段時，考慮 PCA，並觀察運用後對準確度的影響
    * 參考資料
      * [主成份分析](https://www.youtube.com/watch?v=5MbjJOnUZDc&t=2s)
      * [PCA降維概述](https://www.youtube.com/watch?v=IvgYo1qeGZc)
      * [PCA要優化的目標](https://www.youtube.com/watch?v=HU8WuvMdTVE)
      * [PCA求解](https://www.youtube.com/watch?v=lsJmWBZvzf0)
      * [Visual Explanation of Principal Component Analysis, Covariance, SVD](https://www.youtube.com/watch?v=5HNr_j6LmPc)
      * [Principal Component Analysis (PCA)](https://www.youtube.com/watch?v=g-Hb26agBFg)
      * [Eigenvectors and eigenvalues | Essence of linear algebra, chapter 14 (特徵轉換)](https://www.youtube.com/watch?v=PFDu9oVAE-g)
  * t-SNE
* 範例與作業
  * [範例D059](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_059_HW_PCA/Day_059_PCA_sample.ipynb)
    * 資料集：Iris
  * [作業D059](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_059_PCA_Ans.ipynb)
    * 資料集：digits

Back to <a href="#非監督式的機器學習">非監督式的機器學習</a>
<br>
<br>

### D060-程式實作-PCA-使用手寫辨識資料集
* 程式實作
  * 資料集：手寫辨識資料集(MNIST)
    * 簡介：手寫辨識資料集 (MNIST, Modified National Institute of Standards and Technology databas) 原始來源的NIST，應該是來自於美國人口普查局的員工以及學生手寫所得，其中的 Modified 指的是資料集為了適合機器學習做了一些調整 : 將原始圖案一律轉成黑底白字，做了對應的抗鋸齒的調整，最後存成 28x28 的灰階圖案，成為了目前最常聽到的基礎影像資料集
    * sklearn 中的手寫辨識資料集：與完整的MNIST不同，sklearn為了方便非深度學習的計算，再一次將圖片的大小壓縮到 8*8 的大小，雖然仍是灰階，但就形狀上已經有點難以用肉眼辨識，但壓縮到如此大小時，每張手寫圖就可以當作 64(8*8=64) 個特徵的一筆資料，搭配一般的機器學習模型做出學習與預測
  * 挑選原因
    * 高維度.高複雜性 → 人可理解的資料集
      * 由於 PCA 的強大，如果資料有意義的維度太低，則前幾個主成分就可以將資料解釋完畢
      * 使用一般圖形資料，維度又會太高，因此我們使用 sklearn 版本的 MNIST 檢驗 PCA，以兼顧內容的複雜性與可理解性
      * 由範例的折線圖可以看出來：前幾個維度就能解釋 75% 以上的變數
* 範例與作業
  * [範例D060](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_060_HW_PCA/Day_060_PCA.ipynb)
    * 資料集：手寫辨識資料集
    * 方法：羅吉斯迴歸 + PCA
    * penalty component：l2
  * [作業D060](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_060_PCA_Ans.ipynb)
    * 資料集：手寫辨識資料集
    * 方法：羅吉斯迴歸 + PCA
    * penalty component：l1
    * 在取不同 component 時，使用羅吉斯迴歸 + PCA 觀察解釋度與分類準確率的變化

Back to <a href="#非監督式的機器學習">非監督式的機器學習</a>
<br>
<br>

### D061-降維方法DimensionReduction-T-SNE
* PCA 限制 
  * 求共變異數矩陣進行奇異值分解，因此會被資料的差異性影響，無法很好的表現相似性及分佈
  * PCA 是一種線性降維方式，因此若特徵間是非線性關係，會有 underfitting 的問題
* t-SNE
  * 簡介：主要是將高維的資料用 gaussian distribution 的機率密度函數近似，而低維資料的部分用 t 分佈來近似，在用 KL divergence 計算相似度，再以梯度下降 (gradient descent) 求最佳解
  * 視覺化<br>
    ![t-SNE 視覺化](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/t-SNE_%E6%B5%81%E5%9E%8B%E9%82%84%E5%8E%9F.jpg)<br>
    ![t-SNE MNIST視覺化](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/t-SNE_MNIST%E8%A6%96%E8%A6%BA%E5%8C%96.jpg)
    * S 型的展開<br>
      ![S Curve](https://github.com/sueshow/Python_ML- Marathon/blob/main/Picture/t-SNE_S%20Curve.png)
    * 瑞士捲的展開<br>
      ![Swiss Roll](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/t-SNE_Swiss%20Roll.png)
    * 斷球面地展開<br>
      ![Severed Sphere](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/t-SNE_Severed%20Sphere.png)
  * 優點：當特徵數量過多時，使用 PCA 可能會造成降維後的 underfitting，這個可以考慮使用 t-SNE 來降維
  * 缺點：t-SNE 的需要比較多的時間執行
  * 參考資料
    * [資料降維與視覺化：t-SNE 理論與應用](https://www.mropengate.com/2019/06/t-sne.html)
* 範例與作業
  * [範例D061](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_061_HW_tsne/Day_061_tsne_sample.ipynb)
    * 資料集：digits(4個數字)
  * [作業D061](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_061_tsne_Ans.ipynb)
    * 資料集：digits

Back to <a href="#非監督式的機器學習">非監督式的機器學習</a>
<br>
<br>


### D062-程式實作-T-SNE-分群與流形還原
* 流形還原
  * 意義：將高維度上相近的點，對應到低維度上相近的點，沒有資料點的地方不列入考量範圍。簡單的說，如果資料結構像瑞士捲一樣，那麼流形還原就是把它攤開鋪平 (流形還原資料集的其中一種，就是叫做瑞士捲-Swiss Roll)
  * 模型
    * t-sne：較實用<br>
      ![流行還原](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/t-SNE_%E6%B5%81%E5%9E%8B%E9%82%84%E5%8E%9F.jpg)
    * Isomap
    * LLE
    * MDS 
* 範例與作業
  * [範例D062](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_062_HW_tsne/Day_062_tsne.ipynb)
    * 使用同心圓的資料，觀察 t-sne 在不同困惑度(perplexity)下，分群的效果如何變化
    * 觀察結果
      * perplexity 越低時，圓形越零散，越高時不僅分群好，也讓圖形更接近原圖
      * perplexity 越高，執行時間也越久
  * [作業D062](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_062_tsne_Ans.ipynb)
    * 使用 S 形資料集，觀察 t-sne 在困惑度(perplexity)有所不同時，流行還原的效果如何變化
    * 觀察結果
      * perplexity 越高，流形效果越好，但並不會接近原圖，主要是因為資料不像同心圓需要首尾相接，因此在還原流行時會變成一直線
      * perplexity 越高，執行時間也越久，效果越好

Back to <a href="#非監督式的機器學習">非監督式的機器學習</a>
<br>
<br>


## 深度學習理論與實作
### D063-深度學習簡介
* 機器學習與深度學習
  * 機器學習
    * 電腦透過資料中的不同特徵進行學習進而產生預測結果
  * 深度學習
    * 將影像、文字匯入電腦中經由大量運算，使得電腦能夠自動判別
    * 人工智能發展歷史<br>
      ![人工智能發展歷史](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%99%BC%E5%B1%95%E6%AD%B7%E5%8F%B201.png)<br>
      ![人工智能發展歷史](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%99%BC%E5%B1%95%E6%AD%B7%E5%8F%B202.png)
    * 應用爆發的關鍵：類神經應用直到 2012 年 AlexNet 在 ImageNet 圖像分類競賽獲得驚艷表現後，才重回主流舞台
      * 算法改良
        * 網路結構：CNN 與 RNN 等結構在神經連結上做有意義的精省，使得計算力得以用在刀口
        * 細節改良
          * DropOut(隨機移除)：同時有節省連結與集成的效果
          * BatchNormalization(批次正規化)：讓神經層間有更好的傳導力
      * 計算機硬體能力提升：圖形處理器(GPU)的誕生，持續了晶片摩爾定律，讓計算成為可行
      * 巨量資料
        * 細節改良：個人行動裝置的普及及網路速度的持續提升，帶來巨量的資料，使得深度學習有了可以學習的素材
    * 巨觀結構：又為端到端學習，通過卷積、池化和誤差反向傳播等步驟，進行特徵學習
      * 輸入層：輸入資料進入的位置
      * 輸出層：輸出預測值的最後一層 → 視覺對象
      * 隱藏層：除上述兩層外，其他層都稱為隱藏層<br>
        ![巨觀結構](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92_%E5%85%B7%E8%A7%80%E7%B5%90%E6%A7%8B.png)
    * 微觀結構
      * 啟動函數(Activation Function)：位於神經元內部，將上一層神經元的輸入總和，轉換成這一個神經元輸出值的函數
      * 損失函數(Loss Function)：定義預測值與實際值的誤差大小
      * 倒傳遞(Back-Propagation)：將損失值，轉換成類神經全種更新的方法<br>
        ![微觀結構](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92_%E5%BE%AE%E8%A7%80%E7%B5%90%E6%A7%8B.png)    
  * 神經網路
    * 架構示意圖介紹<br>
      ![架構示意圖](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF_%E6%9E%B6%E6%A7%8B%E7%A4%BA%E6%84%8F%E5%9C%96%E4%BB%8B%E7%B4%B9.png)
    * 架構介紹：一個最簡單的神經網路，架構分成輸入層、隱藏層、輸出層(稱為正向傳播)
      * 輸入層(input layer)
        * 輸入層由多個節點所構成，主要功能為接收資料並輸入訊息，此層呈現神經網路之輸入變數，神經元數目會依照輸入的參數決定輸入之數量
      * 隱藏層(hidden layer)
        * 隱藏層介於輸入層與輸出層之間，表示輸入變數間的交互影響。利用活化函數將神經元以非線性的模式，從輸入層至隱藏層及輸出層，以解決較複雜的非線性問題
        * 隱藏層的數量沒有明確規定，若隱藏層數量較多，神經網路複雜程度較高，在學習過程中可能會有難以收斂之情形，因此必須衡量隱藏層的數量再做訓練
      * 輸出層(output layer)
        * 輸出層主要功能為處理隱藏層所輸出的資料，此層呈現神經網路之輸出變數
    * 實務應用
      * 前饋神經網絡(Feedforward Neural Network)<br>
        ![前饋神經網絡](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E5%89%8D%E9%A5%8B%E7%A5%9E%E7%B6%93%E7%B6%B2%E7%B5%A1.jpg)  
        * 各個神經元接受前一級的輸入，並輸出到下一級，模型中(層與層間、層內間)沒有反饋
        * 層與層之間通過「全連接」進行連接，即兩個相鄰層之間的神經元完全成對連接，但層內的神經元不相互連接
        * 感知機網絡(Perceptron Networks)是一種特殊的前饋神經網落
          * 無隱藏層，只有輸入層/輸出層
          * 無法擬合複雜的數據
      * 卷積神經網路(CNN，Convolutional Neural Network)
        * 概念<br>
          ![CNN概念](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/CNN%E6%A6%82%E5%BF%B5.png)
        * 神經元結構<br>
          ![神經元結構](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E7%A5%9E%E7%B6%93%E5%85%83%E7%B5%90%E6%A7%8B_CNN.png)
          * 設計目標：影像處理
          * 結構改進：CNN 參考像素遠近省略神經元，並且用影像特徵的平移不變性來共用權重，大幅減少了影像計算的負擔
          * 衍伸應用：只要符合上述兩種特性的應用，都可以使用 CNN 來計算，例如 AlphaGo 的 v18 版的兩個主網路都是 CNN<br>
         ![CNN屬於前饋神經網路](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/CNN%E5%B1%AC%E6%96%BC%E5%89%8D%E9%A5%8B%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF.jpg)   
    * 遞歸神經網路 (RNN, Recurrent Neural Network)
      * 設計目標：時序資料處理、自然語言處理
      * 結構改進：RNN 雖然看似在 NN 外增加了時序間的橫向傳遞，但實際上還是依照時間遠近省略了部分連結
      * 衍伸應用：只要資料是有順序性的應用，都可以使用 RNN 來計算，近年在自然語言處理 (NLP) 上的應用反而成為大宗
      ![RNN不屬於前饋神經網路](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/RNN%E4%B8%8D%E5%B1%AC%E6%96%BC%E5%89%8D%E9%A5%8B%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF.jpg)
  * 類神經網路(Neural Network)
    * 在 1956 年在達特茅斯會議中誕生，以數學模擬神經傳導輸出預測，在初期人工智慧領域中就是重要分支
    * 因層數一多計算量就大幅增加等問題，過去無法解決，雖不斷有學者試圖改善，在歷史中仍不免大起大落
    * 直到近幾年在算法、硬體能力與巨量資料的改善下，多層的類神經網路才重新成為當前人工智慧的應用主流<br>
      ![類神經網路](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF.png)
  * 比較
    * 機器學習 vs. 深度學習
      <table border="1" width="15%">
        <tr>
          <th width="5%"> </a>
          <th width="5%"> Machine Learning</a>
          <th width="5%"> Deep Learning</a>
        </tr>
        <tr>
          <td> Amount of Data </td>
          <td> Small </td>
          <td> Big </td>
        </tr>
        <tr>
          <td> Features </td>
          <td> Create your own </td>
          <td> Learns automatically </td>
        </tr>
        <tr>
          <td> Time </td>
          <td> Short </td>
          <td> Long </td>
        </tr>
        <tr>
          <td> Accuracy </td>
          <td> Good </td>
          <td> Best </td>
        </tr>
        <tr>
          <td> Debugging </td>
          <td> Easy </td>
          <td> Very Tough </td>
        </tr>
        <tr>
          <td> Expensive </td>
          <td> Less </td>
          <td> More </td>
        </tr>
        <tr>
          <td> Decision Path </td>
          <td> Yes </td>
          <td> No </td>
        </tr>
      </table>
      
    * 類神經網路 vs. 深度學習
      * 基礎要素：深度學習是「比較多層」的類神經網路
      * 實務應用：因著設計思路與連結架構的不同，兩者有了很大的差異性
        <table border="1" width="15%">
          <tr>
            <th width="5%"> </a>
            <th width="5%"> 類神經網路(Neural Network) </a>
            <th width="5%"> 深度學習(Deep Learning)</a>
          </tr>
          <tr>
            <td> 隱藏層數量 </td>
            <td> 1~2層 </td>
            <td> 十數層到百層以上不等 </td>
          </tr>
          <tr>
            <td> 活躍年代 </td>
            <td> 1956~1974 </td>
            <td> 2011至今 </td>
          </tr>
          <tr>
            <td> 代表結構 </td>
            <td> 感知器(Perceptron)<br>
                 啟動函數(Activation Function) </td>
            <td> 卷積神經網路(CNN)<br>
                 遞歸神經網路(RNN) </td>
          </tr>
          <tr>
            <td> 解決問題 </td>
            <td> 基礎迴歸問題 </td>
            <td> 影像、自然語言處理等多樣問題 </td>
          </tr>
        </table>
        
* 參考資料
  * [Machine Learning VS Deep Learning: Whats The Difference](https://www.youtube.com/watch?v=LhVY0-S7cAM&embeds_referring_euri=https%3A%2F%2Fwww.cupoy.com%2F&source_ve_path=OTY3MTQ&feature=emb_imp_woyt)
  * [Deep Learning In 5 Minutes | What Is Deep Learning? | Deep Learning Explained Simply | Simplilearn](https://www.youtube.com/watch?v=6M5VXKLf4D4)
  * 【重要】[深度學習基本概念](https://www.youtube.com/watch?v=TrhwJQ12EN4)
  * [人工神經網路 VS 生物神經網路](https://www.youtube.com/watch?v=lAaCeiqE6CE)
  * [人工智慧大歷史](https://suipichen.medium.com/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7%E5%A4%A7%E6%AD%B7%E5%8F%B2-ffe46a350543)
  * [機器學習簡介](https://developers.google.com/machine-learning?hl=zh-tw)
  * [跟著大神學 AI：Google 免費機器學習課程上線，影片都有中文字幕超佛心](https://buzzorange.com/techorange/2018/03/02/learn-with-google/)
* 範例與作業
  * [作業D063](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_063_Intro_of_DNN_Ans.ipynb)
    * 無程式撰寫，有補充教材

Back to <a href="#深度學習理論與實作">深度學習理論與實作</a>
<br>
<br>

### D064-深度學習-模型調整與學習曲線
* 深度學習
  * 深度神經網路(Supervised LearningDeep Neural Network)
    * 簡介[D064-D065]
    * 套件介紹[D066-D069]
      * Keras 簡介與安裝
      * Keras 內建資料集下載
      * 如何用 Keras 搭建類神經網路
    * 組成概念[D072-D076]
      ![]()
    * 訓練技巧
    * 應用案例
  * 卷積神經網路(Convolutional Neural Network，CNN)
    * 簡介、套件介紹、訓練技巧、電腦視覺
  * 深度學習簡介：神經網路歷史、深度學習概念、深度學習體驗[D064-D065]
* 深度學習體驗平台：[TensorFlowPlayGround](https://playground.tensorflow.org)
  * TensorFlow PlayGround 是 Google 精心開發的體驗網頁，提供學習者在接觸語言之前，就可以對深度學習能概略了解
  * 平台簡介說明<br>
    ![平台簡介說明](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/TF%20PlayGround_%E5%B9%B3%E5%8F%B0%E7%B0%A1%E4%BB%8B%E8%AA%AA%E6%98%8E.png)
  * 平台資料集：有 4 個分類問題與 2 個迴歸問題<br>
    ![平台資料集](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/TF%20PlayGround_%E5%B9%B3%E5%8F%B0%E8%B3%87%E6%96%99%E9%9B%86.png)
  * 練習
    * 1：按下啟動，觀察指標變化
      * 全部使用預測值，按下啟動按鈕
        * 遞迴次數(Epoch)：逐漸增加 → 提高模型成效
        * 神經元(中央)：方框圖案逐漸明顯，權重逐漸加粗，滑鼠移至上方會顯示權重
        * 訓練/測試誤差：開始明顯下降，幅度漸漸趨緩
        * 學習曲線：訓練/測試誤差
        * 結果圖像化：圖像逐漸穩定
      * 實驗結果
        * 訓練/測試誤差趨近 0 為主，這種情況稱作收斂
    * 2：增加隱藏層數：加深
      * 練習
        * 資料及切換：分群資料集(左下)-2群，調整層數後啟動學習
        * 資料及切換：分群資料集(左上)-同心圓，調整層數後啟動學習
        * 資料及切換：迴歸資料集(左)-對角線，調整層數後啟動學習<br>
        ![資料集](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/TF%20PlayGround_%E8%B3%87%E6%96%99%E9%9B%86.png)
      * 實驗結果
        * 2 群與對角線：因資料集結構簡單，即使沒有隱藏層也會收斂
        * 同心圓：資料集稍微複雜(無法線性分割)，因此最少要一層隱藏層才會收斂
    * 3：增減神經元數：加寬
      * 練習
        * 資料集切換：分類資料集(左上)-同心圓，隱藏層設為 1 後啟動學習
        * 切換不同隱藏層神經元數量後，看看學習效果
      * 實驗結果
        * 當神經元少於等於兩個以下時，將無法收斂<br>
          ![增減神經元數](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/TF%20PlayGround_%E5%A2%9E%E6%B8%9B%E7%A5%9E%E7%B6%93%E5%85%83%E6%95%B8.png.crdownload)
    * 4：切換不同特徵
      * 練習
        * 資料集切換：分類資料集(左上)-同心圓，隱藏層 1 層，隱藏神經元 2 個
        * 切換任不同的 2 個特徵後啟動，看看學習效果
      * 實驗結果
        * 當特徵選到兩個特徵的平方時，即使中間只有 2 個神經元也會收
    * 重點
      * 雖然圖像化更直覺，但並非量化指標且可視畫不容易，故深度學習的觀察指標仍以「損失函數/誤差」為主
      * 對於不同資料類型，適合加深與加寬的問題都有，但「加深」適合的問題類型較多
      * 輸入特徵的選擇影響結果甚鉅，因此深度學習也需要考慮「特徵工程」
* 範例與作業
  * [作業D064](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_064_%E4%BD%9C%E6%A5%AD%E8%A7%A3%E7%AD%94/Day_064_Experience_of_DNN1_Ans.ipynb)
    * 選擇迴歸資料集(右)-交錯六群，挑戰測試誤差(Test Loss)最低能到多少
      * 在其他參數(特徵X1、X2)保持預設值的情況下，只允許調整隱藏層層數與神經元數量
      * 隱藏層神經元總數最多 8 個 (即 : 可以單層 8 個，或兩層 5 個與 3 個)
      * 遞迴次數只允許跑到 500 次
      * 練習設定
        * 隱藏層神經元單層 8 個 → Test loss=0.014、Training loss=0.014
        * 隱藏層神經元兩層 5 個與 3 個 → Test loss=0.013、Training loss=0.011
Back to <a href="#深度學習理論與實作">深度學習理論與實作</a>
<br>
<br>

### D065-深度學習-啟動函數與正規化
* 目標
  * 理解批次大小(Batch size)與學習速率(Learning Rate)對學習結果的影響
  * 經由實驗，體驗不同啟動函數的差異性
  * 體驗正規化(Regularization)對學習結果的影響
* 練習
  * 5：切換批次大小
    * 練習
      * 資料集切換：分類資料集(右下)-螺旋雙臂，特徵全選，隱藏層 1 層/ 8 神經元
      * 調整「不同的批次大小」後執行 500 次遞迴，看看學習效果
    * 實驗結果：批次的影響
      * 批次很小時，雖然收斂過程非常不穩定，但平均而言會收斂到較好的結果，但仍會受到 sample size 的影響<br>
        ![切換批次大小](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/TF%20PlayGround_%E5%88%87%E6%8F%9B%E6%89%B9%E6%AC%A1%E5%A4%A7%E5%B0%8F.png)
      * 實務上，批次如果極小，效果確實比較好，但計算時間會相當久，因此通常會依照時間需要而折衷
  * 6：切換學習速率
    * 練習
      * 資料集切換：分類資料集(右下)-螺旋雙臂，特徵全選，隱藏層 1 層/ 8 神經元，批次大小固定 10
      * 調整「不同的學習速率」後執行 500 次遞迴，看看學習效果
    * 實驗結果：學習速率的影響為收斂走得路線，可在執行過程中調整
      * 學習率會先設定較大，經過幾個 Epoch 後再調小
      * 小於 0.3 時，學習速率較大時，收斂過程會越不穩定，但會收斂到較好的結果
      * 大於 1 時，因為過度不穩定而導致無法收斂<br>
        ![切換學習速率](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/TF%20PlayGround_%E5%88%87%E6%8F%9B%E5%AD%B8%E7%BF%92%E9%80%9F%E7%8E%87.png)
  * 7：切換啟動函數
    * 練習
      * 資料集切換：分類資料集(右下)-螺旋雙臂，特徵全選，隱藏層 1 層/ 8 神經元，批次大小固定 10，學習速率固定 1
      * 調整「不同的啟動函數」後執行 500 次遞迴，看看學習效果
    * 實驗結果：啟動函數的影響
      * 在這種極端的情形下，Tanh 會無法收斂，Relu 很快就穩定在很糟糕的分類狀態，惟有 Sigmoid 還可以收斂到不錯的效果
      * 實務上，Sigmoid 需要大量計算時間，而 Relu 則相對快得很多，這也是需要取捨的，在本例中因位只有一層，所以上述狀況不太明顯<br>
        ![切換啟動函數](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/TF%20PlayGround_%E5%88%87%E6%8F%9B%E5%95%9F%E5%8B%95%E5%87%BD%E6%95%B8.png)
  * 8：切換正規化選項與參數(正規化對於模型的影響性)
    * 練習
      * 資料集切換：分類資料集(右下)-螺旋雙臂，特徵全選，隱藏層 1 層/ 8 神經元，批次大小固定 10，學習速率固定 0.3，啟動函數設為 Tanh
      * 調整「不同的正規化選項與參數」後執行 500 次遞迴，看看學習效果
    * 實驗結果：可處理 overfitting 的問題、預防已知答案
      * 已知上述設定會收斂，只是在較小的 L1/L2 正規化參數下收斂比較穩定一點
      * 但正規化參數只要略大，反而會讓本來能收斂的設定變得無法收斂，這點 L1 比 L2 情況略嚴重，因此本例中最適合的正規化參數是 L2 + 參數 0.001
      * 實務上，L1/L2 較常使用在非深度學習上，深度學習上效果有限
  * 重點
    * 批次：批次越小學習曲線越不穩定、但收斂越快
    * 學習速率：學習速率越大學習曲線越不穩定、但收斂越快，但是與批次的大小不同的是學習速率大於一定以上時，有可能不穩定到無法收斂
    * 啟動函數：當類神經網路層數不多時，啟動函數 Sigmoid/Tanh  的效果比 Relu 更好
    * 正規化：L1/L2 正規化在非深度學習上效果較明顯，而正規化參數較小才有效果
* 範例與作業
  * [作業D065](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_065_%E4%BD%9C%E6%A5%AD%E8%A7%A3%E7%AD%94/Day_065_Experience_of_DNN2_Ans.ipynb)
    * 選擇分類資料集(右下)-螺旋雙臂-交錯六群
      * 限定特徵只能使用前兩個(X1、X2)
      * 隱藏層 2 層滿 (共 8 * 2 =16 個神經元)
      * 遞迴次數只允許跑到 500 次
      * 可自由調整批次大小、學習速率、啟動函數、正規化選項與參數
        * 批次大小=3、學習速率=0.03、啟動函數=Tanh、正規化選項=None、正規化參數=0.001 → Test loss=0.042、Training loss=0.08
        * 批次大小=10、學習速率=0.1、啟動函數=Tanh、正規化選項=None、正規化參數=0.001 → Test loss=0.01、Training loss=0.009

Back to <a href="#深度學習理論與實作">深度學習理論與實作</a>
<br>
<br>


## 初探深度學習使用Keras
### D066-Keras安裝與介紹
* Keras
  * 易學易懂的深度學習套件
    * 以設計為出發點：容易上手，因此隱藏很多實作細節，自由度稍嫌不夠，但很適合教學
    * Keras 實作並優化了各式經典組件，因此即使是同時熟悉 TensorFlow 與 Keras 的老手，開發時也會兩者並用互補
  * Keras 包含的組件
    * 模型形狀類
      * 直覺概念：神經元數/隱藏層數/啟動函數
      * Keras 組件：Sequential Model/Functional Model/Layers
    * 配置參數類
      * 直覺概念：學習速率/批次大小/正規化
      * Keras 組件：Ｏptimier/Reguliarizes/Callbacks
  * 深度學習寫法封裝
    * TensorFlow 將深度學習中的 GPU/CPU 指令封裝起來，減少語法差異
    * Keras 是將 TensorFlow 等後端程式更近一步封裝成單一套件並優化，用少量的程式便能實現經典模型 
  * Keras的後端
    * Keras 的實現實際上完全依賴 TensorFlow 的語法完成，這種情形我們稱 TensorFlow 是 Keras 的一種後端(Backend)<br>
      ![Keras的後端](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/Keras_Keras%E7%9A%84%E5%BE%8C%E7%AB%AF.png) 
    * Keras/TensorFlow 的比較：由於目前整併兩者的 TensorFlow 2.0 仍在測試階段，本教材仍以 Keras 為主
      <table border="1" width="15%">
          <tr>
            <th width="5%"> </a>
            <th width="5%"> Keras </a>
            <th width="5%"> Tensorflow </a>
          </tr>
          <tr>
            <td> 學習難度 </td>
            <td> 低 </td>
            <td> 高 </td>
          </tr>
          <tr>
            <td> 模型彈性 </td>
            <td> 中 </td>
            <td> 高 </td>
          </tr>
          <tr>
            <td> 主要差異 </td>
            <td> 處理神經層 </td>
            <td> 處理資料流 </td>
          </tr>
          <tr>
            <td> 代表組件 </td>
            <td> Layers/Model </td>
            <td> Tensor/Session/Placeholder </td>
          </tr>
      </table>
      
  * 安裝流程
    * 是否有 GPU：可平行運算、加速矩陣運算，適用於 Nvidia 的 GPU 框架上
      * 因有 GPU 需要先裝 GPU 的指令集，故有 GPU 需要 4 個步驟，沒有只需 2 步驟<br>
        ![安裝步驟](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/Keras_Keras%E5%AE%89%E8%A3%9D%E6%AD%A5%E9%A9%9F.png)
      * 有 GPU 版：如找不到 pip 指令，可採用 pip3 代替 pip 執行安裝
        * 由於 GPU 的 CUDA/cuDNN 版本經常升級，因此 TensorFlow/Keras 的版本也需要頻繁更換版本，因此建議以安裝當時的[官網](https://www.tensorflow.org/install/gpu)資訊為準，查閱 Software requirements 部分<br>
          ![Software requirements](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/Keras_Software%20requirements.png)
        * Step 1：安裝 CUDA
          * 到[官網](https://developer.nvidia.com/cuda-downloads)依作業系統版本與需求選擇選項，下載 CUDA 驅動程式並安裝
            * Linux 選項範例<br>
              ![Linux 選項範例](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/Keras_Linux%E9%81%B8%E9%A0%85%E7%AF%84%E4%BE%8B.png)
            * Windows 選項範例<br>
              ![Windows 選項範例](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/Keras_Windows%E9%81%B8%E9%A0%85%E7%AF%84%E4%BE%8B.png)
            * Mac OS 選項範例
              ![Mac OS 選項範例](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/Keras_Mac%20OS%E9%81%B8%E9%A0%85%E7%AF%84%E4%BE%8B.png)
        * Step 2：安裝 cuDNN
          * cuDNN 安裝時，需要從[官網](https://developer.nvidia.com/cudnn)點選
          * 這個步驟需要在 NVIDIA 註冊才能下載
          * 註冊並登入後，會看到類似下面的畫面，請依照上一步驟的 CUDA 版本，選擇對應的 cuDNN 版本並安裝
            ![cuDNN安裝](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/Keras_cuDNN%E5%AE%89%E8%A3%9D.png)
        * Step 3：安裝 TensorFlow GPU 版，後面有 -gpu 才是正確的「GPU版」Tensorflow
          * 指令：pip install tensorflow-gpu (Ubuntu 前面加上 sudo)
        * Step 4：安裝 Keras
          * 指令：pip install keras
          * (Windows) 新增環境變數於 PATH
            * Win10：可從 開始/控制台/系統 開啟視窗後，點選「進階」分頁最下面的按鈕「環境變數」，會跳出下列視窗，請在下半視窗中尋找「Path」變數，把下列兩個路徑加入，Windows 其他版本請搜尋「[windows版號] set path environment」
               * C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin
               * C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\libnvvp
               * (舊版項目間要用分號(;)隔開/CUDA 版號請依 Step1 實際安裝版本為準)<br>
              ![set path environment](https://github.com/sueshow/Python_ML-Marathon/blob/main/Picture/Keras_4-2%E6%AD%A5%E9%A9%9FWindows.png)
      * 沒有 GPU 版：如找不到 pip 指令，可採用 pip3 代替 pip 執行安裝
        * Step 1：安裝 TensorFlow：pip install tensorflow (Ubuntu 前面加上 sudo)
        * Step 2：安裝 Keras：pip install keras
    * 作業系統
      * 因為不同作業系統間，GPU 的安裝步驟會因介面或指令有所不同，所以我們會分 Windows/Linux (以Ubuntu為例)/Mac 分別介紹流程
    * 注意事項
      * 是否使用 Anaconda 虛擬環境
        * Python 環境是採用 Anaconda 安裝，那後續安裝需切換至常用的虛擬環境下安裝(點選 Anaconda/Anaconda Prompt 後再安裝)，以確保安裝與常用環境是同一目錄
        * Python 環境是用指令安裝，那麼直接用指令安裝即可( Windows 可以用命令提示字元)
    * 驗證安裝
      * 安裝完後，開啟 .ipynb 檔輸入下列指令並執行，確認是否有順利執行
        * import tensorflow
        * import keras
* 範例與作業
  * [範例D066](https://github.com/sueshow/Python_ML-Marathon/blob/main/Homework/Day_066_HW/Day_066_Introduction_of_Keras.ipynb)
    * Keras 的使用方法及常見錯誤
    * 檔案：Day_066_Introduction_of_Keras.ipynb
  * [作業D066](https://github.com/sueshow/Python_ML-Marathon/blob/main/Solution/Day_066_Keras_Introduction_Ans.ipynb)
    * 檢查 Keras 的 backend
    * 使用 fuzz factor 產生 1e-99
    * 設定 Keras 浮點運算為 float16
Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D067-KerasDataset
* Keras
  * 資料集
    * CIFAR10 小圖像分類---影像分類與識別學習
      * 數據集 50,000 張 32X32 彩色(RGB)訓練圖像，標註超過 10 個類別，10,000 張測試圖像
      * 語法
        ```
        from keras.datasets import cifar10
        (x_train, y_train), (x_test, y_test) = cifar10.load_data() 
        ```
    * CIFAR100 小圖像分類---影像分類與識別學習
      * 數據集 50,000 張 32X32 彩色訓練圖像，標註超過 100 個類別，10,000 張測試圖像
      * 語法
        ```
        from keras.datasets import cifar100
        (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')   # 詳細標示
        # label_mode='coarace' 大致標示， 
        ```
    * IMDB 電影評論情緒分類---文本分析與情緒分類
      * 來自 IMDB 的 25,000 部電影評論的數據集，標有情緒(正面/負面)。評論已經過預處理，每個評論都被編碼為一系列單詞索引(整數)
      * 單詞由數據集中的整體頻率索引
        * 整數「3」編碼數據中第 3 個最頻繁的單詞
        * 「0」不代表特定單詞，而是用於編碼任何位知單詞
      * 語法
        ```
        from keras.datasets import imdb
        (x_train, y_train), (x_test, y_test) = imdb.load_data(path="imdb.npz",num_words= None,skip_top=0,maxlen=None, seed=113,start_char=1,oov_char=2,index_from=3)
        ```
        * path：資料夾位置，如果沒有本地數據('~/.keras/datasets/'+path)，數據集將被下載到此位置
        * num_words：整數或無，保留前 n 個最常出現的詞彙(最小索引值)。最常見的詞彙需要考慮，任何不太頻繁的單詞將 oov_char 在序列數據中顯示為值
        * skip_top：整數，索引比 n 大的數字會被 oov_char 取代。最常被忽略的詞 (它們將 oov_char 在序列數據中顯示為值)
        * maxlen：int。最大序列長度，超過 n 個單詞的評論會被截斷
        * 種子：int。用於可重複數據改組的種子
        * start_char：int，每條評論的起始索引為 n，也就是起始字為 n，不符合規定也會被 oov_char 取代。序列的開頭將標有此字符，設置為 1，然 0 通常是填充字符
        * oov_char：int，不滿足 num_words 或 skip_top 限制的單詞會設定為 n。這是因為切出字 num_words 或 skip_top 限制將這個字符替換
        * index_from：int，索引值 = 索引值加上 n，調整索引值大小的參數，default = 3。使用此索引和更高的索引實際單詞
    * 路透社 newswire 話題分類---文本分析與情緒分類
      * 來自路透社的 11,228 條新聞專線的數據集，標註 46 個主題，與 IMDB 數據集一樣，每條線都被編碼為一系列字索引
      * 語法
        ```
        from keras.datasets import reuters

        (x_train, y_train), (x_test, y_test) = reuters.load_data(path=“reuters npz”,num_words= None,skip_top=0,maxlen=None, test_split=0.2,seed=113,start_char=1,oov_char=2,index_from=3)
        ```
    * 手寫數字的 MNIST 數據庫---影像分類與識別學習
      * 數據集包含 10 個數字的 60,000 個 28x28 灰度圖像，及 10,000 個圖像的測試集
      * 語法
        ```
        from keras.datasets import mnist
        (x_train, y_train), (x_test, y_test) = mnsit.load_data() 
        ```
    * 時尚文章的時尚 MNIST 數據庫---影像分類與識別學習
      * 數據集包含 10 個時尚類別的 60,000 個 28x28 灰度圖像，及 10,000 個圖像的測試集
      * 語法
        ```
        from keras.datasets import fashion_mnsit
        (x_train, y_train), (x_test, y_test) = fashion_mnsit.load_data()
        ```
    * 波士頓房屋價格迴歸數據集---Data/Numerical 學習
      * 數據集取自卡內基梅隆大學維護的 StatLib 庫
      * 20 世紀 70 年代後期，樣本在波士頓郊區的不同位置包含 13 個房屋屬性。目標是一個地點房屋的中位值(單位：k$)
      * 語法
        ```
        from keras.datasets import boston_housing
        (x_train, y_train), (x_test, y_test) = boston_housing.load_data() 
        ```
  * (預設)資料夾：在 Windows 環境，使用 Anaconda 安裝
    * Anaconda 應用程式安裝目錄下的 Keras 子資料夾，需要搜索找到
    * Anaconda 影用程式存儲 Keras 模型和資料集檔，用對應的用戶資料夾下的「.keras」資料夾下
    * 資料集下載後預設存儲目錄「C:Users\Administrator\.keras\datasets」下的同名檔
  * 執行下載
    ```
    from keras.datasets import cifar10    #從 Keras 導入相應的模組<br>
    (x_train, y_train), (x_validate, y_validate) = cifar10.load_data()   #從網路即時下載
    ```
* 範例與作業(待上傳)
  * [範例D067]()
    * 資料集：CIFAR10
    * 過程：影像正規化、轉換 label (onehot encoding)
  * [作業D067]()
    * 資料集：CIFAR100 → 分類類別為100
    * 過程：影像正規化、轉換 label (onehot encoding)

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D068-KerasSequentialAPI
* Keras 框架<br>
  ![框架]()
* 序列模型(Sequential Model)：宣告式 API，靜態
  * 序列模型是多個網路層的線性堆疊，循序性地加入，可在構造函數中傳入一些列的網路層
  * 語法
    ```
    from keras.models import Sequential
    from keras.layers import Dense, Activation

    model = Sequential()
    model.add(Dense(32, _input_dim=784))
    model.add(Activation("relu"))

    # 另外的寫法
    model = Sequential([Dense(32, _input_shap=(784,)), Activation('relu')
    ```
  * 基礎元件
    * 宣告 Model
    * model.add 添加層
    * model.compile 模型訓練
    * model.fit 模型訓練參數設置+訓練
    * 模型評估
    * 模型預測
      ```
      # 先匯入套件
      import tensorflow as tf
      from tensorflow.keras import layers

      # 再來建立序列模型
      model=tf.keras.Sequential([layers.Dense(20,input_shape=(1000,)),   # 輸入為1000的一維向量
                                 layers.Dense(30),                       # 每一層的數值是units，用來定義該層輸出的大小
                                 layers.Dense(25),
                                 layers.Dense(10)])
      ```  
  * 指定模型的輸入維度
    * Sequential 的第一層(只有第一層，後面的層會自動匹配)需要知道輸入的 shape 
在第一層加入一個 input_shape 參數，input_shape 應該是一個 shape 的 tuple 資料類型
      * input_shape 是一系列整數的 tuple，某些位置可為 None
      * input_shape 中不用指明 batch_size 的數目
    * 2D 網路層：如 Dense 允許在層的構造函數的 input_dim 中指定輸入的維度
    * 3D 時間層：可在構造函數中指定 input_dim 和 input_length 來實現
    * 如：RNN 可指定 batch_size，後面輸入必須是 (batch_size, input_shape)
    * 常用參數
      <table border="1" width="15%">
          <tr>
            <th width="5%"> 名稱 </a>
            <th width="5%"> 作用 </a>
            <th width="5%"> 原型參數 </a>
          </tr>
          <tr>
            <td> Dense </td>
            <td> 實現全連接層 </td>
            <td> Dense(units, activation, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros') </td>
          </tr>
          <tr>
            <td> Activation </td>
            <td> 對上層輸出應用激活函數 </td>
            <td> Activation(activation) </td>
          </tr>
          <tr>
            <td> Dropout </td>
            <td> 對上層輸出應用 dropout 以防止過擬合 </td>
            <td> Dropout(ration) </td>
          </tr>
          <tr>
            <td> Flatten </td>
            <td> 對上層輸出一維化 </td>
            <td> Flatten() </td>
          </tr>
          <tr>
            <td> Reshape </td>
            <td> 對上層輸出 reshape </td>
            <td> Reshape(target_shape) </td>
          </tr>
      </table>
      
* 範例與作業(待上傳)
  * [範例D068]()
    * 資料集：CIFAR10
    * 重點：說明身經網路模型在 Keras 所需相關的 library
  * [作業D068]()
    * 資料集：CIFAR10
    * 重點：修改 input shape: (Conv2D(64,(3,3))) 的設定，新增一層 Dense 並觀看 model.summary 的輸出

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D069-KerasModuleAPI
* 函數式 API：宣告式 API，靜態
  * 用戶定義多輸出模型、非循環有向模型(有向無環圖)或具有共享層的模型等複雜模型的途徑
  * 利用函數式 API，可輕易地重用訓練好的模型：可將任何模型看作是一個層，然後通過傳遞一個張量來調用它。注意：在調用模型時，您不僅重用模型結構，還重用它的權重
* 比較
  * 模型需要多於一個的輸出，選擇函數式模型
  * 函數式模型是最廣泛的一類模型，序貫模型(Sequential)是特例
* 衍伸說明
  * 層對象接受張量為參數，返回一個張量
  * 輸入是張量，輸出也是張量的一個框架就是一個模型，通過 Model 定義
  * 這樣的模型可像 Keras 的 Sequential 一樣被訓練
* 配置
  * 搭建多輸入、多輸出的模型
    ```
    from keras.layers import Input, Embedding, LSTM, Dense
    from keras.models import Model

    # 這些整數在 1 到 10,000 之間(10,000 個詞的詞彙表)，且序列長度為 100 個詞
    # 宣告一個 NAME 去定義 Input
    main_input = Input(shape=(100,), dtype='int32', name='main_input')

    # Embedding 層將輸入序列編碼為一個稠密向量的序列，每個向量維度為 512
    x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)

    # LSTM 層把向量序列轉換成單個向量，它包含整個序列的上下文信息
    lstm_out = LSTM(32)(x)

    # 插入輔助損失，使得即使在模型主損失很高的情況下，LSTM 層和 Embedding 層都能被平穩地訓練
    news_output = Dense(1, activation='sigmoid', name='news_out')(lstm_out)

    # 輔助輸入數據與 LSTM 層的輸出連接起來，輸入到模型
    import keras
    news_input = Input(shape=(5,), name='news_in')
    x = keras.layers.concatenate([lstm_out, news_input])

    # 堆疊多個全連接網路層
    x = Dense(64, activation='relu')(x)
    x = Dense(64, activation='relu')(x)
    # 最後添加主要的邏輯回歸層
    main_output = Dense(1, activation='sigmoid', name='main_output')(x)

    # 宣告 MODEL API，分別採用自行定義的 Input/Output Layer
    model = Model(inputs=[main_input, news_input], outputs=[news_output, main_output])
    model.compile(optimizer='rmsprop',
                  loss={'main_output':'binary_crossentropy', 'news_output':'binary_crossentropy'},
                  loss_weights={'main_output':1., 'news_output':0.2})
    ```
    <br>
    ![Module]()
  * 應用說明
    * 利用函數式 API 可輕易地重用訓練好的模型：可將任何模型看作是一個層，然後通過傳遞一個張量來調用它。注意：在調用模型時，不僅重用模型的架構，還重用了它的權重
    * 具有多個輸入和輸出的模型，函數式 API 使處理大量交織的數據流變得容易
    * 範例
      * 預測 Twitter 上的一條新聞標題有多少轉發和點讚數<br>
        ![Twitter]()
        * 模型的主要輸入將是新聞標題本身，即為一系列詞語
        * 為了增添趣味，模型還添加其他的輔助輸入來接收額外的數據，如新聞標題的發布時間等
        * 該模型將通過兩個損失函數進行監督學習，較早地在模型中使用主損失函數，是深度學習模型的一個良好正則方法
      * 推特推文數據集
        * 建立一個模型來分辨兩條推文是否來自同一個人(如：通過推文的相似性來對用戶進行比較)
        * 將兩條推文編碼成兩個向量，連接向量，然後添加邏輯回歸層，這將輸出兩條推文來自同一作者的概率，模型將接收一對對正負表示的推特數據
        * 由於這個問題是對稱的，編碼第一條推文的機制應該被完全重用來編碼第二條推文(權重及其他全部)
* 範例與作業(待上傳)
  * [範例D069]()
    * 資料集：housing = fetch_california_housing()
    * 重點
      * 用 tensorflow.keras 的 functional API 建立簡單的多輸入單輸出模型，並進行房價預測
      * 輸入 A (4 個 feature)經過隱藏層 1 (30個神經元)和 2(30個神經元)(deep)，輸入 B (5 個 feature)經過隱藏層 3 (30個神經元)後再進行合併(1個神經元)再輸出(指定輸入[A,B]，輸出結果為 concat 後經過 1 個神經元的結果)
      * 說明 compile 與 fit 用法的不同
  * [作業D069]()
    * 資料集
    * 重點
      * 修改 Name 中，自訂義的 Layer 名稱，並增加一層全連接層
      * 宣告 Model API，分別採用自訂義的 Input/Output Layer
      * 透過 model.summary 查看 Layers stack

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D070-深度神經網路的基礎知識
* 深度學習網路歷史
  * 單層感知器(Perceptron)<br>
    ![]()
  * 多層感知器(Multilayer Perceptron)<br>
    ![]()
  * 深度學習網路(DNN)<br>
    ![]()
  * 差異
    * 多層感知器與深度學習網路主要差異為可以多層隱藏層建構出一個具有深度的神經
* 深度學習本身是機器學習領域下的一個分支，核心概念為「從資料中尋找一組最適合解決某種特定問題的函式」
  * 語音辨識：輸入一段語音，機器能辨識出語音對應的文字
  * 圖像辨識：輸入一張貓咪圖片，機器能分辨出這張圖裡有貓
  * 棋藝競賽：輸入圍棋的落子狀況，機器能算出勝率最高的下個落子點
  * 對話系統：在對話系統中，機器接受到 Hi，它能作出一個適當的回應，如 Hello
* 需要深度學習網路
  * 手寫辨識照片利用單一層隱藏層神經網路能達 97% 以上的正確率，但近年來不斷訓練僅能達到接近 97% 的正確率
  * 主要原因為神經網路同時須考慮到照片大小，照片裡出現的圖像、像素及 RGB 等問題，因此需要較大的神經網路才能學習到更多照片裡的資訊
  * 在一個語言辨識的測驗中，無論是淺的或深度神經網路，辨識率都隨著神經元數目的增加而成長
  * 在相同數目的神經元時，深度神經網路的表現總是比較好
* 深度學習網路
  * 概念：從資料中尋找一組最適合解決某種特定問題的函數
    * Define a set of Function
      * 由這一層神經元接收上一層神經元的輸入，經過計算(W*x+b)後可以得到一個輸出結果 $a_1$，而 $a_1$ 又會成為下一層的輸入
        * $[x_1, x_2,..., x_n]$ 是資料輸入層，神經元接受來自 $[X_1,..., X_n]$ 輸入層的資訊，經過 $W=[w_1,..., w_n]$ 權重相乘後加總，加上 Bias 偏移項後，再經過激活函數輸出 Y
        * 整個運算過程都是數值運算，故傳入的內容必須要轉化為數值
        * 每一層的輸入內容皆來自前一層權重矩陣與輸入值向量與偏移量的計算結果 -> 輸出向量 $a_1$ 為一個函數型式<br>
          ![]()
      * $a_1$ 向量為一個函數，當函數只要 x 數量有增減或 Ｗ (權重)/ b (偏移項)發生改變，整個神經網路的函數內容就會改變
      * 當神經網路的神經元越多，則神經網路的候選函數就越多，對問題的解釋能力就越強
      * 核心概念
        * 每一層的神經數量或連接方式改動，就會產生不同的函式集合
        * 每次權重一旦改動，就會產生一組新的函數<br>
          ![]()
    * Evaluate the Functions
      * 使用 Loss Function 作為評估整體神經網路的效能
      * Cross-Entropy 稱 K-L 交叉熵，目的是用來評估模型預測結果與真實結果兩者之間的差距(損失)
    * Pick the best Function
  * 
* 範例與作業
  * [範例D070]()
  * [作業D070]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D071-損失函數
* 範例與作業
  * [範例D071]()
  * [作業D071]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D072-啟動函數
* 範例與作業
  * [範例D072]()
  * [作業D072]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D073-梯度下降GradientDescent
* 梯度下降(Gradient Descent)：最常用的優化算法
  * 定義：機器學習算法當中，優化算法的功能，是通過改善訓練方式，來最小化(或最大化)損失函數
    * 除梯度下降(以趨近的方式找 solution)外，有：最小平方法、SVD分解，這些都有 close form solution
  * 概念
    * 通過尋找最小值，控制方差，更新模型參數，最終使模型收斂
    * $w_{i+1} = w_i - d_i*η_i,  i=0,1,…$
    * 學習率 $η$：可設置為固定值，也可用於一維優化方法沿著訓練的方向逐步更新計算
      * 學習率定義了每次疊代中應該更改的參數量。換句話說，它控制應該收斂到最低的速度
      * 小學習率可以使迭代收斂，大學習率可能超過最小值<br>
        ![]()
    * 參數的更新分為兩步：第一步計算梯度下降的方向，第二步計算合適的學習
  * 過程
    * 首先需要設定一個初始參數值，通常情況下將初值設為零(w=0)，接下來需要計算成本函數 cost
    * 計算函數的導數在某個點處的斜率值，並設定學習效率參數(lr)的值
    * 重複執行上述過程，直到參數值收斂，這樣就能獲得函數的最優解<br>
      ![]()
  * 確定到極值點？
    * 學習率是個挪動步長的基數，df(x)/dx是導函數，當離得遠時導數大，移動的越快；當接近極值時，導數會非常小，移動的速度就非常小，為防止跨過極值點<br>
      ![]()
    * Different initial point will be caused reach different minima, so different results<br>
      ![]()
    * momentum 是梯度下降法中一種常用的加速技術
      * Gradient Descent 的實現：SGD
        * 對於一般的 SGD，其表達式為 $x ← x − α * dx$ (x沿負梯度方向下降)
        * 帶 momentum 項的 SGD 如下形式：<br>
          $$ v =  ß * v − a * dx $$ <br>
          $$ x ← x + v $$ <br>
          * 說明：其中 $ß$ 為 momentum 係數
            * 若上一次的 momentum(即$ß$)與這一次的負梯度方向是相同的，那這次下降的幅度就會加大，這樣做能夠達到加速收斂的過程
            * 若上一次的 momentum 與這一次的負梯度方向是相反的，那這次下降的幅度就會縮減，這樣做能夠達到減速收斂的過程
    * avoid local minima
      * 在訓練神經網絡的時候，通常在訓練剛開始的時候使用較大的 learning rate，隨著訓練的進行，我們會慢慢的減小 learning rate
      * 隨著 iteration 改變 Learning：衰減越大，學習率衰減地越快。衰減確實能夠對震盪起到減緩的作用
  * 缺點
    * 靠近極小值時速度減慢
    * 直線搜索可能產生問題
    * 可能會以「之字型」地下降
* 從程式中微調(Fine Tune)相關的參數
* 範例與作業(待上傳)
  * [範例D073]()
    * function：$y=(x+5)^2$
    * learning rate = 0.01 -> 595次
  * [作業D073]()
    * function：$y=(x+5)^2$
    * 使用不同的組合驗證 learning rate 對所需 iteration 的影響 lr = [0.1, 0.0001] 主要驗證 Lr 對於 grandient 收斂的速度
      * learning rate = 0.1 -> 66次
      * learning rate = 0.0001 -> 執行 10000 次，尚未收斂 

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D074-GradientDescent數學原理
* Gradient 梯度
  * 在微積分裡面，對多元函數的參數求 $∂$ 偏導數，把求得的各個參數的偏導數以向量的形式寫出來，就是梯度
  * 如函數 $f(x)$，對 $x$ 求偏導數，求得的梯度向量就是 $(∂f/∂x)$，簡稱 $grad f(x)$ 或 $▽f(x)$
* 最常用的優化算法：梯度下降
  * 目的：沿著目標函數梯度下降的方向搜索極小值(也可以沿著梯度上升的方向搜索極大值)
  * 要計算 Gradient Descent，考慮
    * Loss = 實際 ydata – 預測 ydata
      -> Loss =  w* 實際 xdata – w*預測 xdata (bias 為 init value，被消除)
    * Gradient = ▽f(θ) (Gradient = ∂L/∂w)
    * 調整後的權重 = 原權重 – η(Learning rate) * Gradient：每走一步，更新一次
* 梯度下降的算法調優
  * Learning rate 選擇，實際上取值取決於數據樣本，如果損失函數在變小，說明取值有效，否則要增大 Learning rate
  * 自動更新 Learning rate  - 衰減因子 decay
    * 算法參數的初始值選擇。初始值不同，獲得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；當然如果損失函數是凸函數則一定是最優解
學習率衰減公式<br>
      (1) $lr_i = lr_{start} * 1.0 / (1.0 + decay * i)$ <br>
      (2) 其中 $lr_i$ 為第一迭代 i 時的學習率，$lr_{start}$ 為初始值，decay 為一個介於[0.0, 1.0]的小數。從公式上可看出：<br>
      
          * decay 越小，學習率衰減地越慢，當 decay=0 時，學習率保持不變 
          * decay 越大，學習率衰減地越快，當 decay=1 時，學習率衰減最快
  * 學習率較小時，收斂到正確結果的速度較慢；學習率較大時，容易在搜索的過程發生震盪
  * 梯度下降的類型
    * 批次(一般)
    * 小批次(mini batch)
    * 隨機 
* 範例與作業(待上傳)
  * [範例D074]()
    * function：$y=(x)^2$
    * learning rate=$10^{-6}$
    * 有提及 AdaGrad (李宏毅) 的 learning rate=1
  * [作業D074]()
    * function：$y=(x+3)^2$
    * 調整其它 Hyperparameters：w_init、epochs、lr、decay(隨參數更新而衰減)、momentom 測試逼近的過程

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D075-BackPropagation
* 反向式傳播 BackPropagation
  * 反向傳播(BP)是「誤差反向傳播」的簡稱，是一種與最優化方法(如梯度下降法)結合使用的方法，對網路中所有權重計算損失函數的梯度。這個梯度會反饋給最優化方法，用來更新權值以最小化損失函數
  * 反向傳播要求有對每個輸入值想得到的已知輸出，來計算損失函數梯度，因此，它通常被認為是一種監督式學習方法，可對每層疊代計算梯度。反向傳播要求人工神經元（或「節點」）的啟動函數可微
  * 流程<br>
    ![推導流程]()
  * 範例說明：以預測水果銷售為例
    * 應給付的價格決定因子：數量(顆數或是單位重量)、單價、稅金
    * 建立運算單元
      * 稅金是恆定的，可當成是 Bias，給定 TAX
      * Input-1：數量，給定 X
      * Input-2：單價，給定 Y
      * Output：f(x) = X * Y * TAX
    * 說明
      * 付費總價格是根據水果價格，稅金變動而受影響
      * 水果價格是根據購買數量與單品價格而變動
      * 可利用每一個 cell (cell - 1：水果價格；cell - 2：付費總價格)，推導微分的結果
      * 更改 Init Data：更改購買數量、TAX的增加
        * 輸出會有變動，模型的執行結果跟預期有落差也是變動，這個落差就是 error rate = (Target 輸出)–(實際輸出) 
  * BP 神經網路是一種按照逆向傳播算法訓練的多層前饋神經網路
    * 優點
      * 具有任意複雜的模式分類能力和優良的多維函數映射能力，解決了簡單感知器不能解決的異或或者一些其他的問題
      * 從結構上講，BP 神經網路具有輸入層、隱含層和輸出層
      * 從本質上講，BP 算法就是以網路誤差平方目標函數、採用梯度下降法來計算目標函數的最小值
    * 缺點
      * 學習速度慢，即使是一個簡單的過程，也需要幾百次甚至上千次的學習才能收斂
      * 容易陷入局部極小值
      * 網路層數、神經元個數的選擇沒有相應的理論指導
      * 網路推廣能力有限
    * 應用
      * 函數逼近
      * 模式識別
      * 分類
      * 數據壓縮
* 前行網路傳播(ForwardPropagation)/反向式傳播(BackPropagation)的差異
  * 第 1 階段：解函數微分
    * 每次疊代中的傳播環節包含兩步： 
      * (前向傳播階段)將訓練輸入送入網路以獲得啟動響應
      * (反向傳播階段)將啟動響應同訓練輸入對應的目標輸出求差，從而獲得輸出層和隱藏層的響應誤差
  * 第 2 階段：權重更新
    * Follow Gradient Descent
      * 第 1 和第 2 階段可以反覆循環疊代，直到網路對輸入的響應達到滿意的預定的目標範圍為止
* 範例與作業(待上傳)
  * [範例D075]()
    * 重點
      * 自定義神經網路架構
      * 初始值設定
      * 使用範例
  * [作業D075]()
    * 權重更新與 Loss function 是 BP Neural Network 重要的一環，請參考範例D075-Back_Propagation，然後練習把網路增加到第三層

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D076-優化器Optimizers
* 優化器(optimizers)
  * 通過最優化方法對目標函數進行優化從而訓練出最好的模型
    * 功能：通過改善訓練方式，來最小化(或最大化)損失函數 $E(x)$
    * 算法：用來更新和計算影響模型訓練和模型輸出的網絡參數，使其逼近或達到最優值
  * SGD：隨機梯度下降法(stochastic gradient decent)
    * 找出參數的梯度(利用微分的方法)，往梯度的方向去更新參數(weight)
    * 優點：SGD 每次更新時對每個樣本進行梯度更新，對於很大的數據集來說，可能會有相似的樣本，而 SGD 一次只進行一次更新，就沒有冗餘，而且比較快
    * 缺點：但是 SGD 因為更新比較頻繁，會造成 cost function 有嚴重的震盪
    * SGD 調用
      * 語法：keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)
      * 參數
        * lr：<float>，學習率
        * Momentum 動量：<float>，用於加速 SGD 在相關方向上前進，並抑制震盪
        * Decay(衰變)：<float>，每次參數更新後學習率衰減值
        * nesterov：布爾值，是否使用 Nesterov 動量

        ```
        from keras import optimizers 

        model = Sequential() 
        model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))
        model.add(Activation('softmax’)) 

        #實例化一個優化器對象，然後將它傳入model.compile()，可以修改參數
        sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='mean_squared_error', optimizer=sgd) 

        # 通過名稱來調用優化器，將使用優化器的默認參數
        model.compile(loss='mean_squared_error', optimizer='sgd')
        ```
      * SGD, mini-batch gradient descent
        * batch-gradient：就是普通的梯度下降算法，但是採用批量處理
          * 當數據集很大(比如有 100000 個左右時)，每次 iteration 都要將 1000000 個數據跑一遍，機器帶不動。於是有了 mini-batch-gradient ——將 1000000 個樣本分成 1000 份，每份 1000 個，都看成一組獨立的數據集，進行 forward_propagation 和 backward_propagation
        * 在整個算法的流程中，cost function 是局部的，但是 W 和 b 是全局的
          * 批量梯度下降對訓練集上每一個數據都計算誤差，但只在所有訓練數據計算完成後才更新模型
          * 對訓練集上的一次訓練過程稱為一代(epoch)。因此，批量梯度下降是在每一個訓練 epoch 之後更新模型
        * epoch、iteration、batchsize，mini-batch
          * batchsize：批量大小，即每次訓練在訓練集中取 batchsize 個樣本訓練
            * batchsize=1
            * batchsize = mini-batch;
            * batchsize = whole training set
          * iteration：1個 iteration 等於使用 batchsize 個樣本訓練一次
          * epoch：1個 epoch 等於使用訓練集中的全部樣本訓練一次
        * 範例
          * features is (50000, 400)
          * labels is (50000, 10)
          * batch_size is 128
          * Iteration = 50000/128+1 = 391
        * 配置 mini-batch 梯度下降
          * Mini-batch sizes 簡稱為「batch sizes」，是算法設計中需要調節的參數
          * 較小的值讓學習過程收斂更快，但是產生更多噪聲
          * 較大的值讓學習過程收斂較慢，但是準確的估計誤差梯度
          * batch size 的默認值最好是 32 盡量選擇 2 的冪次方，有利於 GPU 的加速
          * 調節 batch size 時，最好觀察模型在不同 batch size 下的訓練時間和驗證誤差的學習曲線
          * 調整其他所有超參數之後再調整 batch size 和學習率
  * Adagrad
    * 對於常見的數據給予比較小的學習率去調整參數，對於不常見的數據給予比較大的學習率調整參數
      * 每個參數都有不同的 learning rate
      * 根據之前所有 gradient 的 root mean square 修改 
    * 優點：Adagrad 的是減少了學習率的手動調節
    * 缺點：它的缺點是分母會不斷積累，這樣學習率就會收縮並最終會變得非常小
    * Adagrad 調用
      * 語法：keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)
      * 參數
        * lr：float >= 0，學習率，一般 η 就取 0.01
        * epsilon：float >= 0，若為 None，默認為 K.epsilon()
        * decay：float >= 0，每次參數更新後學習率衰減值
          ```
          from keras import optimizers 

          model = Sequential() 
          model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))
          model.add(Activation('softmax')) 

          #實例化一個優化器對象，然後將它傳入 model.compile()，可以修改參數
          opt = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)
          model.compile(loss='mean_squared_error', optimizer=opt) 
          ```
  * RMSprop
    * RMSProp 算法旨在抑制梯度的鋸齒下降，但與動量相比，RMSProp 不需要手動配置學習率超參數，由算法自動完成。更重要的是，RMSProp 可為每個參數選擇不同的學習率
    * This optimizer is usually a good choice for recurrent neural networks
    * RMSprop 為了解決 Adagrad 學習率急劇下降問題的，比對梯度更新規則：<br>
      ![]()
    * RMSprop 調用
      * 語法：keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0) 
      * 參數
        * lr：float >= 0，Learning rate
        * rho：float >= 0
        * epsilon：float >= 0，Fuzz factor。If None, defaults to K.epsilon()
        * decay：float >= 0，Learning rate decay over each update
          ```
          from keras import optimizers 

          model = Sequential() 
          model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))
          model.add(Activation('softmax')) 

          #實例化一個優化器對象，然後將它傳入model.compile() , 可以修改參數
          opt = optimizers.RMSprop(lr=0.001, epsilon=None, decay=0.0) 
          model.compile(loss='mean_squared_error', optimizer=opt)
          ```
  * Adam
    * 除像 RMSprop 一樣存儲了過去梯度的平方 $v_t$ 的指數衰減平均值，也像 momentum 一樣保持了過去梯度 $m_t$ 的指數衰減平均值，「t」
    * 計算梯度的指數移動平均數，$m_0$ 初始化為 0。綜合考慮之前時間步的梯度動量
    * $β_1$ 係數為指數衰減率，控制權重分配(動量與當前梯度)，通常取接近於 1 的值。默認為 0.9
    * 其次，計算梯度平方的指數移動平均數，$v_0$ 初始化為 0。$β_2$ 係數為指數衰減率，控制之前的梯度平方的影響情況。類似於 RMSProp 算法，對梯度平方進行加權均值。默認為 0.999
    * 由於 $m_0$ 初始化為 0，會導致 $m_t$ 偏向於 0，尤其在訓練初期階段。所以，此處需要對梯度均值 $m_t$ 進行偏差糾正，降低偏差對訓練初期的影響
    * 與 $m_0$ 類似，因為 $v_0$ 初始化為 0 導致訓練初始階段 $v_t$ 偏向 0，對其進行糾正
    * 更新參數，初始的學習率 $lr$ 乘以梯度均值與梯度方差的平方根之比。其中默認學習率 lr =0.001, eplison (ε=10^-8)，避免除數變為 0
    * 對更新的步長計算，能夠從梯度均值及梯度平方兩個角度進行自適應地調節，而不是直接由當前梯度決定
    * Adam 調用
      * 語法：keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
      * 參數
        * lr：float >= 0，學習率
        * beta_1：float，0 < beta < 1，通常接近於 1
        * beta_2：float，0 < beta < 1，通常接近於 1
        * epsilon：float >= 0，模糊因數，若為 None，默認為 K.epsilon()
        * amsgrad：boolean，是否應用此演算法的 AMSGrad 變種，來自論文 「On the Convergence of Adam and Beyond」
        * decay：float >= 0，每次參數更新後學習率衰減值
          ```
          from keras import optimizers 

          model = Sequential() 
          model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))
          model.add(Activation('softmax')) 

          #實例化一個優化器對象，然後將它傳入 model.compile()，可修改參數
          opt = optimizers. Adam(lr=0.001, epsilon=None, decay=0.0) 
          model.compile(loss='mean_squared_error', optimizer=opt) 
          ```
* 最常用的優化算法：Gradient Descent
  * 使用各參數的梯度值來最小化或最大化損失函數 $E(x)$，通過尋找最小值，控制方差，更新模型參數，最終使模型收斂
  * 複習：梯度下降 Gradient Descent
    * $w_{i+1} = w_i - d_i*η_i, i=0,1,…
    * $η$ 是學習率：可設置為固定值，也可以用一維優化方法沿著訓練的方向逐步更新計算
    * 參數的更新分為兩步：第一步計算梯度下降的方向，第二步計算合適的學習 
  * 複習：動量 Momentum
    * 加入一項：可使得梯度方向不變的維度上速度變快，梯度方向有所改變的維度上的更新速度變慢，這樣就可以加快收斂並減小震盪
      ![]()
* 如何選擇優化器
  * 隨機梯度下降(SGD)：SGD 指的是 mini batch gradient descent
    * 優點：針對大數據集，訓練速度很快從訓練集樣本中隨機選取一個 batch 計算一次梯度，更新一次模型參數
    * 缺點
      * 對所有參數使用相同的學習率。對於稀疏數據或特徵，希望盡快更新一些不經常出現的特徵，慢一些更新常出現的特徵。所以選擇合適的學習率比較困難
      * 容易收斂到局部最優
    * 隨機梯度下降法是將數據分成一小批一小批的進行訓練，但是速度比較慢
  * AdaGrad 採用改變學習率的方式
  * Adam：利用梯度的一階矩估計和二階矩估計動態調節每個參數的學習率
    * 優點
      * 經過偏置校正後，每一次迭代都有確定的範圍，使得參數比較平穩。善於處理稀疏梯度和非平穩目標
      * 對內存需求小
      * 對不同內存計算不同的學習率
    * Adam 係結合 AdaGrad 和 RMSProp 兩種優化算法的優點。對梯度的一階矩估計(First Moment Estimation，即梯度的均值)和二階矩估計(Second Moment Estimation，即梯度的未中心化的方差)進行綜合考慮，計算出更新步長
  * RMSProp
    * 自適應調節學習率。對學習率進行了約束，適合處理非平穩目標和 RNN
    * RMSProp 是將 Momentum 與 AdaGrad 部分相結合
  * 如果輸入數據集比較稀疏，SGD、NAG和動量項等方法可能效果不好。因此對於稀疏數據集，應該使用某種自適應學習率的方法，且另一好處為不需要人為調整學習率，使用默認參數就可能獲得最優值：Adagrad, RMSprop, Adam
  * 如果想使訓練深層網絡模型快速收斂或所構建的神經網絡較為複雜，則應該使用Adam或其他自適應學習速率的方法，因為這些方法的實際效果更優
    * Adam 就是在 RMSprop 的基礎上加了 bias-correction 和 momentum
    * 隨著梯度變的稀疏，Adam 比 RMSprop 效果會好
* 範例與作業
  * [範例D076]()
  * [作業D076]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D077-神經網路-訓練細節與技巧-ValidationAndOverfit
* 範例與作業
  * [範例D077]()
  * [作業D077]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D078-神經網路-訓練前注意事項
* 範例與作業
  * [範例D078]()
  * [作業D078]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D079-神經網路-訓練細節與技巧-LearningRateEffect
* 範例與作業
  * [範例D079]()
  * [作業D079]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D080-程式實作-優化器與學習率的組合與比較
* 範例與作業
  * [範例D080]()
  * [作業D080]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D081-神經網路-訓練細節與技巧-Regularization
* 範例與作業
  * [範例D081]()
  * [作業D081]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D082-神經網路-訓練細節與技巧-Dropout
* 範例與作業
  * [範例D082]()
  * [作業D082]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D083-神經網路-訓練細節與技巧-BatchNormalization
* 範例與作業
  * [範例D083]()
  * [作業D083]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D084-程式實作-正規化AND機移除AND批次標準化的組合與比較
* 範例與作業
  * [範例D084]()
  * [作業D084]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D085-神經網路-訓練細節與技巧-使用Callbacks函數做Earlystop
* 範例與作業
  * [範例D085]()
  * [作業D085]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D086-神經網路-訓練細節與技巧-使用Callbacks函數儲存Model
* 範例與作業
  * [範例D086]()
  * [作業D086]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D087-神經網路-訓練細節與技巧-使用Callbacks函數做ReduceLearningRate
* 範例與作業
  * [範例D087]()
  * [作業D087]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D088-神經網路-訓練細節與技巧-撰寫Callbacks函數
* 範例與作業
  * [範例D088]()
  * [作業D088]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D089-神經網路-訓練細節與技巧-撰寫Lossfunction
* 範例與作業
  * [範例D089]()
  * [作業D089]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D090-使用傳統電腦視覺與機器學習進行影像辨識
* 範例與作業
  * [範例D090]()
  * [作業D090]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>

### D091-程式實作-使用傳統電腦視覺與機器學習進行影像辨識
* 範例與作業
  * [範例D091]()
  * [作業D091]()

Back to <a href="#初探深度學習使用Keras">初探深度學習使用Keras</a>
<br>
<br>


